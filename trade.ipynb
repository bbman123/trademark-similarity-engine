{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd138098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (2.9.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: click in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sacremoses) (1.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers) (2026.1.4)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 6.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/12.0 MB 8.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.0 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.2/12.0 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 9.9 MB/s  0:00:01\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 7.4 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 6.7 MB/s  0:00:00\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 5.6 MB/s  0:00:00\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 897.5/897.5 kB 6.0 MB/s  0:00:00\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-win_amd64.whl (154 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Installing collected packages: sentencepiece, safetensors, pyyaml, sacremoses, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [safetensors]\n",
      "   ----------- ---------------------------- 2/7 [pyyaml]\n",
      "   ----------- ---------------------------- 2/7 [pyyaml]\n",
      "   ----------------- ---------------------- 3/7 [sacremoses]\n",
      "   ----------------- ---------------------- 3/7 [sacremoses]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [tokenizers]\n",
      "   ---------------------------- ----------- 5/7 [tokenizers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------------- 7/7 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.36.0 pyyaml-6.0.3 sacremoses-0.1.1 safetensors-0.7.0 sentencepiece-0.2.1 tokenizers-0.22.2 transformers-4.57.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b2429c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRADEMARK VOCABULARY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Overview:\n",
      "   Total trademark pairs: 18737\n",
      "   Total wordmarks extracted: 36718\n",
      "\n",
      "üìù Vocabulary Statistics:\n",
      "   Total words (with repetition): 89049\n",
      "   Unique words: 15302\n",
      "   Average word length: 6.5 chars\n",
      "\n",
      "üîù Top 50 Most Frequent Words:\n",
      "--------------------------------------------------------------------------------\n",
      "  1. o                    ‚Üí  1827 occurrences\n",
      "  2. e                    ‚Üí  1457 occurrences\n",
      "  3. de                   ‚Üí  1262 occurrences\n",
      "  4. v                    ‚Üí   858 occurrences\n",
      "  5. s                    ‚Üí   660 occurrences\n",
      "  6. do                   ‚Üí   584 occurrences\n",
      "  7. v√¥es                 ‚Üí   468 occurrences\n",
      "  8. a                    ‚Üí   459 occurrences\n",
      "  9. da                   ‚Üí   382 occurrences\n",
      " 10. rio                  ‚Üí   304 occurrences\n",
      " 11. brasil               ‚Üí   300 occurrences\n",
      " 12. casa                 ‚Üí   276 occurrences\n",
      " 13. engenharia           ‚Üí   272 occurrences\n",
      " 14. avlon                ‚Üí   266 occurrences\n",
      " 15. cafv                 ‚Üí   261 occurrences\n",
      " 16. avon                 ‚Üí   258 occurrences\n",
      " 17. the                  ‚Üí   252 occurrences\n",
      " 18. d                    ‚Üí   233 occurrences\n",
      " 19. mv                   ‚Üí   225 occurrences\n",
      " 20. grupo                ‚Üí   201 occurrences\n",
      " 21. moda                 ‚Üí   200 occurrences\n",
      " 22. em                   ‚Üí   191 occurrences\n",
      " 23. os                   ‚Üí   190 occurrences\n",
      " 24. store                ‚Üí   190 occurrences\n",
      " 25. hair                 ‚Üí   188 occurrences\n",
      " 26. digital              ‚Üí   183 occurrences\n",
      " 27. tica                 ‚Üí   171 occurrences\n",
      " 28. care                 ‚Üí   171 occurrences\n",
      " 29. ria                  ‚Üí   168 occurrences\n",
      " 30. m                    ‚Üí   168 occurrences\n",
      " 31. solar                ‚Üí   165 occurrences\n",
      " 32. by                   ‚Üí   165 occurrences\n",
      " 33. pet                  ‚Üí   165 occurrences\n",
      " 34. la                   ‚Üí   152 occurrences\n",
      " 35. av                   ‚Üí   152 occurrences\n",
      " 36. of                   ‚Üí   152 occurrences\n",
      " 37. consultoria          ‚Üí   149 occurrences\n",
      " 38. sv                   ‚Üí   148 occurrences\n",
      " 39. soluv                ‚Üí   145 occurrences\n",
      " 40. com                  ‚Üí   143 occurrences\n",
      " 41. shoes                ‚Üí   143 occurrences\n",
      " 42. energia              ‚Üí   142 occurrences\n",
      " 43. nova                 ‚Üí   142 occurrences\n",
      " 44. bar                  ‚Üí   141 occurrences\n",
      " 45. imobiliv             ‚Üí   139 occurrences\n",
      " 46. melissa              ‚Üí   136 occurrences\n",
      " 47. science              ‚Üí   135 occurrences\n",
      " 48. instituto            ‚Üí   134 occurrences\n",
      " 49. ticos                ‚Üí   134 occurrences\n",
      " 50. id                   ‚Üí   132 occurrences\n",
      "\n",
      "üè∑Ô∏è  Categorized Terms:\n",
      "   Business       : mercado\n",
      "   Quality        : super, top, premium\n",
      "   Location       : rio, brasil, rios\n",
      "   Food           : casa, restaurante, pizza, food, pizzaria\n",
      "   Fashion        : moda, modas, fashion\n",
      "   Tech           : digital, tech\n"
     ]
    }
   ],
   "source": [
    "# Add this cell BEFORE your preprocessing code\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "def analyze_trademark_vocabulary(csv_path: str, top_n: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze your trademark dataset to extract frequent words for lexicon building\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to trademark CSV file\n",
    "        top_n: How many top words to extract\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TRADEMARK VOCABULARY ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path, encoding='MacRoman')\n",
    "    \n",
    "    # Extract wordmark columns\n",
    "    wordmarks = []\n",
    "    if 'Name RM' in df.columns:\n",
    "        wordmarks.extend(df['Name RM'].dropna().astype(str).tolist())\n",
    "    if 'Name TM' in df.columns:\n",
    "        wordmarks.extend(df['Name TM'].dropna().astype(str).tolist())\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   Total trademark pairs: {len(df)}\")\n",
    "    print(f\"   Total wordmarks extracted: {len(wordmarks)}\")\n",
    "    \n",
    "    # Tokenize and count words\n",
    "    all_words = []\n",
    "    for mark in wordmarks:\n",
    "        # Clean and tokenize\n",
    "        mark = mark.lower()\n",
    "        # Remove special characters but keep letters\n",
    "        words = re.findall(r'\\b[a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß√º√Ø]+\\b', mark)\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count frequencies\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    print(f\"\\nüìù Vocabulary Statistics:\")\n",
    "    print(f\"   Total words (with repetition): {len(all_words)}\")\n",
    "    print(f\"   Unique words: {len(word_freq)}\")\n",
    "    print(f\"   Average word length: {sum(len(w) for w in word_freq.keys()) / len(word_freq):.1f} chars\")\n",
    "    \n",
    "    # Get top N words\n",
    "    top_words = word_freq.most_common(top_n)\n",
    "    \n",
    "    print(f\"\\nüîù Top {min(50, len(top_words))} Most Frequent Words:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (word, count) in enumerate(top_words[:50], 1):\n",
    "        print(f\"{i:3d}. {word:20s} ‚Üí {count:5d} occurrences\")\n",
    "    \n",
    "    # Categorize words by domain\n",
    "    print(f\"\\nüè∑Ô∏è  Categorized Terms:\")\n",
    "    categories = {\n",
    "        'business': ['empresa', 'comercio', 'loja', 'mercado', 'servicos', 'industria'],\n",
    "        'quality': ['premium', 'super', 'mega', 'ultra', 'top', 'best', 'melhor'],\n",
    "        'location': ['brasil', 'rio', 's√£o', 'paulo', 'nacional', 'internacional'],\n",
    "        'food': ['cafe', 'food', 'pizza', 'restaurante', 'padaria', 'casa'],\n",
    "        'fashion': ['moda', 'fashion', 'style', 'wear', 'roupa'],\n",
    "        'tech': ['tech', 'digital', 'smart', 'web', 'online', 'net'],\n",
    "    }\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        category_words = [w for w, _ in top_words if any(kw in w for kw in keywords)]\n",
    "        if category_words:\n",
    "            print(f\"   {category.capitalize():15s}: {', '.join(category_words[:10])}\")\n",
    "    \n",
    "    # Return analysis results\n",
    "    return {\n",
    "        'top_words': top_words,\n",
    "        'word_freq': word_freq,\n",
    "        'total_unique': len(word_freq),\n",
    "        'categorized': categories\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "vocab_analysis = analyze_trademark_vocabulary('trademark_file.csv', top_n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74df0ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFace models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded Yoruba model (NLLB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded Hausa model (NLLB)\n",
      "Loading HuggingFace models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded Yoruba model (NLLB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded Hausa model (NLLB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:================================================================================\n",
      "INFO:__main__:BRAZILIAN TRADEMARK DATASET PREPROCESSING\n",
      "INFO:__main__:================================================================================\n",
      "INFO:__main__:\n",
      "1Ô∏è‚É£ Loading dataset...\n",
      "INFO:__main__:Detected encoding: MacRoman (confidence: 72.30%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: PROCESSING POSITIVE PAIRS (Similar Trademarks)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úì Successfully loaded with encoding: MacRoman\n",
      "INFO:__main__:   Loaded: (18737, 18)\n",
      "INFO:__main__:\n",
      "2Ô∏è‚É£ Extracting relevant columns...\n",
      "INFO:__main__:‚úì Extracted 18737 trademark pairs\n",
      "INFO:__main__:   Columns: ['mark1_id', 'mark2_id', 'mark1_wordmark', 'mark2_wordmark', 'mark1_status', 'mark2_status', 'mark1_class', 'mark2_class', 'mark1_presentation', 'mark2_presentation', 'mark1_date', 'mark2_date', 'similarity_reason', 'label']\n",
      "INFO:__main__:\n",
      "3Ô∏è‚É£ Applying sampling strategy...\n",
      "INFO:__main__:üìù Sampling strategy: random 5 rows (seed=42)\n",
      "INFO:__main__:   After sampling: (5, 14)\n",
      "INFO:__main__:\n",
      "3Ô∏è‚É£ Cleaning text...\n",
      "INFO:__main__:\n",
      "4Ô∏è‚É£ Handling missing values...\n",
      "INFO:__main__:\n",
      "5Ô∏è‚É£ Removing duplicates...\n",
      "INFO:__main__:\n",
      "6Ô∏è‚É£ Translating dataset...\n",
      "INFO:__main__:üåç Starting translation pipeline...\n",
      "INFO:__main__:   Translating mark1_wordmark...\n",
      "INFO:__main__:      ‚úì Portuguese ‚Üí English: mark1_wordmark_en\n",
      "INFO:__main__:      ‚úì English ‚Üí Hausa (lexicon+fallback): mark1_wordmark_ha\n",
      "INFO:__main__:      ‚úì English ‚Üí Yoruba (lexicon+fallback): mark1_wordmark_yo\n",
      "INFO:__main__:   Translating mark2_wordmark...\n",
      "INFO:__main__:      ‚úì Portuguese ‚Üí English: mark2_wordmark_en\n",
      "INFO:__main__:      ‚úì English ‚Üí Hausa (lexicon+fallback): mark2_wordmark_ha\n",
      "INFO:__main__:      ‚úì English ‚Üí Yoruba (lexicon+fallback): mark2_wordmark_yo\n",
      "INFO:__main__:\n",
      "7Ô∏è‚É£ Adding features...\n",
      "INFO:__main__:\n",
      "================================================================================\n",
      "INFO:__main__:‚úÖ PREPROCESSING COMPLETE\n",
      "INFO:__main__:   Final dataset: (5, 24)\n",
      "INFO:__main__:   Columns: ['mark1_id', 'mark2_id', 'mark1_wordmark', 'mark2_wordmark', 'mark1_status', 'mark2_status', 'mark1_class', 'mark2_class', 'mark1_presentation', 'mark2_presentation', 'mark1_date', 'mark2_date', 'similarity_reason', 'label', 'mark1_wordmark_en', 'mark1_wordmark_ha', 'mark1_wordmark_yo', 'mark2_wordmark_en', 'mark2_wordmark_ha', 'mark2_wordmark_yo', 'mark1_length', 'mark2_length', 'same_class', 'length_diff']\n",
      "INFO:__main__:================================================================================\n",
      "INFO:__main__:\n",
      "üìä Translation Statistics:\n",
      "INFO:__main__:   Lexicon hits: 4 (13.3%)\n",
      "INFO:__main__:   HuggingFace: 24 (80.0%)\n",
      "INFO:__main__:   Google Translate: 0 (0.0%)\n",
      "INFO:__main__:   Cache hits: 2 (6.7%)\n",
      "INFO:__main__:\n",
      "üìä Creating balanced dataset...\n",
      "INFO:__main__:\n",
      "üîÑ Generating negative pairs (strategy: mixed, ratio: 1.0)...\n",
      "INFO:__main__:   Valid wordmarks available: 8\n",
      "INFO:__main__:   Positive pairs: 5\n",
      "INFO:__main__:   Target negative pairs: 5\n",
      "INFO:__main__:   Existing pairs to avoid: 4\n",
      "INFO:__main__:‚úì Generated 5 negative pairs in 5 attempts\n",
      "INFO:__main__:   Translating negative pairs...\n",
      "INFO:__main__:‚úì Balanced dataset created:\n",
      "INFO:__main__:   Positive pairs: 5 (50.0%)\n",
      "INFO:__main__:   Negative pairs: 5 (50.0%)\n",
      "INFO:__main__:   Total: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Positive pairs ready: 5\n",
      "\n",
      "================================================================================\n",
      "STEP 2: GENERATING NEGATIVE PAIRS (Dissimilar Trademarks)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 3: ANALYZING DATASET BALANCE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DATASET BALANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Label Distribution:\n",
      "   Dissimilar (Negative): 5 (50.0%)\n",
      "   Similar (Positive): 5 (50.0%)\n",
      "\n",
      "üîÑ Pair Type Distribution:\n",
      "   Negative: 5 (50.0%)\n",
      "   Positive: 5 (50.0%)\n",
      "\n",
      "üè∑Ô∏è  Same Class Distribution:\n",
      "   Different Class: 4 (40.0%)\n",
      "   Same Class: 1 (10.0%)\n",
      "\n",
      "üìè Length Statistics by Label:\n",
      "   Negative pairs:\n",
      "      Avg mark1 length: 8.0\n",
      "      Avg mark2 length: 11.2\n",
      "      Avg length diff: nan\n",
      "   Positive pairs:\n",
      "      Avg mark1 length: 11.2\n",
      "      Avg mark2 length: 9.0\n",
      "      Avg length diff: 4.2\n",
      "\n",
      "================================================================================\n",
      "SAMPLE DATA\n",
      "================================================================================\n",
      "\n",
      "üü¢ POSITIVE PAIRS (Similar - Label=1):\n",
      "mark1_wordmark   mark2_wordmark  label same_class\n",
      "  LIDER DESIGN Lv√ÅDER ESTOFADOS      1          0\n",
      "       KOLENZA          COSENZA      1          0\n",
      "      SECPOWER        Power Sek      1          1\n",
      "\n",
      "üî¥ NEGATIVE PAIRS (Dissimilar - Label=0):\n",
      "mark1_wordmark mark2_wordmark  label same_class\n",
      "     Power Sek  Digital Light      0       None\n",
      "       COSENZA      Power Sek      0       None\n",
      "      SECPOWER        COSENZA      0       None\n",
      "\n",
      "‚úÖ Dataset saved to: trademark_data_balanced_multilingual_4.csv\n",
      "   Total pairs: 10\n",
      "   Columns: 25\n",
      "   File size: 0.03 MB\n",
      "\n",
      "‚úÖ Dataset ready for Step 1 (CNN + SVM training)\n",
      "   Use file: trademark_data_balanced_multilingual_4.csv\n",
      "   Summary saved to: dataset_summary.json\n",
      "\n",
      "üìÑ Summary:\n",
      "{\n",
      "  \"dataset_info\": {\n",
      "    \"total_pairs\": 10,\n",
      "    \"positive_pairs\": 5,\n",
      "    \"negative_pairs\": 5,\n",
      "    \"balance_ratio\": 1.0,\n",
      "    \"file_size_mb\": 0.02743244171142578,\n",
      "    \"has_negative_pairs\": true\n",
      "  },\n",
      "  \"preprocessing\": {\n",
      "    \"languages\": [\n",
      "      \"Portuguese\",\n",
      "      \"English\",\n",
      "      \"Hausa\",\n",
      "      \"Yoruba\"\n",
      "    ],\n",
      "    \"max_rows_processed\": 5,\n",
      "    \"sampling_strategy\": \"random\",\n",
      "    \"negative_generation_strategy\": \"mixed\",\n",
      "    \"negative_ratio\": 1.0\n",
      "  },\n",
      "  \"columns\": {\n",
      "    \"total_columns\": 25,\n",
      "    \"column_list\": [\n",
      "      \"mark1_id\",\n",
      "      \"mark2_id\",\n",
      "      \"mark1_wordmark\",\n",
      "      \"mark2_wordmark\",\n",
      "      \"mark1_status\",\n",
      "      \"mark2_status\",\n",
      "      \"mark1_class\",\n",
      "      \"mark2_class\",\n",
      "      \"mark1_presentation\",\n",
      "      \"mark2_presentation\",\n",
      "      \"mark1_date\",\n",
      "      \"mark2_date\",\n",
      "      \"similarity_reason\",\n",
      "      \"label\",\n",
      "      \"mark1_wordmark_en\",\n",
      "      \"mark1_wordmark_ha\",\n",
      "      \"mark1_wordmark_yo\",\n",
      "      \"mark2_wordmark_en\",\n",
      "      \"mark2_wordmark_ha\",\n",
      "      \"mark2_wordmark_yo\",\n",
      "      \"mark1_length\",\n",
      "      \"mark2_length\",\n",
      "      \"same_class\",\n",
      "      \"length_diff\",\n",
      "      \"pair_type\"\n",
      "    ]\n",
      "  },\n",
      "  \"class_distribution\": {\n",
      "    \"same_class_pairs\": 1,\n",
      "    \"different_class_pairs\": 4,\n",
      "    \"same_class_percentage\": 20.0\n",
      "  },\n",
      "  \"length_statistics\": {\n",
      "    \"avg_mark1_length\": 10.285714285714286,\n",
      "    \"avg_mark2_length\": 10.0,\n",
      "    \"avg_length_diff\": 4.2\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Latest\n",
    "##NEWWWWWWWWWWWWWWWWW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import re\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from deep_translator import GoogleTranslator\n",
    "import chardet  # For encoding detection\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_incremented_filename(base_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns an incremented filename if the file already exists.\n",
    "    Example:\n",
    "      - 'data.csv' ‚Üí 'data_1.csv' ‚Üí 'data_2.csv' ...\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_name):\n",
    "        return base_name\n",
    "\n",
    "    # Split name and extension\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    \n",
    "    # Check for existing numbered versions\n",
    "    counter = 1\n",
    "    while True:\n",
    "        new_name = f\"{name}_{counter}{ext}\"\n",
    "        if not os.path.exists(new_name):\n",
    "            return new_name\n",
    "        counter += 1\n",
    "\n",
    "# Add this near the top of your file (after imports)\n",
    "TRADEMARK_LEXICON = {\n",
    "    # ========== BUSINESS & COMMERCE ==========\n",
    "    \"company\": {\"ha\": \"kamfani\", \"yo\": \"il√©-i·π£·∫πÃÅ\"},\n",
    "    \"business\": {\"ha\": \"kasuwanci\", \"yo\": \"i·π£·∫πÃÅ ow√≥\"},\n",
    "    \"commerce\": {\"ha\": \"kasuwanci\", \"yo\": \"√¨·π£√≤w√≤\"},\n",
    "    \"market\": {\"ha\": \"kasuwa\", \"yo\": \"·ªçj√†\"},\n",
    "    \"store\": {\"ha\": \"shago\", \"yo\": \"·π£·ªçÃÅ·ªçÃÄb√π\"},\n",
    "    \"shop\": {\"ha\": \"shago\", \"yo\": \"·π£·ªçÃÅ·ªçÃÄb√π\"},\n",
    "    \"trade\": {\"ha\": \"ciniki\", \"yo\": \"√¨·π£√≤w√≤\"},\n",
    "    \"service\": {\"ha\": \"sabis\", \"yo\": \"√¨·π£·∫πÃÅ\"},\n",
    "    \"services\": {\"ha\": \"ayyuka\", \"yo\": \"√†w·ªçn i·π£·∫πÃÅ\"},\n",
    "    \"brand\": {\"ha\": \"alama\", \"yo\": \"√†m√¨\"},\n",
    "    \"logo\": {\"ha\": \"tambari\", \"yo\": \"√†m√¨\"},\n",
    "    \n",
    "    # ========== QUALITY DESCRIPTORS ==========\n",
    "    \"premium\": {\"ha\": \"mai kyau\", \"yo\": \"iyeb√≠ye\"},\n",
    "    \"super\": {\"ha\": \"babba\", \"yo\": \"p√∫p·ªçÃÄ\"},\n",
    "    \"mega\": {\"ha\": \"babba sosai\", \"yo\": \"≈Ñl√° p√∫p·ªçÃÄ\"},\n",
    "    \"ultra\": {\"ha\": \"mafi\", \"yo\": \"j√πl·ªç\"},\n",
    "    \"best\": {\"ha\": \"mafi kyau\", \"yo\": \"d√°ra j√πl·ªç\"},\n",
    "    \"top\": {\"ha\": \"sama\", \"yo\": \"√≤k√®\"},\n",
    "    \"elite\": {\"ha\": \"za…ìa…ì…ìu\", \"yo\": \"√†·π£√†y√†n\"},\n",
    "    \"royal\": {\"ha\": \"sarauta\", \"yo\": \"·ªçba\"},\n",
    "    \"gold\": {\"ha\": \"zinariya\", \"yo\": \"w√∫r√†\"},\n",
    "    \"silver\": {\"ha\": \"azurfa\", \"yo\": \"f√†d√°k√†\"},\n",
    "    \"diamond\": {\"ha\": \"lu'ulu'u\", \"yo\": \"√≤k√∫ta iyeb√≠ye\"},\n",
    "    \"original\": {\"ha\": \"asali\", \"yo\": \"√†t·ªçw·ªçÃÅd·ªçÃÅw·ªçÃÅ\"},\n",
    "    \"classic\": {\"ha\": \"na gargajiya\", \"yo\": \"√¨b√≠l·∫πÃÄ\"},\n",
    "    \"modern\": {\"ha\": \"na zamani\", \"yo\": \"·ªçÃÄd·∫π\"},\n",
    "    \"new\": {\"ha\": \"sabon\", \"yo\": \"t√≠tun\"},\n",
    "    \"fresh\": {\"ha\": \"sabo\", \"yo\": \"tuntun\"},\n",
    "    \"natural\": {\"ha\": \"na halitta\", \"yo\": \"√†d√°y√©b√°\"},\n",
    "    \"organic\": {\"ha\": \"kwayoyi na halitta\", \"yo\": \"·ªçganiki\"},\n",
    "    \n",
    "    # ========== FOOD & BEVERAGE ==========\n",
    "    \"coffee\": {\"ha\": \"kofi\", \"yo\": \"k·ªçÃÅf√≠\"},\n",
    "    \"tea\": {\"ha\": \"shayi\", \"yo\": \"tii\"},\n",
    "    \"food\": {\"ha\": \"abinci\", \"yo\": \"o√∫nj·∫π\"},\n",
    "    \"restaurant\": {\"ha\": \"gidan cin abinci\", \"yo\": \"il√© o√∫nj·∫π\"},\n",
    "    \"cafe\": {\"ha\": \"kantin kofi\", \"yo\": \"il√© k·ªçÃÅf√≠\"},\n",
    "    \"bakery\": {\"ha\": \"gidan burodi\", \"yo\": \"il√© b√∫r·∫πÃÅd√¨\"},\n",
    "    \"pizza\": {\"ha\": \"pizza\", \"yo\": \"p√≠t√≠s√†\"},\n",
    "    \"burger\": {\"ha\": \"burger\", \"yo\": \"b·ªçÃÅg√†\"},\n",
    "    \"bread\": {\"ha\": \"burodi\", \"yo\": \"b√∫r·∫πÃÅd√¨\"},\n",
    "    \"meat\": {\"ha\": \"nama\", \"yo\": \"·∫πran\"},\n",
    "    \"milk\": {\"ha\": \"madara\", \"yo\": \"w√†r√†\"},\n",
    "    \"juice\": {\"ha\": \"ruwan 'ya'yan itace\", \"yo\": \"omi √®so\"},\n",
    "    \"water\": {\"ha\": \"ruwa\", \"yo\": \"omi\"},\n",
    "    \"drink\": {\"ha\": \"abin sha\", \"yo\": \"ohun m√≠mu\"},\n",
    "    \"sweet\": {\"ha\": \"mai dadi\", \"yo\": \"d√≠d√πn\"},\n",
    "    \"delicious\": {\"ha\": \"mai dadi\", \"yo\": \"d√≠d√πn\"},\n",
    "    \n",
    "    # ========== FASHION & CLOTHING ==========\n",
    "    \"fashion\": {\"ha\": \"salon\", \"yo\": \"√†·π£√† a·π£·ªç\"},\n",
    "    \"style\": {\"ha\": \"salo\", \"yo\": \"√†·π£√†\"},\n",
    "    \"wear\": {\"ha\": \"sawa\", \"yo\": \"w·ªçÃÄ\"},\n",
    "    \"clothing\": {\"ha\": \"tufafi\", \"yo\": \"a·π£·ªç\"},\n",
    "    \"dress\": {\"ha\": \"riga\", \"yo\": \"a·π£·ªç\"},\n",
    "    \"shirt\": {\"ha\": \"riga\", \"yo\": \"·π£·∫πÃÅt√π\"},\n",
    "    \"shoes\": {\"ha\": \"takalma\", \"yo\": \"b√†t√†\"},\n",
    "    \"hat\": {\"ha\": \"hula\", \"yo\": \"f√¨l√†\"},\n",
    "    \"bag\": {\"ha\": \"jaka\", \"yo\": \"√†p√≤\"},\n",
    "    \"accessories\": {\"ha\": \"kayan ado\", \"yo\": \"ohun ·ªçÃÄ·π£·ªçÃÅ\"},\n",
    "    \n",
    "    # ========== TECHNOLOGY ==========\n",
    "    \"tech\": {\"ha\": \"fasaha\", \"yo\": \"√¨m·ªçÃÄ-·∫πr·ªç\"},\n",
    "    \"technology\": {\"ha\": \"fasahar zamani\", \"yo\": \"√¨m·ªçÃÄ-·∫πr·ªç\"},\n",
    "    \"digital\": {\"ha\": \"na dijital\", \"yo\": \"on√≠j√¨t√π\"},\n",
    "    \"smart\": {\"ha\": \"mai hankali\", \"yo\": \"ol√≥gb·ªçÃÅn\"},\n",
    "    \"online\": {\"ha\": \"kan layi\", \"yo\": \"l√≥r√≠ ay√©luj√°ra\"},\n",
    "    \"internet\": {\"ha\": \"yanar gizo\", \"yo\": \"√≠≈Ñt√°n·∫πÃÅ·∫πÃÄt√¨\"},\n",
    "    \"app\": {\"ha\": \"app\", \"yo\": \"√¨·π£√†m√∫l√≤\"},\n",
    "    \"software\": {\"ha\": \"software\", \"yo\": \"s·ªçÃÅf√¨t√≠w√≠√†\"},\n",
    "    \"web\": {\"ha\": \"yanar gizo\", \"yo\": \"w·∫πÃÅ·∫πÃÄb√π\"},\n",
    "    \"mobile\": {\"ha\": \"wayar hannu\", \"yo\": \"f√≥√≤n√π al√°gb√®√©k√°\"},\n",
    "    \"phone\": {\"ha\": \"waya\", \"yo\": \"f√≥√≤n√π\"},\n",
    "    \"computer\": {\"ha\": \"kwamfuta\", \"yo\": \"k·ªçÃÄmp√∫t√†\"},\n",
    "    \n",
    "    # ========== LOCATION & GEOGRAPHY ==========\n",
    "    \"brazil\": {\"ha\": \"Brazil\", \"yo\": \"Brazil\"},\n",
    "    \"brazilian\": {\"ha\": \"na Brazil\", \"yo\": \"ar√° Brazil\"},\n",
    "    \"city\": {\"ha\": \"birni\", \"yo\": \"√¨l√∫\"},\n",
    "    \"house\": {\"ha\": \"gida\", \"yo\": \"il√©\"},\n",
    "    \"home\": {\"ha\": \"gida\", \"yo\": \"il√©\"},\n",
    "    \"place\": {\"ha\": \"wuri\", \"yo\": \"ib√¨\"},\n",
    "    \"center\": {\"ha\": \"cibiyar\", \"yo\": \"√†√°r√≠n\"},\n",
    "    \"plaza\": {\"ha\": \"filin\", \"yo\": \"gb√†gede\"},\n",
    "    \n",
    "    # ========== SIZE & QUANTITY ==========\n",
    "    \"big\": {\"ha\": \"babba\", \"yo\": \"≈Ñl√°\"},\n",
    "    \"small\": {\"ha\": \"∆ôarami\", \"yo\": \"k√©ker√©\"},\n",
    "    \"large\": {\"ha\": \"babba\", \"yo\": \"≈Ñl√°\"},\n",
    "    \"mini\": {\"ha\": \"∆ôan∆ôanta\", \"yo\": \"k√©√©k√®√®k√©\"},\n",
    "    \"maxi\": {\"ha\": \"babba sosai\", \"yo\": \"t√≥bi\"},\n",
    "    \"extra\": {\"ha\": \"∆ôari\", \"yo\": \"√†fik√∫n\"},\n",
    "    \"plus\": {\"ha\": \"da ∆ôari\", \"yo\": \"√†fik√∫n\"},\n",
    "    \n",
    "    # ========== TIME & STATUS ==========\n",
    "    \"new\": {\"ha\": \"sabon\", \"yo\": \"t√≠tun\"},\n",
    "    \"old\": {\"ha\": \"tsoho\", \"yo\": \"√†tij·ªçÃÅ\"},\n",
    "    \"now\": {\"ha\": \"yanzu\", \"yo\": \"b√°y√¨√≠\"},\n",
    "    \"today\": {\"ha\": \"yau\", \"yo\": \"√≤n√≠\"},\n",
    "    \"fast\": {\"ha\": \"sauri\", \"yo\": \"y√°ra\"},\n",
    "    \"quick\": {\"ha\": \"sauri\", \"yo\": \"y√°ra\"},\n",
    "    \"express\": {\"ha\": \"gaggawa\", \"yo\": \"k√≠√°k√≠√°\"},\n",
    "    \n",
    "    # ========== COLORS ==========\n",
    "    \"red\": {\"ha\": \"ja\", \"yo\": \"pupa\"},\n",
    "    \"blue\": {\"ha\": \"shu…ói\", \"yo\": \"b√∫l√∫√π\"},\n",
    "    \"green\": {\"ha\": \"kore\", \"yo\": \"al√°w·ªçÃÄ ew√©\"},\n",
    "    \"yellow\": {\"ha\": \"rawaya\", \"yo\": \"on√≠b√πd√≥\"},\n",
    "    \"white\": {\"ha\": \"fari\", \"yo\": \"funfun\"},\n",
    "    \"black\": {\"ha\": \"ba∆ôi\", \"yo\": \"d√∫d√∫\"},\n",
    "    \n",
    "    # ========== COMMON ACTIONS ==========\n",
    "    \"buy\": {\"ha\": \"saya\", \"yo\": \"r√†\"},\n",
    "    \"sell\": {\"ha\": \"sayar\", \"yo\": \"t√†\"},\n",
    "    \"make\": {\"ha\": \"yi\", \"yo\": \"·π£e\"},\n",
    "    \"create\": {\"ha\": \"∆ôir∆ôira\", \"yo\": \"d√°\"},\n",
    "    \"build\": {\"ha\": \"gina\", \"yo\": \"k·ªçÃÅ\"},\n",
    "    \"design\": {\"ha\": \"∆ôira\", \"yo\": \"y√†w√≤r√°n\"},\n",
    "    \"repair\": {\"ha\": \"gyara\", \"yo\": \"t√∫n\"},\n",
    "    \"fix\": {\"ha\": \"gyara\", \"yo\": \"t√∫n\"},\n",
    "    \n",
    "    # ========== COMMON NOUNS ==========\n",
    "    \"king\": {\"ha\": \"sarki\", \"yo\": \"·ªçba\"},\n",
    "    \"queen\": {\"ha\": \"sarauniya\", \"yo\": \"ayaba\"},\n",
    "    \"people\": {\"ha\": \"mutane\", \"yo\": \"√†w·ªçn √®n√¨y√†n\"},\n",
    "    \"woman\": {\"ha\": \"mace\", \"yo\": \"ob√¨nrin\"},\n",
    "    \"man\": {\"ha\": \"mutum\", \"yo\": \"·ªçk√πnrin\"},\n",
    "    \"child\": {\"ha\": \"yaro\", \"yo\": \"·ªçm·ªçd√©\"},\n",
    "    \"family\": {\"ha\": \"iyali\", \"yo\": \"·∫πb√≠\"},\n",
    "    \"friend\": {\"ha\": \"aboki\", \"yo\": \"·ªçÃÄr·∫πÃÅ\"},\n",
    "    \"love\": {\"ha\": \"∆ôauna\", \"yo\": \"√¨f·∫πÃÅ\"},\n",
    "    \"life\": {\"ha\": \"rayuwa\", \"yo\": \"√¨gb√©s√≠ ay√©\"},\n",
    "    \"world\": {\"ha\": \"duniya\", \"yo\": \"ay√©\"},\n",
    "    \n",
    "    # Add more based on your dataset analysis...\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Add this BEFORE the BrazilianTrademarkPreprocessor class\n",
    "\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedTranslator:\n",
    "    \"\"\"\n",
    "    Multi-strategy translator with:\n",
    "    1. Domain-specific lexicon (fastest)\n",
    "    2. HuggingFace models (better quality)\n",
    "    3. Google Translate (fallback)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lexicon = TRADEMARK_LEXICON\n",
    "        self.hf_models = {}\n",
    "        self.google_translators = {\n",
    "            'ha': GoogleTranslator(source='en', target='ha'),\n",
    "            'yo': GoogleTranslator(source='en', target='yo')\n",
    "        }\n",
    "        self.translation_cache = {}\n",
    "        self.stats = {'lexicon': 0, 'huggingface': 0, 'google': 0, 'cache': 0}\n",
    "        \n",
    "        # Try to load HuggingFace models (optional)\n",
    "        self._load_hf_models()\n",
    "    \n",
    "    def _load_hf_models(self):\n",
    "        \"\"\"\n",
    "        Load HuggingFace translation models for Yoruba/Hausa\n",
    "        Falls back gracefully if models not available\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Facebook's NLLB model supports 200+ languages including Yoruba\n",
    "            print(\"Loading HuggingFace models...\")\n",
    "            self.hf_models['yo'] = pipeline(\n",
    "                \"translation\", \n",
    "                model=\"facebook/nllb-200-distilled-600M\",\n",
    "                src_lang=\"eng_Latn\",\n",
    "                tgt_lang=\"yor_Latn\"\n",
    "            )\n",
    "            print(\"‚úì Loaded Yoruba model (NLLB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† HuggingFace Yoruba model not available: {e}\")\n",
    "            print(\"  Will use Google Translate fallback\")\n",
    "        \n",
    "        try:\n",
    "            # Hausa model\n",
    "            self.hf_models['ha'] = pipeline(\n",
    "                \"translation\",\n",
    "                model=\"facebook/nllb-200-distilled-600M\",\n",
    "                src_lang=\"eng_Latn\",\n",
    "                tgt_lang=\"hau_Latn\"\n",
    "            )\n",
    "            print(\"‚úì Loaded Hausa model (NLLB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† HuggingFace Hausa model not available: {e}\")\n",
    "            print(\"  Will use Google Translate fallback\")\n",
    "    \n",
    "    def translate_word(self, word: str, target_lang: str) -> str:\n",
    "        \"\"\"\n",
    "        Translate a single word using multi-strategy approach\n",
    "        Priority: Lexicon ‚Üí HuggingFace ‚Üí Google Translate\n",
    "        \"\"\"\n",
    "        if not word or not word.strip():\n",
    "            return word\n",
    "        \n",
    "        word_lower = word.lower().strip()\n",
    "        cache_key = f\"{target_lang}:{word_lower}\"\n",
    "        \n",
    "        # 1. Check cache\n",
    "        if cache_key in self.translation_cache:\n",
    "            self.stats['cache'] += 1\n",
    "            return self.translation_cache[cache_key]\n",
    "        \n",
    "        # 2. Try lexicon (fastest and most accurate for domain terms)\n",
    "        if word_lower in self.lexicon:\n",
    "            translation = self.lexicon[word_lower].get(target_lang, word)\n",
    "            self.stats['lexicon'] += 1\n",
    "            self.translation_cache[cache_key] = translation\n",
    "            return translation\n",
    "        \n",
    "        # 3. Try HuggingFace model (better than Google for African languages)\n",
    "        if target_lang in self.hf_models:\n",
    "            try:\n",
    "                result = self.hf_models[target_lang](word)\n",
    "                translation = result[0]['translation_text']\n",
    "                self.stats['huggingface'] += 1\n",
    "                self.translation_cache[cache_key] = translation\n",
    "                return translation\n",
    "            except Exception as e:\n",
    "                pass  # Fall through to Google Translate\n",
    "        \n",
    "        # 4. Fallback to Google Translate\n",
    "        try:\n",
    "            translation = self.google_translators[target_lang].translate(word)\n",
    "            self.stats['google'] += 1\n",
    "            self.translation_cache[cache_key] = translation\n",
    "            return translation\n",
    "        except Exception as e:\n",
    "            # Ultimate fallback: return original\n",
    "            return word\n",
    "    \n",
    "    def translate_text(self, text: str, target_lang: str) -> str:\n",
    "        \"\"\"\n",
    "        Translate full text word-by-word\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return \"\"\n",
    "        \n",
    "        words = re.findall(r\"\\b\\w+\\b\", str(text))\n",
    "        translated_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            trans = self.translate_word(word, target_lang)\n",
    "            translated_words.append(trans)\n",
    "        \n",
    "        return \" \".join(translated_words)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get translation statistics\"\"\"\n",
    "        total = sum(self.stats.values())\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'total': total,\n",
    "            'lexicon_pct': (self.stats['lexicon'] / total * 100) if total > 0 else 0,\n",
    "            'hf_pct': (self.stats['huggingface'] / total * 100) if total > 0 else 0,\n",
    "            'google_pct': (self.stats['google'] / total * 100) if total > 0 else 0,\n",
    "            'cache_pct': (self.stats['cache'] / total * 100) if total > 0 else 0\n",
    "        }\n",
    "\n",
    "# Initialize enhanced translator\n",
    "enhanced_translator = EnhancedTranslator()\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"Configuration for preprocessing strategy\"\"\"\n",
    "    fix_encoding: bool = True\n",
    "    translate_to_english: bool = True\n",
    "    translate_to_local: bool = True  # HA/YO\n",
    "    remove_duplicates: bool = True\n",
    "    handle_missing: str = 'drop'  # 'drop' or 'fill'\n",
    "     # NEW: Sampling control\n",
    "    max_rows: Optional[int] = None      # e.g., 1000 ‚Üí process only 1000 rows\n",
    "    sampling_strategy: str = 'first'    # 'first', 'random', or 'all'\n",
    "\n",
    "\n",
    "class BrazilianTrademarkPreprocessor:\n",
    "    \"\"\"\n",
    "    Specialized preprocessor for Brazilian INPI trademark dataset\n",
    "    Handles: Portuguese ‚Üí English ‚Üí Hausa/Yoruba translation pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PreprocessingConfig = None):\n",
    "        self.config = config or PreprocessingConfig()\n",
    "        self.original_df = None\n",
    "        self.processed_df = None\n",
    "        \n",
    "        # Initialize translators\n",
    "        self.pt_to_en = GoogleTranslator(source='pt', target='en')\n",
    "        self.enhanced_translator = EnhancedTranslator()  # NEW: Use enhanced translator\n",
    "        \n",
    "        self.translation_cache = {}\n",
    "    \n",
    "    def detect_encoding(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Detect file encoding to fix those vÔøΩ characters\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            result = chardet.detect(f.read(100000))  # Read first 100KB\n",
    "        \n",
    "        detected = result['encoding']\n",
    "        confidence = result['confidence']\n",
    "        \n",
    "        logger.info(f\"Detected encoding: {detected} (confidence: {confidence:.2%})\")\n",
    "        return detected\n",
    "    \n",
    "    def _get_lexicon_translation(self, word: str, target_lang: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get translation from domain lexicon (case-insensitive)\n",
    "        target_lang: 'ha' or 'yo'\n",
    "        \"\"\"\n",
    "        word_lower = word.lower().strip()\n",
    "        if word_lower in TRADEMARK_LEXICON:\n",
    "            return TRADEMARK_LEXICON[word_lower].get(target_lang)\n",
    "        return None\n",
    "\n",
    "    def translate_word_with_fallback(self, word: str, target_lang: str) -> str:\n",
    "        \"\"\"\n",
    "        Translate a single word using lexicon first, then Google Translate\n",
    "        \"\"\"\n",
    "        if not word or not word.strip():\n",
    "            return word\n",
    "        \n",
    "        # 1. Try domain lexicon\n",
    "        lex_trans = self._get_lexicon_translation(word, target_lang)\n",
    "        if lex_trans:\n",
    "            return lex_trans\n",
    "        \n",
    "        # 2. Fallback to Google Translate\n",
    "        translator = self.en_to_ha if target_lang == 'ha' else self.en_to_yo\n",
    "        cache_key = f\"en_{target_lang}\"\n",
    "        return self.translate_with_cache(word, translator, cache_key)\n",
    "\n",
    "    def translate_text_lexicon_first(self, text_en: str, target_lang: str) -> str:\n",
    "        \"\"\"\n",
    "        Translate using enhanced multi-strategy translator\n",
    "        \"\"\"\n",
    "        return self.enhanced_translator.translate_text(text_en, target_lang)\n",
    "\n",
    "    def load_and_fix_encoding(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load CSV with proper encoding detection and fixing\n",
    "        \"\"\"\n",
    "        # Try to detect encoding\n",
    "        try:\n",
    "            encoding = self.detect_encoding(file_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Encoding detection failed: {e}. Trying latin-1...\")\n",
    "            encoding = 'latin-1'\n",
    "        \n",
    "        # Try loading with detected encoding\n",
    "        encodings_to_try = [encoding, 'utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "        \n",
    "        for enc in encodings_to_try:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc)\n",
    "                logger.info(f\"‚úì Successfully loaded with encoding: {enc}\")\n",
    "                self.original_df = df\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed with {enc}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        raise ValueError(\"Could not load CSV with any common encoding\")\n",
    "    \n",
    "    def extract_relevant_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract and rename relevant columns from Brazilian dataset\n",
    "        \n",
    "        Original columns:\n",
    "        - Process number RM, Name RM, Status RM, etc. (Registered Mark)\n",
    "        - Process number TM, Name TM, Status TM, etc. (Target Mark - the contested one)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Map to standardized column names\n",
    "        column_mapping = {\n",
    "            # Pair identification\n",
    "            'Process number RM': 'mark1_id',\n",
    "            'Process number TM': 'mark2_id',\n",
    "            \n",
    "            # Wordmarks (the actual text)\n",
    "            'Name RM': 'mark1_wordmark',\n",
    "            'Name TM': 'mark2_wordmark',\n",
    "            \n",
    "            # Status\n",
    "            'Status RM': 'mark1_status',\n",
    "            'Status TM': 'mark2_status',\n",
    "            \n",
    "            # Classification\n",
    "            'Nice classification RM': 'mark1_class',\n",
    "            'Nice classification TM': 'mark2_class',\n",
    "            \n",
    "            # Presentation (Nominative/Mixed/Figurative)\n",
    "            'Presentation RM': 'mark1_presentation',\n",
    "            'Presentation TM': 'mark2_presentation',\n",
    "            \n",
    "            # Date\n",
    "            'Application date RM': 'mark1_date',\n",
    "            'Application date TM': 'mark2_date',\n",
    "            \n",
    "            # Complementary text (explains why similar)\n",
    "            'Complementary text': 'similarity_reason'\n",
    "        }\n",
    "        \n",
    "        # Select and rename columns\n",
    "        available_cols = [col for col in column_mapping.keys() if col in df.columns]\n",
    "        df_processed = df[available_cols].copy()\n",
    "        df_processed.rename(columns=column_mapping, inplace=True)\n",
    "        \n",
    "        # Add label: these pairs are SIMILAR (that's why they're in the rejection dataset)\n",
    "        df_processed['label'] = 1  # 1 = similar/confusing\n",
    "        \n",
    "        logger.info(f\"‚úì Extracted {len(df_processed)} trademark pairs\")\n",
    "        logger.info(f\"   Columns: {list(df_processed.columns)}\")\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean trademark text\n",
    "        - Remove extra whitespace\n",
    "        - Fix common encoding artifacts\n",
    "        - Normalize\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Fix common encoding issues\n",
    "        encoding_fixes = {\n",
    "            'vÔøΩ': '√ß',\n",
    "            'vÔøΩo': '√ß√£o',\n",
    "            'vÔøΩ': '√≠',\n",
    "            'v=': '√≥',\n",
    "            '?': '',  # Remove question marks from encoding errors\n",
    "        }\n",
    "        \n",
    "        for bad, good in encoding_fixes.items():\n",
    "            text = text.replace(bad, good)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def translate_with_cache(self, text: str, translator, cache_key: str) -> str:\n",
    "        \"\"\"\n",
    "        Translate text with caching to avoid redundant API calls\n",
    "        \"\"\"\n",
    "        if not text or text == '':\n",
    "            return ''\n",
    "        \n",
    "        # Check cache\n",
    "        cache_id = f\"{cache_key}:{text}\"\n",
    "        if cache_id in self.translation_cache:\n",
    "            return self.translation_cache[cache_id]\n",
    "        \n",
    "        # Translate\n",
    "        try:\n",
    "            translation = translator.translate(text)\n",
    "            self.translation_cache[cache_id] = translation\n",
    "            return translation\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Translation failed for '{text[:50]}...': {e}\")\n",
    "            return text  # Return original on failure\n",
    "    \n",
    "    def translate_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Translate wordmarks: Portuguese ‚Üí English ‚Üí [Hausa/Yoruba via lexicon + fallback]\n",
    "        \"\"\"\n",
    "        logger.info(\"üåç Starting translation pipeline...\")\n",
    "        \n",
    "        wordmark_cols = ['mark1_wordmark', 'mark2_wordmark']\n",
    "        \n",
    "        for col in wordmark_cols:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"   Translating {col}...\")\n",
    "            \n",
    "            # Portuguese ‚Üí English (unchanged)\n",
    "            if self.config.translate_to_english:\n",
    "                en_col = col.replace('_wordmark', '_wordmark_en')\n",
    "                df[en_col] = df[col].apply(\n",
    "                    lambda x: self.translate_with_cache(\n",
    "                        self.clean_text(x), \n",
    "                        self.pt_to_en, \n",
    "                        'pt_en'\n",
    "                    )\n",
    "                )\n",
    "                logger.info(f\"      ‚úì Portuguese ‚Üí English: {en_col}\")\n",
    "            \n",
    "            # English ‚Üí Hausa & Yoruba (enhanced)\n",
    "            if self.config.translate_to_local:\n",
    "                # Hausa\n",
    "                ha_col = col.replace('_wordmark', '_wordmark_ha')\n",
    "                df[ha_col] = df[en_col].apply(\n",
    "                    lambda x: self.translate_text_lexicon_first(str(x), 'ha')\n",
    "                )\n",
    "                logger.info(f\"      ‚úì English ‚Üí Hausa (lexicon+fallback): {ha_col}\")\n",
    "                \n",
    "                # Yoruba\n",
    "                yo_col = col.replace('_wordmark', '_wordmark_yo')\n",
    "                df[yo_col] = df[en_col].apply(\n",
    "                    lambda x: self.translate_text_lexicon_first(str(x), 'yo')\n",
    "                )\n",
    "                logger.info(f\"      ‚úì English ‚Üí Yoruba (lexicon+fallback): {yo_col}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values based on configuration\n",
    "        \"\"\"\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        if self.config.handle_missing == 'drop':\n",
    "            # Drop rows where critical columns are missing\n",
    "            critical_cols = ['mark1_wordmark', 'mark2_wordmark']\n",
    "            df = df.dropna(subset=critical_cols)\n",
    "            removed = initial_count - len(df)\n",
    "            if removed > 0:\n",
    "                logger.info(f\"‚ö† Removed {removed} rows with missing wordmarks\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove duplicate trademark pairs\n",
    "        \"\"\"\n",
    "        if not self.config.remove_duplicates:\n",
    "            return df\n",
    "        \n",
    "        initial_count = len(df)\n",
    "        \n",
    "        # Sort pairs to avoid (A,B) vs (B,A) duplicates\n",
    "        df['pair_key'] = df.apply(\n",
    "            lambda row: tuple(sorted([\n",
    "                str(row['mark1_wordmark']), \n",
    "                str(row['mark2_wordmark'])\n",
    "            ])),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        df = df.drop_duplicates(subset='pair_key')\n",
    "        df = df.drop('pair_key', axis=1)\n",
    "        \n",
    "        removed = initial_count - len(df)\n",
    "        if removed > 0:\n",
    "            logger.info(f\"‚ö† Removed {removed} duplicate pairs\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add basic features for later use\n",
    "        \"\"\"\n",
    "        # Length features\n",
    "        df['mark1_length'] = df['mark1_wordmark'].str.len()\n",
    "        df['mark2_length'] = df['mark2_wordmark'].str.len()\n",
    "        \n",
    "        # Same class indicator\n",
    "        df['same_class'] = (df['mark1_class'] == df['mark2_class']).astype(int)\n",
    "        \n",
    "        # Length difference\n",
    "        df['length_diff'] = abs(df['mark1_length'] - df['mark2_length'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _sample_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample the dataframe based on config: first N, random N, or all.\n",
    "        Applied AFTER loading but BEFORE heavy processing (cleaning, translation).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.config.max_rows is not None and self.config.max_rows <= 0:\n",
    "            raise ValueError(\"max_rows must be positive integer or None\")\n",
    "        \n",
    "        if self.config.sampling_strategy == 'all' or self.config.max_rows is None:\n",
    "            logger.info(\"üìù Sampling strategy: using all rows\")\n",
    "            return df\n",
    "\n",
    "        n = min(self.config.max_rows, len(df))\n",
    "        \n",
    "        if self.config.sampling_strategy == 'first':\n",
    "            sampled_df = df.head(n).copy()\n",
    "            logger.info(f\"üìù Sampling strategy: first {n} rows\")\n",
    "            \n",
    "        elif self.config.sampling_strategy == 'random':\n",
    "            sampled_df = df.sample(n=n, random_state=42).copy()  # reproducible\n",
    "            logger.info(f\"üìù Sampling strategy: random {n} rows (seed=42)\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling strategy: {self.config.sampling_strategy}\")\n",
    "        \n",
    "        return sampled_df\n",
    "\n",
    "    def process(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Full preprocessing pipeline\n",
    "        \"\"\"\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"BRAZILIAN TRADEMARK DATASET PREPROCESSING\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # 1. Load with encoding fix\n",
    "        logger.info(\"\\n1Ô∏è‚É£ Loading dataset...\")\n",
    "        df = self.load_and_fix_encoding(file_path)\n",
    "        logger.info(f\"   Loaded: {df.shape}\")\n",
    "        \n",
    "        # 2. Extract relevant columns\n",
    "        logger.info(\"\\n2Ô∏è‚É£ Extracting relevant columns...\")\n",
    "        df = self.extract_relevant_columns(df)\n",
    "        \n",
    "        # >>> NEW: Apply sampling here <<<\n",
    "        logger.info(\"\\n3Ô∏è‚É£ Applying sampling strategy...\")\n",
    "        df = self._sample_dataframe(df)\n",
    "        logger.info(f\"   After sampling: {df.shape}\")\n",
    "\n",
    "        # 3. Clean text\n",
    "        logger.info(\"\\n3Ô∏è‚É£ Cleaning text...\")\n",
    "        for col in df.columns:\n",
    "            if 'wordmark' in col or 'reason' in col:\n",
    "                df[col] = df[col].apply(self.clean_text)\n",
    "        \n",
    "        # 4. Handle missing values\n",
    "        logger.info(\"\\n4Ô∏è‚É£ Handling missing values...\")\n",
    "        df = self.handle_missing_values(df)\n",
    "        \n",
    "        # 5. Remove duplicates\n",
    "        logger.info(\"\\n5Ô∏è‚É£ Removing duplicates...\")\n",
    "        df = self.remove_duplicates(df)\n",
    "        \n",
    "        # 6. Translate\n",
    "        logger.info(\"\\n6Ô∏è‚É£ Translating dataset...\")\n",
    "        df = self.translate_dataset(df)\n",
    "        \n",
    "        # 7. Add features\n",
    "        logger.info(\"\\n7Ô∏è‚É£ Adding features...\")\n",
    "        df = self.add_features(df)\n",
    "        \n",
    "        self.processed_df = df\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(f\"‚úÖ PREPROCESSING COMPLETE\")\n",
    "        logger.info(f\"   Final dataset: {df.shape}\")\n",
    "        logger.info(f\"   Columns: {list(df.columns)}\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # After processing, show translation stats\n",
    "        stats = self.enhanced_translator.get_stats()\n",
    "        logger.info(\"\\nüìä Translation Statistics:\")\n",
    "        logger.info(f\"   Lexicon hits: {stats['lexicon']} ({stats['lexicon_pct']:.1f}%)\")\n",
    "        logger.info(f\"   HuggingFace: {stats['huggingface']} ({stats['hf_pct']:.1f}%)\")\n",
    "        logger.info(f\"   Google Translate: {stats['google']} ({stats['google_pct']:.1f}%)\")\n",
    "        logger.info(f\"   Cache hits: {stats['cache']} ({stats['cache_pct']:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_sample_report(self, n: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get sample of processed data for inspection\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            raise ValueError(\"No processed data. Run process() first.\")\n",
    "        \n",
    "        cols_to_show = [\n",
    "            'mark1_wordmark', 'mark1_wordmark_en', 'mark1_wordmark_ha', 'mark1_wordmark_yo',\n",
    "            'mark2_wordmark', 'mark2_wordmark_en', 'mark2_wordmark_ha', 'mark2_wordmark_yo',\n",
    "            'label', 'same_class'\n",
    "        ]\n",
    "        \n",
    "        available_cols = [col for col in cols_to_show if col in self.processed_df.columns]\n",
    "        \n",
    "        return self.processed_df[available_cols].head(n)\n",
    "    \n",
    "    def save_processed(self, output_path: str):\n",
    "        \"\"\"\n",
    "        Save processed dataset\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            raise ValueError(\"No processed data to save\")\n",
    "        \n",
    "        self.processed_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"‚úÖ Saved processed dataset to: {output_path}\")\n",
    "\n",
    "\n",
    "    # Add these methods to the BrazilianTrademarkPreprocessor class\n",
    "\n",
    "    def generate_negative_pairs(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        ratio: float = 1.0,\n",
    "        strategy: str = 'mixed'  # 'random', 'class_based', 'mixed'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate negative (dissimilar) pairs from positive pairs\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with positive pairs (label=1)\n",
    "            ratio: How many negative pairs to generate (1.0 = same as positive)\n",
    "            strategy: 'random', 'class_based', or 'mixed'\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with negative pairs (label=0)\n",
    "        \"\"\"\n",
    "        logger.info(f\"\\nüîÑ Generating negative pairs (strategy: {strategy}, ratio: {ratio})...\")\n",
    "        \n",
    "        # Get unique wordmarks for each mark position\n",
    "        mark1_pool = df['mark1_wordmark'].unique()\n",
    "        mark2_pool = df['mark2_wordmark'].unique()\n",
    "        \n",
    "        # Combine all unique wordmarks and filter out empty/invalid ones\n",
    "        all_wordmarks = list(set(list(mark1_pool) + list(mark2_pool)))\n",
    "        all_wordmarks = [\n",
    "            str(w).strip() for w in all_wordmarks \n",
    "            if pd.notna(w) and str(w).strip() and len(str(w).strip()) > 0\n",
    "        ]\n",
    "        \n",
    "        if len(all_wordmarks) < 2:\n",
    "            logger.error(f\"‚ùå Not enough valid wordmarks to generate pairs. Found: {len(all_wordmarks)}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        logger.info(f\"   Valid wordmarks available: {len(all_wordmarks)}\")\n",
    "        \n",
    "        # Get class information if available\n",
    "        mark1_classes = df.set_index('mark1_wordmark')['mark1_class'].to_dict()\n",
    "        mark2_classes = df.set_index('mark2_wordmark')['mark2_class'].to_dict()\n",
    "        all_classes = {**mark1_classes, **mark2_classes}\n",
    "        \n",
    "        # Calculate number of negative pairs needed\n",
    "        n_positive = len(df)\n",
    "        n_negative = int(n_positive * ratio)\n",
    "        \n",
    "        logger.info(f\"   Positive pairs: {n_positive}\")\n",
    "        logger.info(f\"   Target negative pairs: {n_negative}\")\n",
    "        \n",
    "        negative_pairs = []\n",
    "        max_attempts = n_negative * 50  # Increased attempts for better success rate\n",
    "        attempts = 0\n",
    "        consecutive_fails = 0\n",
    "        max_consecutive_fails = 100\n",
    "        \n",
    "        # Track existing pairs to avoid duplicates\n",
    "        existing_pairs = set()\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                mark1_str = str(row['mark1_wordmark']).strip()\n",
    "                mark2_str = str(row['mark2_wordmark']).strip()\n",
    "                if mark1_str and mark2_str:\n",
    "                    pair = tuple(sorted([mark1_str, mark2_str]))\n",
    "                    existing_pairs.add(pair)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Skipping invalid pair: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"   Existing pairs to avoid: {len(existing_pairs)}\")\n",
    "        \n",
    "        while len(negative_pairs) < n_negative and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "            \n",
    "            try:\n",
    "                # Random selection\n",
    "                mark1 = np.random.choice(all_wordmarks)\n",
    "                mark2 = np.random.choice(all_wordmarks)\n",
    "                \n",
    "                # Validate marks\n",
    "                if not mark1 or not mark2 or not isinstance(mark1, str) or not isinstance(mark2, str):\n",
    "                    consecutive_fails += 1\n",
    "                    continue\n",
    "                \n",
    "                mark1 = mark1.strip()\n",
    "                mark2 = mark2.strip()\n",
    "                \n",
    "                # Skip if empty\n",
    "                if not mark1 or not mark2:\n",
    "                    consecutive_fails += 1\n",
    "                    continue\n",
    "                \n",
    "                # Skip if same\n",
    "                if mark1.lower() == mark2.lower():\n",
    "                    consecutive_fails += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create sorted pair to check for duplicates\n",
    "                pair_key = tuple(sorted([mark1, mark2]))\n",
    "                \n",
    "                # Skip if already exists (either as positive or already generated negative)\n",
    "                if pair_key in existing_pairs:\n",
    "                    consecutive_fails += 1\n",
    "                    continue\n",
    "                \n",
    "                # Apply strategy-specific filtering\n",
    "                if strategy == 'class_based':\n",
    "                    # Only accept if different classes\n",
    "                    class1 = all_classes.get(mark1, 0)\n",
    "                    class2 = all_classes.get(mark2, 0)\n",
    "                    if class1 == class2:\n",
    "                        consecutive_fails += 1\n",
    "                        continue\n",
    "                \n",
    "                elif strategy == 'mixed':\n",
    "                    # Accept if: different class OR very different length OR different first letter\n",
    "                    class1 = all_classes.get(mark1, 0)\n",
    "                    class2 = all_classes.get(mark2, 0)\n",
    "                    len1 = len(mark1)\n",
    "                    len2 = len(mark2)\n",
    "                    \n",
    "                    # Safely get first character\n",
    "                    try:\n",
    "                        first1 = mark1[0].lower() if len(mark1) > 0 else ''\n",
    "                        first2 = mark2[0].lower() if len(mark2) > 0 else ''\n",
    "                    except (IndexError, AttributeError):\n",
    "                        first1 = ''\n",
    "                        first2 = ''\n",
    "                    \n",
    "                    # Must satisfy at least one dissimilarity criterion\n",
    "                    if not (class1 != class2 or abs(len1 - len2) > 3 or (first1 and first2 and first1 != first2)):\n",
    "                        consecutive_fails += 1\n",
    "                        continue\n",
    "                \n",
    "                # If we get here, it's a valid negative pair\n",
    "                existing_pairs.add(pair_key)\n",
    "                consecutive_fails = 0  # Reset counter\n",
    "                \n",
    "                negative_pairs.append({\n",
    "                    'mark1_wordmark': mark1,\n",
    "                    'mark2_wordmark': mark2,\n",
    "                    'mark1_class': all_classes.get(mark1, 0),\n",
    "                    'mark2_class': all_classes.get(mark2, 0),\n",
    "                    'label': 0,  # Dissimilar\n",
    "                    'pair_type': 'negative'\n",
    "                })\n",
    "                \n",
    "                # Progress indicator\n",
    "                if len(negative_pairs) % 10 == 0:\n",
    "                    logger.debug(f\"   Generated {len(negative_pairs)}/{n_negative} pairs (attempt {attempts})\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error generating pair at attempt {attempts}: {e}\")\n",
    "                consecutive_fails += 1\n",
    "                continue\n",
    "            \n",
    "            # Safety check: if too many consecutive failures, stop\n",
    "            if consecutive_fails >= max_consecutive_fails:\n",
    "                logger.warning(f\"‚ö† Stopping after {consecutive_fails} consecutive failures\")\n",
    "                break\n",
    "        \n",
    "        if len(negative_pairs) == 0:\n",
    "            logger.error(f\"‚ùå Failed to generate any negative pairs after {attempts} attempts\")\n",
    "            logger.error(f\"   Available wordmarks: {len(all_wordmarks)}\")\n",
    "            logger.error(f\"   Sample wordmarks: {all_wordmarks[:5] if len(all_wordmarks) >= 5 else all_wordmarks}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if len(negative_pairs) < n_negative:\n",
    "            logger.warning(f\"‚ö† Only generated {len(negative_pairs)}/{n_negative} negative pairs after {attempts} attempts\")\n",
    "        else:\n",
    "            logger.info(f\"‚úì Generated {len(negative_pairs)} negative pairs in {attempts} attempts\")\n",
    "        \n",
    "        negative_df = pd.DataFrame(negative_pairs)\n",
    "        return negative_df\n",
    "\n",
    "\n",
    "    def create_balanced_dataset(\n",
    "        self,\n",
    "        positive_df: pd.DataFrame,\n",
    "        negative_ratio: float = 1.0,\n",
    "        strategy: str = 'mixed'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a balanced dataset with both positive and negative pairs\n",
    "        \n",
    "        Args:\n",
    "            positive_df: DataFrame with positive pairs\n",
    "            negative_ratio: Ratio of negative to positive pairs\n",
    "            strategy: Strategy for generating negatives\n",
    "        \n",
    "        Returns:\n",
    "            Combined balanced dataset (or just positive_df if negative generation fails)\n",
    "        \"\"\"\n",
    "        logger.info(\"\\nüìä Creating balanced dataset...\")\n",
    "        \n",
    "        # Validate input\n",
    "        if positive_df is None or len(positive_df) == 0:\n",
    "            logger.error(\"‚ùå No positive pairs provided\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Add pair_type to positive pairs\n",
    "        positive_df = positive_df.copy()\n",
    "        positive_df['pair_type'] = 'positive'\n",
    "        \n",
    "        # Generate negative pairs\n",
    "        try:\n",
    "            negative_df = self.generate_negative_pairs(\n",
    "                positive_df, \n",
    "                ratio=negative_ratio, \n",
    "                strategy=strategy\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to generate negative pairs: {e}\")\n",
    "            logger.warning(\"‚ö† Returning only positive pairs\")\n",
    "            return positive_df\n",
    "        \n",
    "        # Check if negative generation succeeded\n",
    "        if negative_df is None or len(negative_df) == 0:\n",
    "            logger.warning(\"‚ö† No negative pairs generated. Returning only positive pairs\")\n",
    "            return positive_df\n",
    "        \n",
    "        # Align columns\n",
    "        # For negative pairs, we don't have English/HA/YO translations yet\n",
    "        # So we'll need to translate them too\n",
    "        if 'mark1_wordmark_en' in positive_df.columns:\n",
    "            try:\n",
    "                logger.info(\"   Translating negative pairs...\")\n",
    "                negative_df = self._translate_negative_pairs(negative_df)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Translation failed for negative pairs: {e}\")\n",
    "                logger.warning(\"‚ö† Returning only positive pairs\")\n",
    "                return positive_df\n",
    "        \n",
    "        # Add missing columns from positive pairs\n",
    "        for col in positive_df.columns:\n",
    "            if col not in negative_df.columns:\n",
    "                try:\n",
    "                    if 'mark1' in col and col != 'mark1_wordmark' and col != 'mark1_class':\n",
    "                        negative_df[col] = negative_df['mark1_wordmark'].map(\n",
    "                            positive_df.set_index('mark1_wordmark')[col].to_dict()\n",
    "                        )\n",
    "                    elif 'mark2' in col and col != 'mark2_wordmark' and col != 'mark2_class':\n",
    "                        negative_df[col] = negative_df['mark2_wordmark'].map(\n",
    "                            positive_df.set_index('mark2_wordmark')[col].to_dict()\n",
    "                        )\n",
    "                    else:\n",
    "                        negative_df[col] = None\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Could not map column {col}: {e}\")\n",
    "                    negative_df[col] = None\n",
    "        \n",
    "        # Combine\n",
    "        try:\n",
    "            balanced_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "            \n",
    "            # Shuffle\n",
    "            balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            logger.info(f\"‚úì Balanced dataset created:\")\n",
    "            logger.info(f\"   Positive pairs: {len(positive_df)} ({len(positive_df)/len(balanced_df)*100:.1f}%)\")\n",
    "            logger.info(f\"   Negative pairs: {len(negative_df)} ({len(negative_df)/len(balanced_df)*100:.1f}%)\")\n",
    "            logger.info(f\"   Total: {len(balanced_df)}\")\n",
    "            \n",
    "            return balanced_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to combine datasets: {e}\")\n",
    "            logger.warning(\"‚ö† Returning only positive pairs\")\n",
    "            return positive_df\n",
    "\n",
    "\n",
    "    def _translate_negative_pairs(self, negative_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Translate negative pairs using the same pipeline as positive pairs\n",
    "        \"\"\"\n",
    "        if negative_df is None or len(negative_df) == 0:\n",
    "            return negative_df\n",
    "        \n",
    "        negative_df = negative_df.copy()\n",
    "        \n",
    "        try:\n",
    "            # Build translation dictionaries from cache\n",
    "            for col in ['mark1_wordmark', 'mark2_wordmark']:\n",
    "                base_col = col\n",
    "                \n",
    "                try:\n",
    "                    # English\n",
    "                    en_col = col.replace('_wordmark', '_wordmark_en')\n",
    "                    negative_df[en_col] = negative_df[base_col].apply(\n",
    "                        lambda x: self.translate_with_cache(\n",
    "                            self.clean_text(str(x)), \n",
    "                            self.pt_to_en, \n",
    "                            'pt_en'\n",
    "                        ) if pd.notna(x) and str(x).strip() else ''\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"English translation failed for {col}: {e}\")\n",
    "                    negative_df[en_col] = negative_df[base_col]\n",
    "                \n",
    "                try:\n",
    "                    # Hausa\n",
    "                    ha_col = col.replace('_wordmark', '_wordmark_ha')\n",
    "                    negative_df[ha_col] = negative_df[en_col].apply(\n",
    "                        lambda x: self.translate_text_lexicon_first(str(x), 'ha') \n",
    "                        if pd.notna(x) and str(x).strip() else ''\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Hausa translation failed for {col}: {e}\")\n",
    "                    negative_df[ha_col] = negative_df[en_col]\n",
    "                \n",
    "                try:\n",
    "                    # Yoruba\n",
    "                    yo_col = col.replace('_wordmark', '_wordmark_yo')\n",
    "                    negative_df[yo_col] = negative_df[en_col].apply(\n",
    "                        lambda x: self.translate_text_lexicon_first(str(x), 'yo')\n",
    "                        if pd.notna(x) and str(x).strip() else ''\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Yoruba translation failed for {col}: {e}\")\n",
    "                    negative_df[yo_col] = negative_df[en_col]\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Critical error in translation: {e}\")\n",
    "            raise\n",
    "        \n",
    "        return negative_df\n",
    "\n",
    "\n",
    "    def analyze_dataset_balance(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze the balance and characteristics of the dataset\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"DATASET BALANCE ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Label distribution\n",
    "        label_dist = df['label'].value_counts()\n",
    "        print(f\"\\nüìä Label Distribution:\")\n",
    "        for label, count in label_dist.items():\n",
    "            label_name = \"Similar (Positive)\" if label == 1 else \"Dissimilar (Negative)\"\n",
    "            print(f\"   {label_name}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Pair type distribution\n",
    "        if 'pair_type' in df.columns:\n",
    "            pair_dist = df['pair_type'].value_counts()\n",
    "            print(f\"\\nüîÑ Pair Type Distribution:\")\n",
    "            for ptype, count in pair_dist.items():\n",
    "                print(f\"   {ptype.capitalize()}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Class overlap\n",
    "        same_class = df['same_class'].value_counts()\n",
    "        print(f\"\\nüè∑Ô∏è  Same Class Distribution:\")\n",
    "        for same, count in same_class.items():\n",
    "            class_label = \"Same Class\" if same == 1 else \"Different Class\"\n",
    "            print(f\"   {class_label}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Length statistics by label\n",
    "        print(f\"\\nüìè Length Statistics by Label:\")\n",
    "        for label in [0, 1]:\n",
    "            subset = df[df['label'] == label]\n",
    "            label_name = \"Positive\" if label == 1 else \"Negative\"\n",
    "            avg_len1 = subset['mark1_length'].mean()\n",
    "            avg_len2 = subset['mark2_length'].mean()\n",
    "            avg_diff = subset['length_diff'].mean()\n",
    "            print(f\"   {label_name} pairs:\")\n",
    "            print(f\"      Avg mark1 length: {avg_len1:.1f}\")\n",
    "            print(f\"      Avg mark2 length: {avg_len2:.1f}\")\n",
    "            print(f\"      Avg length diff: {avg_diff:.1f}\")\n",
    "        \n",
    "        return {\n",
    "            'total_pairs': len(df),\n",
    "            'positive_pairs': label_dist.get(1, 0),\n",
    "            'negative_pairs': label_dist.get(0, 0),\n",
    "            'balance_ratio': label_dist.get(0, 0) / label_dist.get(1, 1) if label_dist.get(1, 0) > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ENHANCED USAGE WITH NEGATIVE PAIRS\n",
    "# ===========================\n",
    "\n",
    "# Configure preprocessing\n",
    "config = PreprocessingConfig(\n",
    "    max_rows=5,  # Process first 100 rows (or use None for all)\n",
    "    sampling_strategy='random',  # 'first', 'random', or 'all'\n",
    "    fix_encoding=True,\n",
    "    translate_to_english=True,\n",
    "    translate_to_local=True,  # Translate to HA/YO\n",
    "    remove_duplicates=True,\n",
    "    handle_missing='drop'\n",
    ")\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = BrazilianTrademarkPreprocessor(config)\n",
    "\n",
    "# Process the dataset\n",
    "try:\n",
    "    # Step 1: Process positive pairs (similar trademarks)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1: PROCESSING POSITIVE PAIRS (Similar Trademarks)\")\n",
    "    print(\"=\" * 80)\n",
    "    positive_df = preprocessor.process('trademark_file.csv')\n",
    "    \n",
    "    if positive_df is None or len(positive_df) == 0:\n",
    "        raise ValueError(\"No positive pairs generated from preprocessing\")\n",
    "    \n",
    "    print(f\"\\n‚úì Positive pairs ready: {len(positive_df)}\")\n",
    "    \n",
    "    # Step 2: Generate negative pairs (dissimilar trademarks)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: GENERATING NEGATIVE PAIRS (Dissimilar Trademarks)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create balanced dataset with negative pairs\n",
    "    balanced_df = preprocessor.create_balanced_dataset(\n",
    "        positive_df,\n",
    "        negative_ratio=1.0,  # 1:1 ratio (equal positive and negative)\n",
    "        strategy='mixed'  # 'random', 'class_based', or 'mixed'\n",
    "    )\n",
    "    \n",
    "    if balanced_df is None or len(balanced_df) == 0:\n",
    "        raise ValueError(\"No balanced dataset generated\")\n",
    "    \n",
    "    # Check if we actually have both positive and negative\n",
    "    has_negatives = 0 in balanced_df['label'].values\n",
    "    \n",
    "    if not has_negatives:\n",
    "        logger.warning(\"‚ö† Dataset contains only positive pairs (no negatives generated)\")\n",
    "        logger.info(\"   This can still be used, but won't be balanced\")\n",
    "    \n",
    "    # Step 3: Analyze balance\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: ANALYZING DATASET BALANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    balance_stats = preprocessor.analyze_dataset_balance(balanced_df)\n",
    "    \n",
    "    # Step 4: Show sample from both classes\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 1 in balanced_df['label'].values:\n",
    "        print(\"\\nüü¢ POSITIVE PAIRS (Similar - Label=1):\")\n",
    "        positive_sample = balanced_df[balanced_df['label'] == 1].head(3)\n",
    "        print(positive_sample[['mark1_wordmark', 'mark2_wordmark', 'label', 'same_class']].to_string(index=False))\n",
    "    \n",
    "    if 0 in balanced_df['label'].values:\n",
    "        print(\"\\nüî¥ NEGATIVE PAIRS (Dissimilar - Label=0):\")\n",
    "        negative_sample = balanced_df[balanced_df['label'] == 0].head(3)\n",
    "        print(negative_sample[['mark1_wordmark', 'mark2_wordmark', 'label', 'same_class']].to_string(index=False))\n",
    "    \n",
    "    # Step 5: Save balanced dataset\n",
    "    base_output_file = 'trademark_data_balanced_multilingual.csv'\n",
    "    output_file = get_incremented_filename(base_output_file)\n",
    "\n",
    "    balanced_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"\\n‚úÖ Dataset saved to: {output_file}\")\n",
    "    print(f\"   Total pairs: {len(balanced_df)}\")\n",
    "    print(f\"   Columns: {len(balanced_df.columns)}\")\n",
    "    print(f\"   File size: {balanced_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Step 6: Save comprehensive summary statistics\n",
    "    summary = {\n",
    "        'dataset_info': {\n",
    "            'total_pairs': int(len(balanced_df)),\n",
    "            'positive_pairs': int(balance_stats['positive_pairs']),\n",
    "            'negative_pairs': int(balance_stats['negative_pairs']),\n",
    "            'balance_ratio': float(balance_stats['balance_ratio']),\n",
    "            'file_size_mb': float(balanced_df.memory_usage(deep=True).sum() / 1024**2),\n",
    "            'has_negative_pairs': has_negatives\n",
    "        },\n",
    "        'preprocessing': {\n",
    "            'languages': ['Portuguese', 'English', 'Hausa', 'Yoruba'],\n",
    "            'max_rows_processed': config.max_rows if config.max_rows else 'all',\n",
    "            'sampling_strategy': config.sampling_strategy,\n",
    "            'negative_generation_strategy': 'mixed',\n",
    "            'negative_ratio': 1.0\n",
    "        },\n",
    "        'columns': {\n",
    "            'total_columns': len(balanced_df.columns),\n",
    "            'column_list': list(balanced_df.columns)\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'same_class_pairs': int(balanced_df['same_class'].sum()),\n",
    "            'different_class_pairs': int((balanced_df['same_class'] == 0).sum()),\n",
    "            'same_class_percentage': float(balanced_df['same_class'].mean() * 100)\n",
    "        },\n",
    "        'length_statistics': {\n",
    "            'avg_mark1_length': float(balanced_df['mark1_length'].mean()),\n",
    "            'avg_mark2_length': float(balanced_df['mark2_length'].mean()),\n",
    "            'avg_length_diff': float(balanced_df['length_diff'].mean())\n",
    "        }\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open('dataset_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n‚úÖ Dataset ready for Step 1 (CNN + SVM training)\")\n",
    "    print(f\"   Use file: {output_file}\")\n",
    "    print(f\"   Summary saved to: dataset_summary.json\")\n",
    "\n",
    "    # Also print the summary\n",
    "    print(\"\\nüìÑ Summary:\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try to save whatever we have\n",
    "    if 'positive_df' in locals() and positive_df is not None and len(positive_df) > 0:\n",
    "        emergency_file = 'trademark_data_positive_only.csv'\n",
    "        positive_df.to_csv(emergency_file, index=False, encoding='utf-8')\n",
    "        print(f\"\\n‚ö† Saved positive pairs only to: {emergency_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c73459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (2.4.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.2.1-cp313-cp313-win_amd64.whl.metadata (642 bytes)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sentence-transformers) (4.57.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.4)\n",
      "Downloading scikit_learn-1.8.0-cp313-cp313-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/8.0 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.7/8.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/8.0 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 9.8 MB/s  0:00:00\n",
      "Downloading jellyfish-1.2.1-cp313-cp313-win_amd64.whl (213 kB)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading scipy-1.17.0-cp313-cp313-win_amd64.whl (36.3 MB)\n",
      "   ---------------------------------------- 0.0/36.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/36.3 MB 11.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 4.7/36.3 MB 11.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 6.8/36.3 MB 11.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 8.4/36.3 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 10.5/36.3 MB 10.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 12.6/36.3 MB 9.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 13.1/36.3 MB 9.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 14.9/36.3 MB 8.8 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.3/36.3 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 17.8/36.3 MB 8.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 19.7/36.3 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 21.0/36.3 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 22.5/36.3 MB 8.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 24.1/36.3 MB 8.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 26.0/36.3 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 27.8/36.3 MB 8.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 29.4/36.3 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 30.1/36.3 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 31.2/36.3 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 32.2/36.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 33.3/36.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 34.3/36.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  35.7/36.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 36.3/36.3 MB 7.3 MB/s  0:00:04\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, jellyfish, scikit-learn, sentence-transformers\n",
      "\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   ---------------- ----------------------- 2/5 [jellyfish]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   ---------------------------------------- 5/5 [sentence-transformers]\n",
      "\n",
      "Successfully installed jellyfish-1.2.1 scikit-learn-1.8.0 scipy-1.17.0 sentence-transformers-5.2.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn jellyfish sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cdf5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.4.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (12.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bubashir\\trademark-similarity-engine\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 6.5 MB/s  0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16c58257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting visual features...\n",
      "üëÇ Extracting phonetic features...\n",
      "üß† Extracting semantic features (English)...\n",
      "üß† Extracting semantic features (Hausa)...\n",
      "üß† Extracting semantic features (Yoruba)...\n",
      "\n",
      "‚úÖ Feature extraction complete!\n",
      "   New feature columns (7): ['visual_levenshtein', 'visual_jaro_winkler', 'soundex_match', 'metaphone_match', 'semantic_similarity_en', 'semantic_similarity_ha', 'semantic_similarity_yo']\n",
      "\n",
      "üíæ Enhanced dataset saved to: trademark_data_balanced_multilingual_with_features.csv\n",
      "   Total rows: 8\n",
      "   Total columns: 32\n",
      "   New feature columns: ['visual_levenshtein', 'visual_jaro_winkler', 'soundex_match', 'semantic_similarity_en', 'semantic_similarity_ha', 'semantic_similarity_yo']\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# STEP 1.5: FEATURE ENGINEERING\n",
    "# ===========================\n",
    "\n",
    "import pandas as pd\n",
    "import jellyfish  # For string distance + phonetic algorithms\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class AdvancedFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract sight/sound/meaning features aligned with legal standards\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load once, reuse\n",
    "        self.semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    def _safe_encode(self, texts: list) -> np.ndarray:\n",
    "        \"\"\"Handle NaN/empty strings during encoding\"\"\"\n",
    "        clean_texts = []\n",
    "        for t in texts:\n",
    "            if pd.isna(t) or t == '' or t is None:\n",
    "                clean_texts.append(\"\")  # Model handles empty string gracefully\n",
    "            else:\n",
    "                clean_texts.append(str(t))\n",
    "        return self.semantic_model.encode(clean_texts, show_progress_bar=False)\n",
    "    \n",
    "    def extract_all(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract all features\"\"\"\n",
    "        df = df.copy()  # Avoid modifying original\n",
    "        \n",
    "        print(\"üîç Extracting visual features...\")\n",
    "        # 1. SIGHT (Visual) - Character-level\n",
    "        df['visual_levenshtein'] = df.apply(\n",
    "            lambda row: jellyfish.levenshtein_distance(\n",
    "                str(row['mark1_wordmark']), str(row['mark2_wordmark'])\n",
    "            ), axis=1\n",
    "        )\n",
    "        df['visual_jaro_winkler'] = df.apply(\n",
    "            lambda row: jellyfish.jaro_winkler_similarity(\n",
    "                str(row['mark1_wordmark']), str(row['mark2_wordmark'])\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        print(\"üëÇ Extracting phonetic features...\")\n",
    "        # 2. SOUND (Phonetic) ‚Äì using jellyfish\n",
    "        df['soundex_match'] = df.apply(\n",
    "            lambda row: int(\n",
    "                jellyfish.soundex(str(row['mark1_wordmark'])) == \n",
    "                jellyfish.soundex(str(row['mark2_wordmark']))\n",
    "            ), axis=1\n",
    "        )\n",
    "        df['metaphone_match'] = df.apply(\n",
    "            lambda row: int(\n",
    "                jellyfish.metaphone(str(row['mark1_wordmark'])) == \n",
    "                jellyfish.metaphone(str(row['mark2_wordmark']))\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        print(\"üß† Extracting semantic features (English)...\")\n",
    "        # 3. MEANING (Semantic) ‚Äì English\n",
    "        mark1_emb_en = self._safe_encode(df['mark1_wordmark_en'].tolist())\n",
    "        mark2_emb_en = self._safe_encode(df['mark2_wordmark_en'].tolist())\n",
    "        df['semantic_similarity_en'] = [\n",
    "            float(cosine_similarity([e1], [e2])[0][0]) \n",
    "            for e1, e2 in zip(mark1_emb_en, mark2_emb_en)\n",
    "        ]\n",
    "        \n",
    "        print(\"üß† Extracting semantic features (Hausa)...\")\n",
    "        mark1_emb_ha = self._safe_encode(df['mark1_wordmark_ha'].tolist())\n",
    "        mark2_emb_ha = self._safe_encode(df['mark2_wordmark_ha'].tolist())\n",
    "        df['semantic_similarity_ha'] = [\n",
    "            float(cosine_similarity([e1], [e2])[0][0]) \n",
    "            for e1, e2 in zip(mark1_emb_ha, mark2_emb_ha)\n",
    "        ]\n",
    "        \n",
    "        print(\"üß† Extracting semantic features (Yoruba)...\")\n",
    "        mark1_emb_yo = self._safe_encode(df['mark1_wordmark_yo'].tolist())\n",
    "        mark2_emb_yo = self._safe_encode(df['mark2_wordmark_yo'].tolist())\n",
    "        df['semantic_similarity_yo'] = [\n",
    "            float(cosine_similarity([e1], [e2])[0][0]) \n",
    "            for e1, e2 in zip(mark1_emb_yo, mark2_emb_yo)\n",
    "        ]\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Usage\n",
    "feature_extractor = AdvancedFeatureExtractor()\n",
    "balanced_df = feature_extractor.extract_all(balanced_df)\n",
    "\n",
    "print(\"\\n‚úÖ Feature extraction complete!\")\n",
    "new_features = [col for col in balanced_df.columns if any(kw in col for kw in ['visual', 'soundex', 'metaphone', 'semantic'])]\n",
    "print(f\"   New feature columns ({len(new_features)}): {new_features}\")\n",
    "\n",
    "# Use a descriptive name\n",
    "output_file = get_incremented_filename('trademark_data_balanced_multilingual_with_features.csv')\n",
    "balanced_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nüíæ Enhanced dataset saved to: {output_file}\")\n",
    "print(f\"   Total rows: {len(balanced_df)}\")\n",
    "print(f\"   Total columns: {len(balanced_df.columns)}\")\n",
    "print(f\"   New feature columns: {[col for col in balanced_df.columns if 'visual' in col or 'soundex' in col or 'semantic' in col]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
