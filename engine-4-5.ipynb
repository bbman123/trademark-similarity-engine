{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d921c3b",
   "metadata": {},
   "source": [
    "# Trademark Similarity Engine: Hybrid CNN + SVM with Multilingual Linguistic AI\n",
    "\n",
    "## PhD Project Overview\n",
    "This notebook implements a trademark similarity classifier/ranker that identifies \"confusingly similar\" marks using:\n",
    "- **Hybrid Architecture**: Character-level CNN for feature extraction + SVM for classification\n",
    "- **Multilingual Support**: English, Hausa, and Yoruba\n",
    "- **Advanced Linguistic Features**: Synonyms, antonyms, phonetic similarity, cross-lingual equivalents\n",
    "\n",
    "## Project Structure\n",
    "1. Data Loading & Exploration\n",
    "2. Text Preprocessing Pipeline\n",
    "3. CNN Architecture & Training\n",
    "4. Linguistic Feature Engineering (Synonyms, Phonetics, Translations)\n",
    "5. Feature Combination & SVM Training\n",
    "6. Evaluation & Metrics\n",
    "7. Similarity Ranking Prototype\n",
    "8. Explainability Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78120d",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Exploration\n",
    "\n",
    "Let's begin by loading the trademark dataset and understanding its structure, distributions, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trademark dataset\n",
    "# TODO: Replace with your actual dataset path\n",
    "# Expected columns: 'wordmark', 'class', 'goods_services', 'label', 'pair_id' (optional)\n",
    "\n",
    "# For demonstration, create a sample structure\n",
    "# In production, use: df = pd.read_csv('your_trademark_data.csv')\n",
    "\n",
    "sample_data = {\n",
    "    'wordmark': ['TechFlow', 'TekFlow', 'DataSync', 'DataLink', 'QuickMart', 'FastMart'],\n",
    "    'class': [9, 9, 42, 42, 35, 35],\n",
    "    'goods_services': ['Software', 'Software products', 'IT Services', 'Data services', \n",
    "                       'Retail', 'Retail store'],\n",
    "    'label': [1, 1, 1, 1, 1, 1],  # 1 = similar pairs\n",
    "    'pair_id': ['A', 'A', 'B', 'B', 'C', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(\"Dataset Structure:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab01591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal records: {len(df)}\")\n",
    "print(f\"Unique wordmarks: {df['wordmark'].nunique()}\")\n",
    "print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "\n",
    "print(\"\\n--- Label Distribution ---\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\n--- Class Distribution ---\")\n",
    "print(df['class'].value_counts().head(10))\n",
    "\n",
    "print(\"\\n--- Wordmark Length Statistics ---\")\n",
    "df['wordmark_length'] = df['wordmark'].str.len()\n",
    "print(df['wordmark_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0bb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Label distribution\n",
    "df['label'].value_counts().plot(kind='bar', ax=axes[0, 0], color='steelblue')\n",
    "axes[0, 0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Label')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Class distribution (top 10)\n",
    "df['class'].value_counts().head(10).plot(kind='barh', ax=axes[0, 1], color='coral')\n",
    "axes[0, 1].set_title('Top 10 Trademark Classes', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Count')\n",
    "axes[0, 1].set_ylabel('Class')\n",
    "\n",
    "# Wordmark length distribution\n",
    "axes[1, 0].hist(df['wordmark_length'], bins=30, color='mediumseagreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Wordmark Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Length (characters)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Character distribution (first character)\n",
    "first_chars = df['wordmark'].str[0].str.upper().value_counts().head(15)\n",
    "first_chars.plot(kind='bar', ax=axes[1, 1], color='mediumpurple')\n",
    "axes[1, 1].set_title('Top 15 Starting Characters', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Character')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b899fb9",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing Pipeline\n",
    "\n",
    "Build robust preprocessing functions that normalize text while preserving original forms for phonetic analysis and language detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for language detection\n",
    "# Run this cell once\n",
    "# !pip install langdetect nltk textblob unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba906cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0  # For reproducibility\n",
    "\n",
    "# Text preprocessing functions\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing for trademark text.\n",
    "    Maintains both normalized and original forms.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text, preserve_case=False):\n",
    "        \"\"\"\n",
    "        Normalize text: lowercasing, punctuation removal, unicode cleanup\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Unicode normalization (NFKD)\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        # Remove accents while preserving characters\n",
    "        text = unidecode(text)\n",
    "        \n",
    "        # Remove special characters except spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Lowercase unless preservation is requested\n",
    "        if not preserve_case:\n",
    "            text = text.lower()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_language(text):\n",
    "        \"\"\"\n",
    "        Detect language of text (EN, HA, YO, or other)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            # Map language codes\n",
    "            lang_map = {\n",
    "                'en': 'English',\n",
    "                'ha': 'Hausa',\n",
    "                'yo': 'Yoruba'\n",
    "            }\n",
    "            return lang_map.get(lang, lang)\n",
    "        except:\n",
    "            return 'unknown'\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_character_ngrams(text, n=3):\n",
    "        \"\"\"\n",
    "        Extract character n-grams for similarity comparison\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_trademark(wordmark):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline for a single trademark\n",
    "        Returns dict with original, normalized, and metadata\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'original': wordmark,\n",
    "            'normalized': TextPreprocessor.normalize_text(wordmark),\n",
    "            'normalized_preserved_case': TextPreprocessor.normalize_text(wordmark, preserve_case=True),\n",
    "            'length': len(wordmark),\n",
    "            'language': TextPreprocessor.detect_language(wordmark),\n",
    "            'char_count': len(set(wordmark.lower())),\n",
    "            'has_numbers': bool(re.search(r'\\d', wordmark)),\n",
    "            'has_special': bool(re.search(r'[^\\w\\s]', wordmark))\n",
    "        }\n",
    "\n",
    "# Test the preprocessor\n",
    "print(\"Testing Text Preprocessor:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_cases = [\n",
    "    \"TechFlow™\",\n",
    "    \"Café Délicieux\",\n",
    "    \"QuickMart\",\n",
    "    \"データSync\",  # Mixed languages\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    result = TextPreprocessor.preprocess_trademark(test)\n",
    "    print(f\"\\nOriginal: {result['original']}\")\n",
    "    print(f\"Normalized: {result['normalized']}\")\n",
    "    print(f\"Language: {result['language']}\")\n",
    "    print(f\"Length: {result['length']}, Unique chars: {result['char_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to entire dataset\n",
    "print(\"Applying preprocessing to dataset...\")\n",
    "\n",
    "# Process all wordmarks\n",
    "processed_data = df['wordmark'].apply(TextPreprocessor.preprocess_trademark)\n",
    "\n",
    "# Extract components into separate columns\n",
    "df['wordmark_normalized'] = processed_data.apply(lambda x: x['normalized'])\n",
    "df['wordmark_original'] = processed_data.apply(lambda x: x['original'])\n",
    "df['detected_language'] = processed_data.apply(lambda x: x['language'])\n",
    "df['has_numbers'] = processed_data.apply(lambda x: x['has_numbers'])\n",
    "df['has_special'] = processed_data.apply(lambda x: x['has_special'])\n",
    "df['unique_chars'] = processed_data.apply(lambda x: x['char_count'])\n",
    "\n",
    "print(\"\\nPreprocessed dataset preview:\")\n",
    "print(df[['wordmark_original', 'wordmark_normalized', 'detected_language']].head(10))\n",
    "\n",
    "print(\"\\n--- Language Distribution ---\")\n",
    "print(df['detected_language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64cdf2a",
   "metadata": {},
   "source": [
    "# 3. Character-Level CNN Architecture\n",
    "\n",
    "Design and implement a character-level CNN for extracting deep features from trademark text. This architecture is particularly effective for capturing spelling variants and misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deep learning libraries if needed\n",
    "# !pip install tensorflow torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aeaa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe30213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level tokenization setup\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"\n",
    "    Converts text to character-level sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_len=50, char_level=True):\n",
    "        self.max_len = max_len\n",
    "        self.char_level = char_level\n",
    "        self.tokenizer = Tokenizer(char_level=char_level, lower=True)\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Fit tokenizer on texts\"\"\"\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Sample characters: {list(self.tokenizer.word_index.keys())[:20]}\")\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        \"\"\"Transform texts to padded sequences\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_len, padding='post')\n",
    "        return padded\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(texts)\n",
    "        return self.transform(texts)\n",
    "\n",
    "# Initialize and fit tokenizer\n",
    "char_tokenizer = CharacterTokenizer(max_len=50)\n",
    "X_sequences = char_tokenizer.fit_transform(df['wordmark_normalized'].values)\n",
    "\n",
    "print(f\"\\nSequence shape: {X_sequences.shape}\")\n",
    "print(f\"Sample sequence (first trademark):\")\n",
    "print(f\"Text: {df['wordmark_normalized'].iloc[0]}\")\n",
    "print(f\"Sequence: {X_sequences[0][:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd757261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Character-Level CNN Architecture\n",
    "def build_char_cnn(vocab_size, max_len, embedding_dim=64, num_filters=128):\n",
    "    \"\"\"\n",
    "    Build a character-level CNN for trademark embedding extraction\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding layer\n",
    "    - Multiple Conv1D layers with different kernel sizes\n",
    "    - MaxPooling\n",
    "    - Dense layers\n",
    "    - Output: embeddings + classification head\n",
    "    \"\"\"\n",
    "    \n",
    "    input_layer = layers.Input(shape=(max_len,), name='char_input')\n",
    "    \n",
    "    # Character embedding\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_len,\n",
    "        name='char_embedding'\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Multi-scale convolutional layers (different kernel sizes)\n",
    "    conv_blocks = []\n",
    "    kernel_sizes = [2, 3, 4, 5]\n",
    "    \n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv = layers.Conv1D(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            name=f'conv_{kernel_size}'\n",
    "        )(embedding)\n",
    "        pool = layers.GlobalMaxPooling1D(name=f'pool_{kernel_size}')(conv)\n",
    "        conv_blocks.append(pool)\n",
    "    \n",
    "    # Concatenate all pooled features\n",
    "    concatenated = layers.Concatenate(name='concat')(conv_blocks)\n",
    "    \n",
    "    # Dense layers\n",
    "    dense1 = layers.Dense(256, activation='relu', name='dense1')(concatenated)\n",
    "    dropout1 = layers.Dropout(0.5, name='dropout1')(dense1)\n",
    "    \n",
    "    # Embedding layer (penultimate) - this is what we'll extract for SVM\n",
    "    embedding_output = layers.Dense(128, activation='relu', name='embedding_layer')(dropout1)\n",
    "    dropout2 = layers.Dropout(0.3, name='dropout2')(embedding_output)\n",
    "    \n",
    "    # Classification head (for training)\n",
    "    output = layers.Dense(1, activation='sigmoid', name='classification')(dropout2)\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Model(inputs=input_layer, outputs=output, name='CharCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "vocab_size = char_tokenizer.vocab_size\n",
    "max_len = char_tokenizer.max_len\n",
    "\n",
    "cnn_model = build_char_cnn(vocab_size, max_len)\n",
    "\n",
    "# Compile model\n",
    "cnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"Character-Level CNN Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "keras.utils.plot_model(\n",
    "    cnn_model,\n",
    "    to_file='cnn_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    dpi=96\n",
    ")\n",
    "\n",
    "print(\"✓ Model architecture diagram saved as 'cnn_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3764ecc",
   "metadata": {},
   "source": [
    "# 4. CNN Training and Embedding Extraction\n",
    "\n",
    "Train the CNN on labeled trademark data and extract embeddings from the penultimate layer for downstream SVM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989690d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare training data\n",
    "# For this demo with sample data, we'll create synthetic labels\n",
    "# In production, use your actual pairwise similarity labels\n",
    "\n",
    "# Create pairwise dataset for training\n",
    "# For simplicity, we'll train on individual marks first\n",
    "X = X_sequences\n",
    "y = df['label'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nLabel distribution in training set:\")\n",
    "print(f\"Class 0: {np.sum(y_train == 0)}\")\n",
    "print(f\"Class 1: {np.sum(y_train == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    'best_cnn_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# NOTE: With sample data, training may not be meaningful\n",
    "# This demonstrates the training pipeline\n",
    "print(\"Starting CNN training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For demonstration with small sample data, use fewer epochs\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,  # Increase to 50-100 for real data\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Training Precision', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)\n",
    "axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Validation Recall', linewidth=2)\n",
    "axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1669136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from penultimate layer\n",
    "# Create a new model that outputs the embedding layer\n",
    "embedding_extractor = models.Model(\n",
    "    inputs=cnn_model.input,\n",
    "    outputs=cnn_model.get_layer('embedding_layer').output,\n",
    "    name='EmbeddingExtractor'\n",
    ")\n",
    "\n",
    "print(\"Embedding Extractor Model:\")\n",
    "embedding_extractor.summary()\n",
    "\n",
    "# Extract embeddings for all data\n",
    "print(\"\\nExtracting embeddings...\")\n",
    "train_embeddings = embedding_extractor.predict(X_train, verbose=0)\n",
    "val_embeddings = embedding_extractor.predict(X_val, verbose=0)\n",
    "test_embeddings = embedding_extractor.predict(X_test, verbose=0)\n",
    "\n",
    "print(f\"\\nEmbedding shapes:\")\n",
    "print(f\"Training embeddings: {train_embeddings.shape}\")\n",
    "print(f\"Validation embeddings: {val_embeddings.shape}\")\n",
    "print(f\"Test embeddings: {test_embeddings.shape}\")\n",
    "\n",
    "# Store embeddings in dataframe for later use\n",
    "df['cnn_embedding'] = list(embedding_extractor.predict(X_sequences, verbose=0))\n",
    "print(f\"\\n✓ Embeddings extracted and stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90bf1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Computing t-SNE visualization of embeddings...\")\n",
    "\n",
    "# Combine train and validation for visualization\n",
    "all_embeddings = np.vstack([train_embeddings, val_embeddings])\n",
    "all_labels = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_embeddings)-1))\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0], \n",
    "    embeddings_2d[:, 1],\n",
    "    c=all_labels,\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=100,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "plt.colorbar(scatter, label='Label')\n",
    "plt.title('t-SNE Visualization of CNN Embeddings', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ t-SNE visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc580b05",
   "metadata": {},
   "source": [
    "# 5. Linguistic Feature Engineering - Synonyms & Antonyms\n",
    "\n",
    "Build synonym and antonym expansion features using WordNet for English and custom lexicons for Hausa/Yoruba languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLP libraries\n",
    "# !pip install nltk spacy sentence-transformers\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa993dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "print(\"✓ NLTK resources loaded\")\n",
    "\n",
    "# Load multilingual sentence embedding model\n",
    "print(\"\\nLoading multilingual sentence transformer...\")\n",
    "semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(\"✓ Sentence transformer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynonymAntonymEngine:\n",
    "    \"\"\"\n",
    "    Extract synonyms and antonyms for English, Hausa, and Yoruba\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Custom lexicons for Hausa and Yoruba\n",
    "        # In production, load from comprehensive lexicon files\n",
    "        self.hausa_lexicon = {\n",
    "            'quick': ['sauri', 'gaggawa'],\n",
    "            'fast': ['sauri', 'gaggawa'],\n",
    "            'good': ['mai kyau', 'kyakkyawa'],\n",
    "            'shop': ['shago', 'kantin'],\n",
    "            'market': ['kasuwa'],\n",
    "            'food': ['abinci'],\n",
    "            'tech': ['fasaha'],\n",
    "            'data': ['bayanai']\n",
    "        }\n",
    "        \n",
    "        self.yoruba_lexicon = {\n",
    "            'quick': ['yara', 'kiakia'],\n",
    "            'fast': ['yara', 'kiakia'],\n",
    "            'good': ['dara', 'rere'],\n",
    "            'shop': ['ile itaja', 'itaja'],\n",
    "            'market': ['oja'],\n",
    "            'food': ['onje'],\n",
    "            'tech': ['imọ-ẹrọ'],\n",
    "            'data': ['data']\n",
    "        }\n",
    "        \n",
    "    def get_synonyms_en(self, word):\n",
    "        \"\"\"Get English synonyms using WordNet\"\"\"\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().lower().replace('_', ' '))\n",
    "        return list(synonyms)\n",
    "    \n",
    "    def get_antonyms_en(self, word):\n",
    "        \"\"\"Get English antonyms using WordNet\"\"\"\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms():\n",
    "                    antonyms.add(lemma.antonyms()[0].name().lower().replace('_', ' '))\n",
    "        return list(antonyms)\n",
    "    \n",
    "    def get_hausa_equivalents(self, word):\n",
    "        \"\"\"Get Hausa equivalents from lexicon\"\"\"\n",
    "        word_lower = word.lower()\n",
    "        return self.hausa_lexicon.get(word_lower, [])\n",
    "    \n",
    "    def get_yoruba_equivalents(self, word):\n",
    "        \"\"\"Get Yoruba equivalents from lexicon\"\"\"\n",
    "        word_lower = word.lower()\n",
    "        return self.yoruba_lexicon.get(word_lower, [])\n",
    "    \n",
    "    def expand_trademark(self, trademark_text):\n",
    "        \"\"\"\n",
    "        Expand a trademark into synonyms, antonyms, and translations\n",
    "        \"\"\"\n",
    "        words = trademark_text.lower().split()\n",
    "        \n",
    "        expansion = {\n",
    "            'synonyms': [],\n",
    "            'antonyms': [],\n",
    "            'hausa': [],\n",
    "            'yoruba': []\n",
    "        }\n",
    "        \n",
    "        for word in words:\n",
    "            expansion['synonyms'].extend(self.get_synonyms_en(word))\n",
    "            expansion['antonyms'].extend(self.get_antonyms_en(word))\n",
    "            expansion['hausa'].extend(self.get_hausa_equivalents(word))\n",
    "            expansion['yoruba'].extend(self.get_yoruba_equivalents(word))\n",
    "        \n",
    "        # Remove duplicates\n",
    "        for key in expansion:\n",
    "            expansion[key] = list(set(expansion[key]))\n",
    "        \n",
    "        return expansion\n",
    "\n",
    "# Initialize engine\n",
    "syn_ant_engine = SynonymAntonymEngine()\n",
    "\n",
    "# Test the engine\n",
    "print(\"Testing Synonym/Antonym Engine:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_marks = ['QuickMart', 'FastFood', 'GoodTech', 'DataShop']\n",
    "\n",
    "for mark in test_marks:\n",
    "    expansion = syn_ant_engine.expand_trademark(mark)\n",
    "    print(f\"\\n{mark}:\")\n",
    "    print(f\"  Synonyms: {expansion['synonyms'][:5]}\")\n",
    "    print(f\"  Antonyms: {expansion['antonyms'][:5]}\")\n",
    "    print(f\"  Hausa: {expansion['hausa']}\")\n",
    "    print(f\"  Yoruba: {expansion['yoruba']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_synonym_overlap(mark1, mark2, engine):\n",
    "    \"\"\"\n",
    "    Compute overlap score between expanded synonym sets\n",
    "    \"\"\"\n",
    "    exp1 = engine.expand_trademark(mark1)\n",
    "    exp2 = engine.expand_trademark(mark2)\n",
    "    \n",
    "    # Combine all expansions\n",
    "    set1 = set(exp1['synonyms'] + exp1['hausa'] + exp1['yoruba'] + [mark1.lower()])\n",
    "    set2 = set(exp2['synonyms'] + exp2['hausa'] + exp2['yoruba'] + [mark2.lower()])\n",
    "    \n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def compute_semantic_similarity(mark1, mark2, model):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity using sentence embeddings\n",
    "    \"\"\"\n",
    "    embeddings = model.encode([mark1, mark2])\n",
    "    \n",
    "    # Cosine similarity\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    \n",
    "    return float(similarity)\n",
    "\n",
    "# Compute pairwise features for sample data\n",
    "print(\"Computing synonym and semantic features...\")\n",
    "\n",
    "# For demonstration, compute for first few pairs\n",
    "sample_pairs = [\n",
    "    ('TechFlow', 'TekFlow'),\n",
    "    ('DataSync', 'DataLink'),\n",
    "    ('QuickMart', 'FastMart')\n",
    "]\n",
    "\n",
    "results = []\n",
    "for mark1, mark2 in sample_pairs:\n",
    "    overlap = compute_synonym_overlap(mark1, mark2, syn_ant_engine)\n",
    "    semantic_sim = compute_semantic_similarity(mark1, mark2, semantic_model)\n",
    "    \n",
    "    results.append({\n",
    "        'mark1': mark1,\n",
    "        'mark2': mark2,\n",
    "        'synonym_overlap': overlap,\n",
    "        'semantic_similarity': semantic_sim\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{mark1} ↔ {mark2}\")\n",
    "    print(f\"  Synonym overlap: {overlap:.3f}\")\n",
    "    print(f\"  Semantic similarity: {semantic_sim:.3f}\")\n",
    "\n",
    "# Create dataframe\n",
    "syn_features_df = pd.DataFrame(results)\n",
    "print(\"\\n✓ Synonym and semantic features computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad464ef",
   "metadata": {},
   "source": [
    "# 6. Linguistic Feature Engineering - Phonetic Similarity\n",
    "\n",
    "Implement phonetic encoding and similarity measures to capture pronunciation-based similarity between trademarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install phonetic libraries\n",
    "# !pip install phonetics jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "class PhoneticSimilarityEngine:\n",
    "    \"\"\"\n",
    "    Compute phonetic similarity using multiple algorithms\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def double_metaphone_encode(text):\n",
    "        \"\"\"Get Double Metaphone encoding\"\"\"\n",
    "        primary, secondary = jellyfish.metaphone(text), jellyfish.metaphone(text)\n",
    "        return primary, secondary\n",
    "    \n",
    "    @staticmethod\n",
    "    def soundex_encode(text):\n",
    "        \"\"\"Get Soundex encoding\"\"\"\n",
    "        try:\n",
    "            return jellyfish.soundex(text)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def match_rating_encode(text):\n",
    "        \"\"\"Get Match Rating Codex\"\"\"\n",
    "        try:\n",
    "            return jellyfish.match_rating_codex(text)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_phonetic_similarity(mark1, mark2):\n",
    "        \"\"\"\n",
    "        Compute comprehensive phonetic similarity metrics\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # 1. Double Metaphone\n",
    "        dm1_pri, dm1_sec = PhoneticSimilarityEngine.double_metaphone_encode(mark1)\n",
    "        dm2_pri, dm2_sec = PhoneticSimilarityEngine.double_metaphone_encode(mark2)\n",
    "        features['metaphone_match'] = int(dm1_pri == dm2_pri or dm1_pri == dm2_sec or \n",
    "                                          dm1_sec == dm2_pri or dm1_sec == dm2_sec)\n",
    "        \n",
    "        # 2. Soundex\n",
    "        sdx1 = PhoneticSimilarityEngine.soundex_encode(mark1)\n",
    "        sdx2 = PhoneticSimilarityEngine.soundex_encode(mark2)\n",
    "        features['soundex_match'] = int(sdx1 == sdx2) if sdx1 and sdx2 else 0\n",
    "        \n",
    "        # 3. Levenshtein distance on phonetic codes\n",
    "        if dm1_pri and dm2_pri:\n",
    "            features['metaphone_distance'] = levenshtein_distance(dm1_pri, dm2_pri)\n",
    "        else:\n",
    "            features['metaphone_distance'] = 999\n",
    "        \n",
    "        # 4. Jaro-Winkler similarity (good for brand names)\n",
    "        features['jaro_winkler'] = jellyfish.jaro_winkler_similarity(mark1.lower(), mark2.lower())\n",
    "        \n",
    "        # 5. Match Rating Comparison\n",
    "        try:\n",
    "            features['match_rating_comparison'] = int(jellyfish.match_rating_comparison(mark1, mark2))\n",
    "        except:\n",
    "            features['match_rating_comparison'] = 0\n",
    "        \n",
    "        # 6. Raw Levenshtein distance\n",
    "        features['levenshtein_distance'] = levenshtein_distance(mark1.lower(), mark2.lower())\n",
    "        \n",
    "        # 7. Normalized Levenshtein similarity\n",
    "        max_len = max(len(mark1), len(mark2))\n",
    "        features['normalized_levenshtein'] = 1 - (features['levenshtein_distance'] / max_len) if max_len > 0 else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_char_ngram_similarity(mark1, mark2, n=3):\n",
    "        \"\"\"\n",
    "        Character n-gram cosine similarity\n",
    "        \"\"\"\n",
    "        def get_ngrams(text, n):\n",
    "            text = text.lower()\n",
    "            return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "        \n",
    "        ngrams1 = set(get_ngrams(mark1, n))\n",
    "        ngrams2 = set(get_ngrams(mark2, n))\n",
    "        \n",
    "        if not ngrams1 or not ngrams2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(ngrams1 & ngrams2)\n",
    "        union = len(ngrams1 | ngrams2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# Initialize engine\n",
    "phonetic_engine = PhoneticSimilarityEngine()\n",
    "\n",
    "# Test phonetic similarity\n",
    "print(\"Testing Phonetic Similarity Engine:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_pairs = [\n",
    "    ('TechFlow', 'TekFlow'),\n",
    "    ('QuickMart', 'KwikMart'),\n",
    "    ('DataSync', 'DataLink'),\n",
    "    ('FastFood', 'FirstFood')\n",
    "]\n",
    "\n",
    "for mark1, mark2 in test_pairs:\n",
    "    features = phonetic_engine.compute_phonetic_similarity(mark1, mark2)\n",
    "    ngram_sim = phonetic_engine.compute_char_ngram_similarity(mark1, mark2)\n",
    "    \n",
    "    print(f\"\\n{mark1} ↔ {mark2}\")\n",
    "    print(f\"  Metaphone match: {features['metaphone_match']}\")\n",
    "    print(f\"  Soundex match: {features['soundex_match']}\")\n",
    "    print(f\"  Jaro-Winkler: {features['jaro_winkler']:.3f}\")\n",
    "    print(f\"  Normalized Levenshtein: {features['normalized_levenshtein']:.3f}\")\n",
    "    print(f\"  Char 3-gram similarity: {ngram_sim:.3f}\")\n",
    "    print(f\"  Raw Levenshtein distance: {features['levenshtein_distance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute phonetic features for sample pairs\n",
    "print(\"\\nComputing phonetic features for all sample pairs...\")\n",
    "\n",
    "phonetic_results = []\n",
    "\n",
    "for mark1, mark2 in sample_pairs:\n",
    "    features = phonetic_engine.compute_phonetic_similarity(mark1, mark2)\n",
    "    features['char_trigram_sim'] = phonetic_engine.compute_char_ngram_similarity(mark1, mark2, n=3)\n",
    "    features['char_bigram_sim'] = phonetic_engine.compute_char_ngram_similarity(mark1, mark2, n=2)\n",
    "    features['mark1'] = mark1\n",
    "    features['mark2'] = mark2\n",
    "    \n",
    "    phonetic_results.append(features)\n",
    "\n",
    "phonetic_df = pd.DataFrame(phonetic_results)\n",
    "\n",
    "print(\"\\nPhonetic Features Summary:\")\n",
    "print(phonetic_df[['mark1', 'mark2', 'jaro_winkler', 'normalized_levenshtein', \n",
    "                   'char_trigram_sim', 'metaphone_match']])\n",
    "\n",
    "print(\"\\n✓ Phonetic features computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7cfc68",
   "metadata": {},
   "source": [
    "# 7. Linguistic Feature Engineering - Hausa & Yoruba Translation\n",
    "\n",
    "Build translation lexicons and compute cross-lingual semantic similarity for Hausa and Yoruba languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualTranslationEngine:\n",
    "    \"\"\"\n",
    "    Handle Hausa and Yoruba translations and cross-lingual similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Expanded business domain lexicons\n",
    "        # In production, load from comprehensive dictionary files\n",
    "        self.en_to_hausa = {\n",
    "            'market': 'kasuwa',\n",
    "            'shop': 'shago',\n",
    "            'store': 'kantin sayarwa',\n",
    "            'quick': 'sauri',\n",
    "            'fast': 'gaggawa',\n",
    "            'good': 'mai kyau',\n",
    "            'best': 'mafi kyau',\n",
    "            'tech': 'fasaha',\n",
    "            'technology': 'fasahar zamani',\n",
    "            'data': 'bayanai',\n",
    "            'food': 'abinci',\n",
    "            'service': 'hidima',\n",
    "            'link': 'hanyar haɗi',\n",
    "            'sync': 'daidaitawa',\n",
    "            'flow': 'gudana',\n",
    "            'smart': 'mai hankali',\n",
    "            'digital': 'dijital',\n",
    "            'online': 'akan layi',\n",
    "            'mobile': 'wayar hannu'\n",
    "        }\n",
    "        \n",
    "        self.en_to_yoruba = {\n",
    "            'market': 'oja',\n",
    "            'shop': 'ile itaja',\n",
    "            'store': 'ile itaja',\n",
    "            'quick': 'yara',\n",
    "            'fast': 'kiakia',\n",
    "            'good': 'dara',\n",
    "            'best': 'to dara ju',\n",
    "            'tech': 'imọ-ẹrọ',\n",
    "            'technology': 'imọ-ẹrọ',\n",
    "            'data': 'data',\n",
    "            'food': 'onje',\n",
    "            'service': 'iṣẹ',\n",
    "            'link': 'asopọ',\n",
    "            'sync': 'muṣiṣẹpọ',\n",
    "            'flow': 'ṣiṣan',\n",
    "            'smart': 'ọlọgbọn',\n",
    "            'digital': 'alagbeka',\n",
    "            'online': 'lori ayelujara',\n",
    "            'mobile': 'alagbeka'\n",
    "        }\n",
    "        \n",
    "        # Reverse mappings\n",
    "        self.hausa_to_en = {v: k for k, v in self.en_to_hausa.items()}\n",
    "        self.yoruba_to_en = {v: k for k, v in self.en_to_yoruba.items()}\n",
    "    \n",
    "    def translate_to_hausa(self, text):\n",
    "        \"\"\"Translate English words to Hausa\"\"\"\n",
    "        words = text.lower().split()\n",
    "        translations = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Clean word\n",
    "            word_clean = ''.join(c for c in word if c.isalnum())\n",
    "            translation = self.en_to_hausa.get(word_clean, word_clean)\n",
    "            translations.append(translation)\n",
    "        \n",
    "        return ' '.join(translations)\n",
    "    \n",
    "    def translate_to_yoruba(self, text):\n",
    "        \"\"\"Translate English words to Yoruba\"\"\"\n",
    "        words = text.lower().split()\n",
    "        translations = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_clean = ''.join(c for c in word if c.isalnum())\n",
    "            translation = self.en_to_yoruba.get(word_clean, word_clean)\n",
    "            translations.append(translation)\n",
    "        \n",
    "        return ' '.join(translations)\n",
    "    \n",
    "    def compute_cross_lingual_similarity(self, mark1, mark2, semantic_model):\n",
    "        \"\"\"\n",
    "        Compute similarity considering translations\n",
    "        \"\"\"\n",
    "        # Get translations\n",
    "        mark1_ha = self.translate_to_hausa(mark1)\n",
    "        mark1_yo = self.translate_to_yoruba(mark1)\n",
    "        mark2_ha = self.translate_to_hausa(mark2)\n",
    "        mark2_yo = self.translate_to_yoruba(mark2)\n",
    "        \n",
    "        # Compute embeddings for all versions\n",
    "        texts = [mark1, mark2, mark1_ha, mark2_ha, mark1_yo, mark2_yo]\n",
    "        embeddings = semantic_model.encode(texts)\n",
    "        \n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        # Original similarity\n",
    "        sim_original = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        \n",
    "        # Hausa cross-lingual\n",
    "        sim_ha_cross = max(\n",
    "            cosine_similarity([embeddings[0]], [embeddings[3]])[0][0],  # mark1 vs mark2_ha\n",
    "            cosine_similarity([embeddings[2]], [embeddings[1]])[0][0]   # mark1_ha vs mark2\n",
    "        )\n",
    "        \n",
    "        # Yoruba cross-lingual\n",
    "        sim_yo_cross = max(\n",
    "            cosine_similarity([embeddings[0]], [embeddings[5]])[0][0],  # mark1 vs mark2_yo\n",
    "            cosine_similarity([embeddings[4]], [embeddings[1]])[0][0]   # mark1_yo vs mark2\n",
    "        )\n",
    "        \n",
    "        # Same-language translated similarity\n",
    "        sim_ha_ha = cosine_similarity([embeddings[2]], [embeddings[3]])[0][0]\n",
    "        sim_yo_yo = cosine_similarity([embeddings[4]], [embeddings[5]])[0][0]\n",
    "        \n",
    "        return {\n",
    "            'original_similarity': float(sim_original),\n",
    "            'hausa_cross_similarity': float(sim_ha_cross),\n",
    "            'yoruba_cross_similarity': float(sim_yo_cross),\n",
    "            'hausa_translation_similarity': float(sim_ha_ha),\n",
    "            'yoruba_translation_similarity': float(sim_yo_yo),\n",
    "            'max_cross_lingual': float(max(sim_ha_cross, sim_yo_cross)),\n",
    "            'mark1_hausa': mark1_ha,\n",
    "            'mark1_yoruba': mark1_yo,\n",
    "            'mark2_hausa': mark2_ha,\n",
    "            'mark2_yoruba': mark2_yo\n",
    "        }\n",
    "\n",
    "# Initialize translation engine\n",
    "translation_engine = MultilingualTranslationEngine()\n",
    "\n",
    "# Test translations\n",
    "print(\"Testing Multilingual Translation Engine:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_marks = ['QuickMart', 'FastFood', 'TechStore', 'DataLink']\n",
    "\n",
    "for mark in test_marks:\n",
    "    hausa = translation_engine.translate_to_hausa(mark)\n",
    "    yoruba = translation_engine.translate_to_yoruba(mark)\n",
    "    \n",
    "    print(f\"\\n{mark}:\")\n",
    "    print(f\"  Hausa: {hausa}\")\n",
    "    print(f\"  Yoruba: {yoruba}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-lingual features for sample pairs\n",
    "print(\"\\nComputing cross-lingual similarity features...\")\n",
    "\n",
    "cross_lingual_results = []\n",
    "\n",
    "for mark1, mark2 in sample_pairs:\n",
    "    features = translation_engine.compute_cross_lingual_similarity(\n",
    "        mark1, mark2, semantic_model\n",
    "    )\n",
    "    features['mark1'] = mark1\n",
    "    features['mark2'] = mark2\n",
    "    \n",
    "    cross_lingual_results.append(features)\n",
    "    \n",
    "    print(f\"\\n{mark1} ↔ {mark2}\")\n",
    "    print(f\"  Original: {features['original_similarity']:.3f}\")\n",
    "    print(f\"  Hausa cross: {features['hausa_cross_similarity']:.3f}\")\n",
    "    print(f\"  Yoruba cross: {features['yoruba_cross_similarity']:.3f}\")\n",
    "    print(f\"  Max cross-lingual: {features['max_cross_lingual']:.3f}\")\n",
    "\n",
    "cross_lingual_df = pd.DataFrame(cross_lingual_results)\n",
    "\n",
    "print(\"\\n✓ Cross-lingual features computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7bb9e",
   "metadata": {},
   "source": [
    "# 8. Feature Vector Combination\n",
    "\n",
    "Concatenate CNN embeddings with all linguistic features to create comprehensive feature vectors for SVM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "class FeatureVectorBuilder:\n",
    "    \"\"\"\n",
    "    Combine CNN embeddings with linguistic features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "    \n",
    "    def build_feature_vector(self, cnn_embedding, linguistic_features):\n",
    "        \"\"\"\n",
    "        Combine CNN embedding with linguistic feature dict\n",
    "        \n",
    "        Args:\n",
    "            cnn_embedding: numpy array from CNN\n",
    "            linguistic_features: dict with all linguistic features\n",
    "        \n",
    "        Returns:\n",
    "            Combined feature vector\n",
    "        \"\"\"\n",
    "        # Extract linguistic features in consistent order\n",
    "        ling_features = [\n",
    "            # Synonym/Semantic features\n",
    "            linguistic_features.get('synonym_overlap', 0),\n",
    "            linguistic_features.get('semantic_similarity', 0),\n",
    "            \n",
    "            # Phonetic features\n",
    "            linguistic_features.get('metaphone_match', 0),\n",
    "            linguistic_features.get('soundex_match', 0),\n",
    "            linguistic_features.get('jaro_winkler', 0),\n",
    "            linguistic_features.get('normalized_levenshtein', 0),\n",
    "            linguistic_features.get('levenshtein_distance', 0),\n",
    "            linguistic_features.get('char_trigram_sim', 0),\n",
    "            linguistic_features.get('char_bigram_sim', 0),\n",
    "            linguistic_features.get('match_rating_comparison', 0),\n",
    "            \n",
    "            # Cross-lingual features\n",
    "            linguistic_features.get('original_similarity', 0),\n",
    "            linguistic_features.get('hausa_cross_similarity', 0),\n",
    "            linguistic_features.get('yoruba_cross_similarity', 0),\n",
    "            linguistic_features.get('hausa_translation_similarity', 0),\n",
    "            linguistic_features.get('yoruba_translation_similarity', 0),\n",
    "            linguistic_features.get('max_cross_lingual', 0)\n",
    "        ]\n",
    "        \n",
    "        # Combine\n",
    "        combined = np.concatenate([cnn_embedding, ling_features])\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def fit_transform(self, embeddings_list, linguistic_features_list):\n",
    "        \"\"\"\n",
    "        Fit scaler and transform features\n",
    "        \"\"\"\n",
    "        # Build all feature vectors\n",
    "        feature_vectors = []\n",
    "        for emb, ling in zip(embeddings_list, linguistic_features_list):\n",
    "            vec = self.build_feature_vector(emb, ling)\n",
    "            feature_vectors.append(vec)\n",
    "        \n",
    "        feature_vectors = np.array(feature_vectors)\n",
    "        \n",
    "        # Fit and transform\n",
    "        scaled_features = self.scaler.fit_transform(feature_vectors)\n",
    "        self.fitted = True\n",
    "        \n",
    "        return scaled_features\n",
    "    \n",
    "    def transform(self, embeddings_list, linguistic_features_list):\n",
    "        \"\"\"\n",
    "        Transform features using fitted scaler\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Scaler not fitted. Call fit_transform first.\")\n",
    "        \n",
    "        # Build feature vectors\n",
    "        feature_vectors = []\n",
    "        for emb, ling in zip(embeddings_list, linguistic_features_list):\n",
    "            vec = self.build_feature_vector(emb, ling)\n",
    "            feature_vectors.append(vec)\n",
    "        \n",
    "        feature_vectors = np.array(feature_vectors)\n",
    "        \n",
    "        # Transform\n",
    "        scaled_features = self.scaler.transform(feature_vectors)\n",
    "        \n",
    "        return scaled_features\n",
    "\n",
    "# Initialize feature builder\n",
    "feature_builder = FeatureVectorBuilder()\n",
    "\n",
    "print(\"Feature Vector Builder initialized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comprehensive feature dataset\n",
    "# For demo purposes, we'll create synthetic pairwise features\n",
    "\n",
    "print(\"Building comprehensive feature vectors...\")\n",
    "\n",
    "# Combine all linguistic features for sample pairs\n",
    "all_linguistic_features = []\n",
    "\n",
    "for i in range(len(sample_pairs)):\n",
    "    combined_features = {\n",
    "        **syn_features_df.iloc[i].to_dict(),\n",
    "        **phonetic_df.iloc[i].to_dict(),\n",
    "        **cross_lingual_df.iloc[i].to_dict()\n",
    "    }\n",
    "    all_linguistic_features.append(combined_features)\n",
    "\n",
    "# For demonstration, use embeddings from first marks in each pair\n",
    "# In production, you would compute pairwise embeddings (e.g., concatenate or difference)\n",
    "sample_embeddings = [\n",
    "    train_embeddings[0],  # TechFlow embedding\n",
    "    train_embeddings[1],  # DataSync embedding  \n",
    "    train_embeddings[2]   # QuickMart embedding\n",
    "]\n",
    "\n",
    "# Build combined feature vectors\n",
    "combined_features = feature_builder.fit_transform(\n",
    "    sample_embeddings,\n",
    "    all_linguistic_features\n",
    ")\n",
    "\n",
    "print(f\"\\nCombined feature shape: {combined_features.shape}\")\n",
    "print(f\"  - CNN embedding dimensions: 128\")\n",
    "print(f\"  - Linguistic feature dimensions: {combined_features.shape[1] - 128}\")\n",
    "print(f\"  - Total dimensions: {combined_features.shape[1]}\")\n",
    "\n",
    "# Show feature statistics\n",
    "feature_df = pd.DataFrame(combined_features)\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(feature_df.describe())\n",
    "\n",
    "print(\"\\n✓ Feature vectors built and normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance through correlation\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create sample labels for correlation\n",
    "sample_labels = [1, 1, 1]  # All similar for demo\n",
    "\n",
    "# Compute correlations\n",
    "correlation_data = np.column_stack([combined_features, sample_labels])\n",
    "correlation_df = pd.DataFrame(correlation_data)\n",
    "correlation_df.columns = [f'F{i}' for i in range(combined_features.shape[1])] + ['Label']\n",
    "\n",
    "# Plot correlation with label\n",
    "correlations = correlation_df.corr()['Label'].drop('Label').sort_values(ascending=False)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "correlations.head(20).plot(kind='barh', color='steelblue')\n",
    "plt.title('Top 20 Features Correlated with Similarity Label', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation with Label')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(combined_features.flatten(), bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of All Feature Values (Normalized)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136021b7",
   "metadata": {},
   "source": [
    "# 9. SVM Classifier Training\n",
    "\n",
    "Train SVM classifiers on the combined feature vectors and compare performance between CNN-only and CNN+linguistic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9123860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "print(\"Preparing SVM training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For this demonstration with limited sample data, we'll create synthetic training data\n",
    "# In production, use your actual labeled pairwise trademark data\n",
    "\n",
    "# Create synthetic training data for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate more samples for meaningful training\n",
    "n_similar = 100\n",
    "n_dissimilar = 100\n",
    "\n",
    "# Similar pairs: high feature values\n",
    "X_similar = np.random.randn(n_similar, combined_features.shape[1]) * 0.5 + 1.5\n",
    "y_similar = np.ones(n_similar)\n",
    "\n",
    "# Dissimilar pairs: low feature values\n",
    "X_dissimilar = np.random.randn(n_dissimilar, combined_features.shape[1]) * 0.5 - 1.5\n",
    "y_dissimilar = np.zeros(n_dissimilar)\n",
    "\n",
    "# Combine\n",
    "X_train_svm = np.vstack([X_similar, X_dissimilar])\n",
    "y_train_svm = np.concatenate([y_similar, y_dissimilar])\n",
    "\n",
    "# Shuffle\n",
    "shuffle_idx = np.random.permutation(len(X_train_svm))\n",
    "X_train_svm = X_train_svm[shuffle_idx]\n",
    "y_train_svm = y_train_svm[shuffle_idx]\n",
    "\n",
    "# Split into train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_svm, X_val_svm, y_train_svm, y_val_svm = train_test_split(\n",
    "    X_train_svm, y_train_svm, test_size=0.2, random_state=42, stratify=y_train_svm\n",
    ")\n",
    "\n",
    "print(f\"SVM Training set: {X_train_svm.shape}\")\n",
    "print(f\"SVM Validation set: {X_val_svm.shape}\")\n",
    "print(f\"Label distribution (train): Similar={np.sum(y_train_svm)}, Dissimilar={np.sum(y_train_svm==0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with Linear Kernel\n",
    "print(\"\\n1. Training Linear SVM...\")\n",
    "\n",
    "svm_linear = SVC(kernel='linear', C=1.0, probability=True, random_state=42)\n",
    "svm_linear.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_linear = svm_linear.predict(X_val_svm)\n",
    "train_score_linear = svm_linear.score(X_train_svm, y_train_svm)\n",
    "val_score_linear = svm_linear.score(X_val_svm, y_val_svm)\n",
    "\n",
    "print(f\"Linear SVM - Training Accuracy: {train_score_linear:.4f}\")\n",
    "print(f\"Linear SVM - Validation Accuracy: {val_score_linear:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Linear SVM):\")\n",
    "print(classification_report(y_val_svm, y_pred_linear, \n",
    "                          target_names=['Dissimilar', 'Similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with RBF Kernel\n",
    "print(\"\\n2. Training RBF SVM...\")\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "svm_rbf.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rbf = svm_rbf.predict(X_val_svm)\n",
    "train_score_rbf = svm_rbf.score(X_train_svm, y_train_svm)\n",
    "val_score_rbf = svm_rbf.score(X_val_svm, y_val_svm)\n",
    "\n",
    "print(f\"RBF SVM - Training Accuracy: {train_score_rbf:.4f}\")\n",
    "print(f\"RBF SVM - Validation Accuracy: {val_score_rbf:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (RBF SVM):\")\n",
    "print(classification_report(y_val_svm, y_pred_rbf,\n",
    "                          target_names=['Dissimilar', 'Similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "print(\"\\n3. Hyperparameter Tuning...\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_val_svm)\n",
    "val_score_best = best_svm.score(X_val_svm, y_val_svm)\n",
    "\n",
    "print(f\"\\nBest SVM - Validation Accuracy: {val_score_best:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Best SVM):\")\n",
    "print(classification_report(y_val_svm, y_pred_best,\n",
    "                          target_names=['Dissimilar', 'Similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e40b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CNN-only vs CNN+Linguistic Features\n",
    "print(\"\\n4. Comparing CNN-only vs CNN+Linguistic...\")\n",
    "\n",
    "# CNN-only features (first 128 dimensions)\n",
    "X_train_cnn_only = X_train_svm[:, :128]\n",
    "X_val_cnn_only = X_val_svm[:, :128]\n",
    "\n",
    "# Train SVM on CNN-only\n",
    "svm_cnn_only = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
    "svm_cnn_only.fit(X_train_cnn_only, y_train_svm)\n",
    "\n",
    "val_score_cnn_only = svm_cnn_only.score(X_val_cnn_only, y_val_svm)\n",
    "\n",
    "# Compare\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Linear SVM', 'RBF SVM', 'Best SVM (Tuned)', 'CNN-only SVM'],\n",
    "    'Validation Accuracy': [val_score_linear, val_score_rbf, val_score_best, val_score_cnn_only]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['Model'], comparison_df['Validation Accuracy'], \n",
    "        color=['steelblue', 'coral', 'mediumseagreen', 'mediumpurple'])\n",
    "plt.title('SVM Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, v in enumerate(comparison_df['Validation Accuracy']):\n",
    "    plt.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ SVM training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607acd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "print(\"\\nSaving trained models...\")\n",
    "\n",
    "# Save best SVM\n",
    "joblib.dump(best_svm, 'best_svm_model.pkl')\n",
    "print(\"✓ Best SVM saved: best_svm_model.pkl\")\n",
    "\n",
    "# Save CNN embedding extractor\n",
    "embedding_extractor.save('cnn_embedding_extractor.h5')\n",
    "print(\"✓ CNN embedding extractor saved: cnn_embedding_extractor.h5\")\n",
    "\n",
    "# Save feature scaler\n",
    "joblib.dump(feature_builder.scaler, 'feature_scaler.pkl')\n",
    "print(\"✓ Feature scaler saved: feature_scaler.pkl\")\n",
    "\n",
    "print(\"\\n✓ All models saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210323f",
   "metadata": {},
   "source": [
    "# 10. Model Evaluation and Metrics\n",
    "\n",
    "Comprehensive evaluation using accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrices. Prioritize recall for confusingly similar cases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
