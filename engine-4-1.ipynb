{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01906e1",
   "metadata": {},
   "source": [
    "# Trademark Similarity Engine: Hybrid CNN + SVM with Multilingual Linguistic AI\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a trademark similarity classifier/ranker using a hybrid CNN → SVM pipeline, enhanced with AI-based linguistic similarity features including synonyms, antonyms, phonetic similarity, and support for English, Hausa, and Yoruba languages.\n",
    "\n",
    "### Architecture Summary:\n",
    "- **Feature Extraction**: Character-level CNN for learning text representations\n",
    "- **Classification**: SVM for robust decision boundaries\n",
    "- **Linguistic Enhancement**: Multi-layered similarity features including phonetic, semantic, and cross-lingual components\n",
    "- **Languages Supported**: English (EN), Hausa (HA), Yoruba (YO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "import spacy\n",
    "from jellyfish import soundex, metaphone\n",
    "import Levenshtein\n",
    "import unicodedata\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Device available: {tf.config.list_physical_devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b053d",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Exploration\n",
    "\n",
    "First, we'll load the trademark dataset and explore its structure, label distribution, and linguistic characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e717ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrademarkDataLoader:\n",
    "    \"\"\"Handle loading and initial exploration of trademark data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str = None):\n",
    "        self.data_path = data_path\n",
    "        self.df = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_data(self, path: Optional[str] = None):\n",
    "        \"\"\"Load trademark dataset from CSV or create sample data\"\"\"\n",
    "        if path:\n",
    "            self.df = pd.read_csv(path)\n",
    "        else:\n",
    "            # Create sample data for demonstration\n",
    "            np.random.seed(42)\n",
    "            sample_size = 1000\n",
    "            \n",
    "            # Sample trademark data with EN/HA/YO examples\n",
    "            trademarks = [\n",
    "                \"CocaCola\", \"Coca-Cola\", \"Koka-Kola\", \"PepsiCo\", \"Pepsi\",\n",
    "                \"Nike\", \"Nikee\", \"Adidas\", \"Addidas\", \"Puma\",\n",
    "                \"Apple\", \"Aple\", \"Samsung\", \"Samsong\", \"Microsoft\",\n",
    "                \"Toyota\", \"Toyotta\", \"Honda\", \"Hondaa\", \"Ford\",\n",
    "                # Hausa-inspired names\n",
    "                \"Gidan Abinci\", \"Gidan Food\", \"Kasuwa Market\", \"Kasuwa Mart\",\n",
    "                # Yoruba-inspired names\n",
    "                \"Oja Market\", \"Oja Titun\", \"Ile Aso\", \"Aso Rock Fashion\"\n",
    "            ]\n",
    "            \n",
    "            # Generate synthetic dataset\n",
    "            data = []\n",
    "            for _ in range(sample_size):\n",
    "                mark = np.random.choice(trademarks) + str(np.random.randint(0, 100))\n",
    "                class_id = np.random.randint(1, 46)  # International trademark classes 1-45\n",
    "                goods_services = f\"Class {class_id} goods/services description\"\n",
    "                # Binary similarity label (can be extended to multi-class)\n",
    "                label = np.random.choice(['similar', 'not_similar'], p=[0.3, 0.7])\n",
    "                \n",
    "                data.append({\n",
    "                    'wordmark': mark,\n",
    "                    'class': class_id,\n",
    "                    'goods_services': goods_services,\n",
    "                    'label': label\n",
    "                })\n",
    "            \n",
    "            self.df = pd.DataFrame(data)\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def explore_data(self):\n",
    "        \"\"\"Perform exploratory data analysis\"\"\"\n",
    "        print(\"Dataset Shape:\", self.df.shape)\n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(self.df.info())\n",
    "        \n",
    "        print(\"\\nLabel Distribution:\")\n",
    "        print(self.df['label'].value_counts())\n",
    "        \n",
    "        # Text length analysis\n",
    "        self.df['mark_length'] = self.df['wordmark'].str.len()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Label distribution\n",
    "        self.df['label'].value_counts().plot(kind='bar', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Label Distribution')\n",
    "        axes[0, 0].set_xlabel('Label')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Mark length distribution\n",
    "        axes[0, 1].hist(self.df['mark_length'], bins=30, edgecolor='black')\n",
    "        axes[0, 1].set_title('Trademark Length Distribution')\n",
    "        axes[0, 1].set_xlabel('Character Length')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        # Class distribution (top 10)\n",
    "        self.df['class'].value_counts().head(10).plot(kind='bar', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Top 10 Trademark Classes')\n",
    "        axes[1, 0].set_xlabel('Class')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Character frequency analysis\n",
    "        all_chars = ''.join(self.df['wordmark'].values)\n",
    "        char_freq = pd.Series(list(all_chars)).value_counts().head(20)\n",
    "        char_freq.plot(kind='bar', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Top 20 Character Frequencies')\n",
    "        axes[1, 1].set_xlabel('Character')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_train_test_split(self, test_size=0.2, val_size=0.1, random_state=42):\n",
    "        \"\"\"Create stratified train/val/test splits\"\"\"\n",
    "        # Encode labels\n",
    "        self.df['label_encoded'] = self.label_encoder.fit_transform(self.df['label'])\n",
    "        \n",
    "        # First split: train+val vs test\n",
    "        X = self.df.drop(['label', 'label_encoded'], axis=1)\n",
    "        y = self.df['label_encoded']\n",
    "        \n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, \n",
    "            stratify=y_temp, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"Train set size: {len(X_train)}\")\n",
    "        print(f\"Validation set size: {len(X_val)}\")\n",
    "        print(f\"Test set size: {len(X_test)}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# Initialize and explore data\n",
    "data_loader = TrademarkDataLoader()\n",
    "df = data_loader.load_data()\n",
    "df = data_loader.explore_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c85a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/val/test split\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = data_loader.create_train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de597dea",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing Pipeline\n",
    "\n",
    "Implement comprehensive text normalization while preserving original forms for phonetic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Comprehensive text preprocessing for trademark similarity\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.original_cache = {}\n",
    "        \n",
    "    def normalize_unicode(self, text: str) -> str:\n",
    "        \"\"\"Normalize Unicode characters\"\"\"\n",
    "        # Normalize to NFKD form and encode to ASCII, ignoring errors\n",
    "        normalized = unicodedata.normalize('NFKD', text)\n",
    "        return normalized.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    def basic_normalize(self, text: str) -> str:\n",
    "        \"\"\"Basic normalization: lowercase and punctuation removal\"\"\"\n",
    "        # Store original\n",
    "        self.original_cache[text.lower()] = text\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation but keep spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def aggressive_normalize(self, text: str) -> str:\n",
    "        \"\"\"Aggressive normalization for matching\"\"\"\n",
    "        text = self.basic_normalize(text)\n",
    "        \n",
    "        # Remove all spaces\n",
    "        text = text.replace(' ', '')\n",
    "        \n",
    "        # Remove numbers (optional)\n",
    "        # text = re.sub(r'\\d', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preserve_for_phonetics(self, text: str) -> str:\n",
    "        \"\"\"Minimal preprocessing for phonetic analysis\"\"\"\n",
    "        # Only remove obvious punctuation\n",
    "        text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_characters(self, text: str, max_len: int = 50) -> List[int]:\n",
    "        \"\"\"Convert text to character-level tokens\"\"\"\n",
    "        # Create character vocabulary\n",
    "        chars = list(set(''.join([self.basic_normalize(text)])))\n",
    "        char_to_idx = {ch: idx + 1 for idx, ch in enumerate(sorted(chars))}\n",
    "        char_to_idx['<PAD>'] = 0\n",
    "        char_to_idx['<UNK>'] = len(char_to_idx)\n",
    "        \n",
    "        # Convert text to indices\n",
    "        normalized = self.basic_normalize(text)\n",
    "        tokens = [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in normalized]\n",
    "        \n",
    "        # Pad or truncate to max_len\n",
    "        if len(tokens) < max_len:\n",
    "            tokens.extend([0] * (max_len - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:max_len]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def create_ngrams(self, text: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"Create character n-grams\"\"\"\n",
    "        text = self.aggressive_normalize(text)\n",
    "        ngrams = []\n",
    "        for i in range(len(text) - n + 1):\n",
    "            ngrams.append(text[i:i+n])\n",
    "        return ngrams\n",
    "    \n",
    "    def preprocess_batch(self, texts: List[str], mode: str = 'basic') -> List[str]:\n",
    "        \"\"\"Preprocess a batch of texts\"\"\"\n",
    "        if mode == 'basic':\n",
    "            return [self.basic_normalize(text) for text in texts]\n",
    "        elif mode == 'aggressive':\n",
    "            return [self.aggressive_normalize(text) for text in texts]\n",
    "        elif mode == 'phonetic':\n",
    "            return [self.preserve_for_phonetics(text) for text in texts]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "# Test preprocessing\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "sample_marks = [\n",
    "    \"Coca-Cola®\", \n",
    "    \"NIKE™\", \n",
    "    \"Gidan Abinci\",  # Hausa\n",
    "    \"Ọjà Market\",    # Yoruba with diacritics\n",
    "    \"Apple Inc.\",\n",
    "    \"7-Eleven\"\n",
    "]\n",
    "\n",
    "print(\"Preprocessing Examples:\")\n",
    "print(\"-\" * 50)\n",
    "for mark in sample_marks:\n",
    "    print(f\"Original: {mark}\")\n",
    "    print(f\"Basic: {preprocessor.basic_normalize(mark)}\")\n",
    "    print(f\"Aggressive: {preprocessor.aggressive_normalize(mark)}\")\n",
    "    print(f\"Phonetic: {preprocessor.preserve_for_phonetics(mark)}\")\n",
    "    print(f\"3-grams: {preprocessor.create_ngrams(mark)[:5]}...\")  # Show first 5\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539eece1",
   "metadata": {},
   "source": [
    "# 3. Character-Level CNN Implementation\n",
    "\n",
    "Build a character-level CNN for extracting deep features from trademark text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6611d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterCNN:\n",
    "    \"\"\"Character-level CNN for trademark text embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 100, embedding_dim: int = 128, \n",
    "                 max_length: int = 50, num_filters: int = 256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.num_filters = num_filters\n",
    "        self.model = None\n",
    "        self.embedding_model = None\n",
    "        self.char_to_idx = self._build_char_vocabulary()\n",
    "        \n",
    "    def _build_char_vocabulary(self) -> Dict[str, int]:\n",
    "        \"\"\"Build character vocabulary\"\"\"\n",
    "        # Common characters in trademarks\n",
    "        chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789 -&.\")\n",
    "        char_to_idx = {ch: idx + 2 for idx, ch in enumerate(chars)}\n",
    "        char_to_idx['<PAD>'] = 0\n",
    "        char_to_idx['<UNK>'] = 1\n",
    "        return char_to_idx\n",
    "    \n",
    "    def text_to_sequence(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Convert text to sequence of character indices\"\"\"\n",
    "        text = text.lower()\n",
    "        sequence = [self.char_to_idx.get(ch, 1) for ch in text]  # 1 is <UNK>\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(sequence) < self.max_length:\n",
    "            sequence.extend([0] * (self.max_length - len(sequence)))\n",
    "        else:\n",
    "            sequence = sequence[:self.max_length]\n",
    "        \n",
    "        return np.array(sequence)\n",
    "    \n",
    "    def build_model(self, num_classes: int = 2):\n",
    "        \"\"\"Build the CNN architecture\"\"\"\n",
    "        inputs = layers.Input(shape=(self.max_length,))\n",
    "        \n",
    "        # Embedding layer\n",
    "        x = layers.Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            input_length=self.max_length\n",
    "        )(inputs)\n",
    "        \n",
    "        # Multiple filter sizes for different n-gram features\n",
    "        filter_sizes = [3, 4, 5]\n",
    "        conv_outputs = []\n",
    "        \n",
    "        for filter_size in filter_sizes:\n",
    "            conv = layers.Conv1D(\n",
    "                filters=self.num_filters,\n",
    "                kernel_size=filter_size,\n",
    "                activation='relu',\n",
    "                padding='valid'\n",
    "            )(x)\n",
    "            \n",
    "            # Max pooling\n",
    "            pool = layers.GlobalMaxPooling1D()(conv)\n",
    "            conv_outputs.append(pool)\n",
    "        \n",
    "        # Concatenate all conv outputs\n",
    "        concatenated = layers.Concatenate()(conv_outputs) if len(conv_outputs) > 1 else conv_outputs[0]\n",
    "        \n",
    "        # Dense layers\n",
    "        dense1 = layers.Dense(256, activation='relu')(concatenated)\n",
    "        dropout1 = layers.Dropout(0.5)(dense1)\n",
    "        \n",
    "        # This will be our embedding layer for SVM\n",
    "        embedding = layers.Dense(128, activation='relu', name='embedding')(dropout1)\n",
    "        dropout2 = layers.Dropout(0.3)(embedding)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(dropout2)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = models.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Create embedding extraction model\n",
    "        self.embedding_model = models.Model(inputs=inputs, outputs=embedding)\n",
    "        \n",
    "        # Compile\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=20, batch_size=32):\n",
    "        \"\"\"Train the CNN model\"\"\"\n",
    "        # Convert text to sequences\n",
    "        X_train_seq = np.array([self.text_to_sequence(text) for text in X_train])\n",
    "        X_val_seq = np.array([self.text_to_sequence(text) for text in X_val])\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stop = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Model checkpoint\n",
    "        checkpoint = callbacks.ModelCheckpoint(\n",
    "            'best_cnn_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = self.model.fit(\n",
    "            X_train_seq, y_train,\n",
    "            validation_data=(X_val_seq, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stop, checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def extract_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Extract embeddings from trained CNN\"\"\"\n",
    "        sequences = np.array([self.text_to_sequence(text) for text in texts])\n",
    "        embeddings = self.embedding_model.predict(sequences, verbose=0)\n",
    "        return embeddings\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "        axes[1].plot(history.history['val_loss'], label='Val Loss')\n",
    "        axes[1].set_title('Model Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize and build CNN\n",
    "char_cnn = CharacterCNN(vocab_size=100, embedding_dim=128, max_length=50)\n",
    "cnn_model = char_cnn.build_model(num_classes=2)\n",
    "\n",
    "print(\"CNN Model Summary:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN model\n",
    "print(\"Training CNN model...\")\n",
    "history = char_cnn.train(\n",
    "    X_train['wordmark'].values, \n",
    "    y_train.values,\n",
    "    X_val['wordmark'].values, \n",
    "    y_val.values,\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "char_cnn.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b805d27",
   "metadata": {},
   "source": [
    "# 4. Linguistic Feature Engineering\n",
    "\n",
    "Implement comprehensive linguistic features including synonyms, antonyms, and multilingual support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from typing import Set\n",
    "\n",
    "class LinguisticFeatureExtractor:\n",
    "    \"\"\"Extract linguistic features for trademark similarity\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize language-specific resources\n",
    "        self.hausa_yoruba_dict = self._load_multilingual_dictionary()\n",
    "        self.domain_lexicon = self._load_domain_lexicon()\n",
    "        \n",
    "    def _load_multilingual_dictionary(self) -> Dict:\n",
    "        \"\"\"Load or create Hausa/Yoruba translation dictionary\"\"\"\n",
    "        # Sample dictionary - in production, load from comprehensive resource\n",
    "        dictionary = {\n",
    "            'english_to_hausa': {\n",
    "                'market': 'kasuwa',\n",
    "                'house': 'gida',\n",
    "                'food': 'abinci',\n",
    "                'water': 'ruwa',\n",
    "                'clothes': 'tufafi',\n",
    "                'shop': 'shago',\n",
    "                'business': 'kasuwanci',\n",
    "                'trade': 'ciniki',\n",
    "                'company': 'kamfani'\n",
    "            },\n",
    "            'english_to_yoruba': {\n",
    "                'market': 'oja',\n",
    "                'house': 'ile',\n",
    "                'food': 'ounje',\n",
    "                'water': 'omi',\n",
    "                'clothes': 'aso',\n",
    "                'shop': 'ile itaja',\n",
    "                'new': 'titun',\n",
    "                'business': 'owo',\n",
    "                'trade': 'isowo',\n",
    "                'company': 'ile-ise'\n",
    "            },\n",
    "            'hausa_to_english': {},\n",
    "            'yoruba_to_english': {}\n",
    "        }\n",
    "        \n",
    "        # Create reverse mappings\n",
    "        for eng, hausa in dictionary['english_to_hausa'].items():\n",
    "            dictionary['hausa_to_english'][hausa] = eng\n",
    "        for eng, yoruba in dictionary['english_to_yoruba'].items():\n",
    "            dictionary['yoruba_to_english'][yoruba] = eng\n",
    "            \n",
    "        return dictionary\n",
    "    \n",
    "    def _load_domain_lexicon(self) -> Dict:\n",
    "        \"\"\"Load domain-specific business/trademark lexicon\"\"\"\n",
    "        return {\n",
    "            'tech_terms': ['software', 'hardware', 'digital', 'cyber', 'cloud', 'data'],\n",
    "            'fashion_terms': ['fashion', 'style', 'wear', 'dress', 'clothes', 'apparel'],\n",
    "            'food_terms': ['food', 'restaurant', 'cafe', 'dining', 'cuisine', 'meal'],\n",
    "            'finance_terms': ['bank', 'finance', 'money', 'investment', 'capital', 'fund']\n",
    "        }\n",
    "    \n",
    "    def get_synonyms(self, word: str) -> Set[str]:\n",
    "        \"\"\"Get English synonyms using WordNet\"\"\"\n",
    "        synonyms = set()\n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        for syn in wordnet.synsets(word_lower):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace('_', ' ')\n",
    "                if synonym != word_lower:\n",
    "                    synonyms.add(synonym)\n",
    "        \n",
    "        return synonyms\n",
    "    \n",
    "    def get_antonyms(self, word: str) -> Set[str]:\n",
    "        \"\"\"Get English antonyms using WordNet\"\"\"\n",
    "        antonyms = set()\n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        for syn in wordnet.synsets(word_lower):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name().replace('_', ' '))\n",
    "        \n",
    "        return antonyms\n",
    "    \n",
    "    def translate_word(self, word: str, target_lang: str) -> Optional[str]:\n",
    "        \"\"\"Translate word to Hausa or Yoruba\"\"\"\n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        if target_lang == 'hausa':\n",
    "            return self.hausa_yoruba_dict['english_to_hausa'].get(word_lower)\n",
    "        elif target_lang == 'yoruba':\n",
    "            return self.hausa_yoruba_dict['english_to_yoruba'].get(word_lower)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def expand_with_translations(self, text: str) -> Dict[str, Set[str]]:\n",
    "        \"\"\"Expand text with translations and variants\"\"\"\n",
    "        words = text.lower().split()\n",
    "        expansions = {\n",
    "            'original': {text},\n",
    "            'synonyms': set(),\n",
    "            'antonyms': set(),\n",
    "            'hausa': set(),\n",
    "            'yoruba': set()\n",
    "        }\n",
    "        \n",
    "        for word in words:\n",
    "            # Get synonyms and antonyms\n",
    "            expansions['synonyms'].update(self.get_synonyms(word))\n",
    "            expansions['antonyms'].update(self.get_antonyms(word))\n",
    "            \n",
    "            # Get translations\n",
    "            hausa_trans = self.translate_word(word, 'hausa')\n",
    "            if hausa_trans:\n",
    "                expansions['hausa'].add(hausa_trans)\n",
    "            \n",
    "            yoruba_trans = self.translate_word(word, 'yoruba')\n",
    "            if yoruba_trans:\n",
    "                expansions['yoruba'].add(yoruba_trans)\n",
    "        \n",
    "        return expansions\n",
    "    \n",
    "    def calculate_expansion_overlap(self, text1: str, text2: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate overlap scores between expanded sets of two texts\"\"\"\n",
    "        exp1 = self.expand_with_translations(text1)\n",
    "        exp2 = self.expand_with_translations(text2)\n",
    "        \n",
    "        overlap_scores = {}\n",
    "        \n",
    "        for key in exp1:\n",
    "            if exp1[key] and exp2[key]:  # Both sets are non-empty\n",
    "                intersection = len(exp1[key].intersection(exp2[key]))\n",
    "                union = len(exp1[key].union(exp2[key]))\n",
    "                overlap_scores[f'{key}_overlap'] = intersection / union if union > 0 else 0\n",
    "            else:\n",
    "                overlap_scores[f'{key}_overlap'] = 0\n",
    "        \n",
    "        return overlap_scores\n",
    "    \n",
    "    def extract_linguistic_features(self, text1: str, text2: str) -> np.ndarray:\n",
    "        \"\"\"Extract all linguistic features between two texts\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. Expansion overlaps\n",
    "        overlaps = self.calculate_expansion_overlap(text1, text2)\n",
    "        features.extend(overlaps.values())\n",
    "        \n",
    "        # 2. Direct word overlap\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        word_overlap = len(words1.intersection(words2)) / max(len(words1), len(words2), 1)\n",
    "        features.append(word_overlap)\n",
    "        \n",
    "        # 3. Character n-gram similarity\n",
    "        prep = TextPreprocessor()\n",
    "        ngrams1 = set(prep.create_ngrams(text1, n=3))\n",
    "        ngrams2 = set(prep.create_ngrams(text2, n=3))\n",
    "        ngram_similarity = len(ngrams1.intersection(ngrams2)) / max(len(ngrams1), len(ngrams2), 1)\n",
    "        features.append(ngram_similarity)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "# Test linguistic features\n",
    "ling_extractor = LinguisticFeatureExtractor()\n",
    "\n",
    "# Test examples\n",
    "test_pairs = [\n",
    "    (\"Market Place\", \"Kasuwa Shop\"),  # English-Hausa mix\n",
    "    (\"Fashion House\", \"Ile Aso\"),      # English-Yoruba mix\n",
    "    (\"Digital Bank\", \"Cyber Finance\"),  # Synonyms\n",
    "]\n",
    "\n",
    "print(\"Linguistic Feature Extraction Examples:\")\n",
    "print(\"-\" * 60)\n",
    "for text1, text2 in test_pairs:\n",
    "    print(f\"\\nComparing: '{text1}' vs '{text2}'\")\n",
    "    \n",
    "    # Get expansions\n",
    "    exp1 = ling_extractor.expand_with_translations(text1)\n",
    "    exp2 = ling_extractor.expand_with_translations(text2)\n",
    "    \n",
    "    print(f\"Text1 expansions: {exp1}\")\n",
    "    print(f\"Text2 expansions: {exp2}\")\n",
    "    \n",
    "    # Calculate overlaps\n",
    "    overlaps = ling_extractor.calculate_expansion_overlap(text1, text2)\n",
    "    print(f\"Overlap scores: {overlaps}\")\n",
    "    \n",
    "    # Extract full feature vector\n",
    "    features = ling_extractor.extract_linguistic_features(text1, text2)\n",
    "    print(f\"Feature vector shape: {features.shape}\")\n",
    "    print(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e03f81",
   "metadata": {},
   "source": [
    "# 5. Phonetic Similarity Features\n",
    "\n",
    "Implement phonetic algorithms for detecting similar-sounding trademarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneticSimilarityExtractor:\n",
    "    \"\"\"Extract phonetic similarity features for trademark comparison\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def get_soundex(self, text: str) -> str:\n",
    "        \"\"\"Get Soundex encoding\"\"\"\n",
    "        # Clean text for phonetic analysis\n",
    "        text = re.sub(r'[^a-zA-Z]', '', text)\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        if text in self.cache:\n",
    "            return self.cache[text]['soundex']\n",
    "        \n",
    "        try:\n",
    "            soundex_code = soundex(text)\n",
    "        except:\n",
    "            soundex_code = \"\"\n",
    "        \n",
    "        if text not in self.cache:\n",
    "            self.cache[text] = {}\n",
    "        self.cache[text]['soundex'] = soundex_code\n",
    "        \n",
    "        return soundex_code\n",
    "    \n",
    "    def get_metaphone(self, text: str) -> str:\n",
    "        \"\"\"Get Metaphone encoding\"\"\"\n",
    "        text = re.sub(r'[^a-zA-Z]', '', text)\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        if text in self.cache and 'metaphone' in self.cache[text]:\n",
    "            return self.cache[text]['metaphone']\n",
    "        \n",
    "        try:\n",
    "            metaphone_code = metaphone(text)\n",
    "        except:\n",
    "            metaphone_code = \"\"\n",
    "        \n",
    "        if text not in self.cache:\n",
    "            self.cache[text] = {}\n",
    "        self.cache[text]['metaphone'] = metaphone_code\n",
    "        \n",
    "        return metaphone_code\n",
    "    \n",
    "    def get_double_metaphone(self, text: str) -> Tuple[str, str]:\n",
    "        \"\"\"Get Double Metaphone encoding (more advanced)\"\"\"\n",
    "        # Simplified implementation - in production use jellyfish.dmetaphone\n",
    "        primary = self.get_metaphone(text)\n",
    "        # Generate alternative by slightly modifying the text\n",
    "        alt_text = text.replace('ph', 'f').replace('gh', 'g')\n",
    "        secondary = self.get_metaphone(alt_text)\n",
    "        return primary, secondary\n",
    "    \n",
    "    def levenshtein_distance(self, text1: str, text2: str) -> int:\n",
    "        \"\"\"Calculate Levenshtein edit distance\"\"\"\n",
    "        return Levenshtein.distance(text1.lower(), text2.lower())\n",
    "    \n",
    "    def jaro_winkler_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate Jaro-Winkler similarity (good for short strings)\"\"\"\n",
    "        return Levenshtein.jaro_winkler(text1.lower(), text2.lower())\n",
    "    \n",
    "    def phonetic_distance(self, text1: str, text2: str, method: str = 'soundex') -> int:\n",
    "        \"\"\"Calculate distance between phonetic encodings\"\"\"\n",
    "        if method == 'soundex':\n",
    "            code1 = self.get_soundex(text1)\n",
    "            code2 = self.get_soundex(text2)\n",
    "        elif method == 'metaphone':\n",
    "            code1 = self.get_metaphone(text1)\n",
    "            code2 = self.get_metaphone(text2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        if not code1 or not code2:\n",
    "            return 999  # Large distance for empty codes\n",
    "        \n",
    "        return self.levenshtein_distance(code1, code2)\n",
    "    \n",
    "    def extract_phonetic_features(self, text1: str, text2: str) -> np.ndarray:\n",
    "        \"\"\"Extract comprehensive phonetic similarity features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. Soundex match\n",
    "        soundex1 = self.get_soundex(text1)\n",
    "        soundex2 = self.get_soundex(text2)\n",
    "        features.append(1.0 if soundex1 == soundex2 and soundex1 != \"\" else 0.0)\n",
    "        \n",
    "        # 2. Metaphone match\n",
    "        meta1 = self.get_metaphone(text1)\n",
    "        meta2 = self.get_metaphone(text2)\n",
    "        features.append(1.0 if meta1 == meta2 and meta1 != \"\" else 0.0)\n",
    "        \n",
    "        # 3. Soundex distance (normalized)\n",
    "        soundex_dist = self.phonetic_distance(text1, text2, 'soundex')\n",
    "        features.append(1.0 / (1.0 + soundex_dist))  # Convert to similarity\n",
    "        \n",
    "        # 4. Metaphone distance (normalized)\n",
    "        meta_dist = self.phonetic_distance(text1, text2, 'metaphone')\n",
    "        features.append(1.0 / (1.0 + meta_dist))\n",
    "        \n",
    "        # 5. Raw Levenshtein distance (normalized)\n",
    "        lev_dist = self.levenshtein_distance(text1, text2)\n",
    "        max_len = max(len(text1), len(text2))\n",
    "        features.append(1.0 - (lev_dist / max_len) if max_len > 0 else 0.0)\n",
    "        \n",
    "        # 6. Jaro-Winkler similarity\n",
    "        jw_sim = self.jaro_winkler_similarity(text1, text2)\n",
    "        features.append(jw_sim)\n",
    "        \n",
    "        # 7. Double Metaphone comparison\n",
    "        dm1_primary, dm1_secondary = self.get_double_metaphone(text1)\n",
    "        dm2_primary, dm2_secondary = self.get_double_metaphone(text2)\n",
    "        \n",
    "        # Check if any combination matches\n",
    "        dm_match = (dm1_primary == dm2_primary or \n",
    "                   dm1_primary == dm2_secondary or\n",
    "                   dm1_secondary == dm2_primary or\n",
    "                   dm1_secondary == dm2_secondary)\n",
    "        features.append(1.0 if dm_match else 0.0)\n",
    "        \n",
    "        # 8. Length similarity\n",
    "        len_sim = 1.0 - abs(len(text1) - len(text2)) / max(len(text1), len(text2))\n",
    "        features.append(len_sim)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "# Test phonetic features\n",
    "phonetic_extractor = PhoneticSimilarityExtractor()\n",
    "\n",
    "# Test cases with phonetically similar trademarks\n",
    "test_cases = [\n",
    "    (\"Coca Cola\", \"Koka Kola\"),      # Phonetically similar\n",
    "    (\"Nike\", \"Nyke\"),                 # Sound-alike\n",
    "    (\"Adidas\", \"Addidas\"),           # Misspelling\n",
    "    (\"Microsoft\", \"Mikrosoft\"),      # Alternative spelling\n",
    "    (\"Apple\", \"Aple\"),               # Missing letter\n",
    "    (\"Samsung\", \"Samsong\"),          # Letter substitution\n",
    "    (\"Toyota\", \"Toyoda\"),            # Historical variant\n",
    "]\n",
    "\n",
    "print(\"Phonetic Similarity Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "for text1, text2 in test_cases:\n",
    "    print(f\"\\nComparing: '{text1}' vs '{text2}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Individual phonetic encodings\n",
    "    print(f\"Soundex: {phonetic_extractor.get_soundex(text1)} vs {phonetic_extractor.get_soundex(text2)}\")\n",
    "    print(f\"Metaphone: {phonetic_extractor.get_metaphone(text1)} vs {phonetic_extractor.get_metaphone(text2)}\")\n",
    "    \n",
    "    # Distances and similarities\n",
    "    print(f\"Levenshtein distance: {phonetic_extractor.levenshtein_distance(text1, text2)}\")\n",
    "    print(f\"Jaro-Winkler similarity: {phonetic_extractor.jaro_winkler_similarity(text1, text2):.3f}\")\n",
    "    \n",
    "    # Full feature vector\n",
    "    features = phonetic_extractor.extract_phonetic_features(text1, text2)\n",
    "    print(f\"Phonetic features: {features}\")\n",
    "    print(f\"Overall phonetic similarity score: {np.mean(features):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185df531",
   "metadata": {},
   "source": [
    "# 6. Multilingual Embeddings (EN/HA/YO)\n",
    "\n",
    "Implement multilingual semantic similarity using transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualEmbeddings:\n",
    "    \"\"\"Multilingual semantic embeddings for EN/HA/YO\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with a multilingual model.\n",
    "        Alternative models:\n",
    "        - 'bert-base-multilingual-cased'\n",
    "        - 'xlm-roberta-base'\n",
    "        - 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.use_sentence_transformer = True\n",
    "        except ImportError:\n",
    "            # Fallback to basic transformers\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "            self.model = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "            self.use_sentence_transformer = False\n",
    "        \n",
    "        self.embedding_cache = {}\n",
    "        \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Simple language detection based on character patterns\"\"\"\n",
    "        # Simplified detection - in production use langdetect or polyglot\n",
    "        hausa_indicators = ['ka', 'ki', 'ku', 'ta', 'ya', 'su', 'gida', 'kasuwa']\n",
    "        yoruba_indicators = ['ọ', 'ẹ', 'ṣ', 'ni', 'ti', 'oja', 'ile']\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for Yoruba diacritics\n",
    "        if any(char in text_lower for char in 'ọẹṣ'):\n",
    "            return 'yoruba'\n",
    "        \n",
    "        # Check for common words\n",
    "        for indicator in hausa_indicators:\n",
    "            if indicator in text_lower:\n",
    "                return 'hausa'\n",
    "        \n",
    "        for indicator in yoruba_indicators:\n",
    "            if indicator in text_lower:\n",
    "                return 'yoruba'\n",
    "        \n",
    "        return 'english'\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get semantic embedding for text\"\"\"\n",
    "        if text in self.embedding_cache:\n",
    "            return self.embedding_cache[text]\n",
    "        \n",
    "        if self.use_sentence_transformer:\n",
    "            # Use sentence-transformers\n",
    "            embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "        else:\n",
    "            # Use basic transformers\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', \n",
    "                                   truncation=True, padding=True, max_length=128)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                # Use mean pooling\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).numpy()[0]\n",
    "        \n",
    "        self.embedding_cache[text] = embedding\n",
    "        return embedding\n",
    "    \n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two texts\"\"\"\n",
    "        emb1 = self.get_embedding(text1)\n",
    "        emb2 = self.get_embedding(text2)\n",
    "        \n",
    "        return self.cosine_similarity(emb1, emb2)\n",
    "    \n",
    "    def cross_lingual_similarity(self, text1: str, text2: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate similarity with language detection\"\"\"\n",
    "        lang1 = self.detect_language(text1)\n",
    "        lang2 = self.detect_language(text2)\n",
    "        \n",
    "        similarity = self.semantic_similarity(text1, text2)\n",
    "        \n",
    "        return {\n",
    "            'similarity': similarity,\n",
    "            'lang1': lang1,\n",
    "            'lang2': lang2,\n",
    "            'cross_lingual': lang1 != lang2\n",
    "        }\n",
    "    \n",
    "    def extract_multilingual_features(self, text1: str, text2: str) -> np.ndarray:\n",
    "        \"\"\"Extract multilingual semantic features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. Direct semantic similarity\n",
    "        similarity = self.semantic_similarity(text1, text2)\n",
    "        features.append(similarity)\n",
    "        \n",
    "        # 2. Language detection features\n",
    "        lang1 = self.detect_language(text1)\n",
    "        lang2 = self.detect_language(text2)\n",
    "        \n",
    "        # One-hot encode languages\n",
    "        languages = ['english', 'hausa', 'yoruba']\n",
    "        for lang in languages:\n",
    "            features.append(1.0 if lang1 == lang else 0.0)\n",
    "        for lang in languages:\n",
    "            features.append(1.0 if lang2 == lang else 0.0)\n",
    "        \n",
    "        # 3. Cross-lingual flag\n",
    "        features.append(1.0 if lang1 != lang2 else 0.0)\n",
    "        \n",
    "        # 4. Embedding statistics\n",
    "        emb1 = self.get_embedding(text1)\n",
    "        emb2 = self.get_embedding(text2)\n",
    "        \n",
    "        # Euclidean distance\n",
    "        euclidean_dist = np.linalg.norm(emb1 - emb2)\n",
    "        features.append(1.0 / (1.0 + euclidean_dist))  # Convert to similarity\n",
    "        \n",
    "        # Manhattan distance\n",
    "        manhattan_dist = np.sum(np.abs(emb1 - emb2))\n",
    "        features.append(1.0 / (1.0 + manhattan_dist))\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "# Initialize multilingual embeddings (using a smaller model for demo)\n",
    "print(\"Initializing multilingual embeddings...\")\n",
    "ml_embeddings = MultilingualEmbeddings()\n",
    "\n",
    "# Test multilingual similarity\n",
    "test_pairs = [\n",
    "    (\"Market Place\", \"Kasuwa\"),       # English-Hausa\n",
    "    (\"House of Fashion\", \"Ile Aso\"),   # English-Yoruba  \n",
    "    (\"Food Market\", \"Gidan Abinci\"),   # English-Hausa\n",
    "    (\"New Market\", \"Ọjà Titun\"),       # English-Yoruba\n",
    "    (\"Bank\", \"Bank\"),                  # Same word\n",
    "    (\"Digital Finance\", \"Cyber Banking\"), # Semantic similarity\n",
    "]\n",
    "\n",
    "print(\"\\nMultilingual Similarity Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "for text1, text2 in test_pairs:\n",
    "    print(f\"\\nComparing: '{text1}' vs '{text2}'\")\n",
    "    \n",
    "    # Get cross-lingual analysis\n",
    "    analysis = ml_embeddings.cross_lingual_similarity(text1, text2)\n",
    "    print(f\"Languages: {analysis['lang1']} vs {analysis['lang2']}\")\n",
    "    print(f\"Cross-lingual: {analysis['cross_lingual']}\")\n",
    "    print(f\"Semantic similarity: {analysis['similarity']:.3f}\")\n",
    "    \n",
    "    # Get full feature vector\n",
    "    features = ml_embeddings.extract_multilingual_features(text1, text2)\n",
    "    print(f\"Feature vector shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751b3ee",
   "metadata": {},
   "source": [
    "# 7. SVM Classifier Training\n",
    "\n",
    "Combine CNN embeddings with engineered features and train the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSVMClassifier:\n",
    "    \"\"\"SVM classifier using combined CNN and linguistic features\"\"\"\n",
    "    \n",
    "    def __init__(self, cnn_model, linguistic_extractor, phonetic_extractor, ml_embeddings):\n",
    "        self.cnn_model = cnn_model\n",
    "        self.linguistic_extractor = linguistic_extractor\n",
    "        self.phonetic_extractor = phonetic_extractor\n",
    "        self.ml_embeddings = ml_embeddings\n",
    "        self.svm_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def extract_all_features(self, text1: str, text2: str) -> np.ndarray:\n",
    "        \"\"\"Extract all features for a pair of trademarks\"\"\"\n",
    "        all_features = []\n",
    "        \n",
    "        # 1. CNN embeddings\n",
    "        cnn_emb1 = self.cnn_model.extract_embeddings([text1])[0]\n",
    "        cnn_emb2 = self.cnn_model.extract_embeddings([text2])[0]\n",
    "        \n",
    "        # CNN embedding similarity features\n",
    "        cnn_cosine = self.ml_embeddings.cosine_similarity(cnn_emb1, cnn_emb2)\n",
    "        cnn_euclidean = np.linalg.norm(cnn_emb1 - cnn_emb2)\n",
    "        cnn_manhattan = np.sum(np.abs(cnn_emb1 - cnn_emb2))\n",
    "        \n",
    "        all_features.extend([cnn_cosine, 1.0/(1.0 + cnn_euclidean), 1.0/(1.0 + cnn_manhattan)])\n",
    "        \n",
    "        # 2. Concatenated CNN embeddings (optional - can be memory intensive)\n",
    "        # all_features.extend(cnn_emb1)\n",
    "        # all_features.extend(cnn_emb2)\n",
    "        \n",
    "        # 3. Linguistic features\n",
    "        ling_features = self.linguistic_extractor.extract_linguistic_features(text1, text2)\n",
    "        all_features.extend(ling_features)\n",
    "        \n",
    "        # 4. Phonetic features\n",
    "        phonetic_features = self.phonetic_extractor.extract_phonetic_features(text1, text2)\n",
    "        all_features.extend(phonetic_features)\n",
    "        \n",
    "        # 5. Multilingual features\n",
    "        ml_features = self.ml_embeddings.extract_multilingual_features(text1, text2)\n",
    "        all_features.extend(ml_features)\n",
    "        \n",
    "        return np.array(all_features)\n",
    "    \n",
    "    def prepare_training_data(self, X_data, y_data, pairs=None):\n",
    "        \"\"\"Prepare feature matrix for training\"\"\"\n",
    "        if pairs is None:\n",
    "            # Generate pairs from the data\n",
    "            # For demo, we'll create synthetic pairs\n",
    "            pairs = []\n",
    "            for i in range(len(X_data)):\n",
    "                # Positive pair (similar)\n",
    "                if i < len(X_data) - 1:\n",
    "                    pairs.append((X_data.iloc[i]['wordmark'], \n",
    "                                X_data.iloc[i+1]['wordmark'],\n",
    "                                y_data[i]))\n",
    "        else:\n",
    "            # Use provided pairs\n",
    "            pairs = [(row['mark1'], row['mark2'], row['label']) \n",
    "                    for _, row in pairs.iterrows()]\n",
    "        \n",
    "        # Extract features for all pairs\n",
    "        X_features = []\n",
    "        y_labels = []\n",
    "        \n",
    "        print(\"Extracting features for training pairs...\")\n",
    "        for mark1, mark2, label in tqdm(pairs[:100]):  # Limit for demo\n",
    "            try:\n",
    "                features = self.extract_all_features(mark1, mark2)\n",
    "                X_features.append(features)\n",
    "                y_labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pair ({mark1}, {mark2}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        return np.array(X_features), np.array(y_labels)\n",
    "    \n",
    "    def train(self, X_train_features, y_train, param_grid=None):\n",
    "        \"\"\"Train SVM with hyperparameter tuning\"\"\"\n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train_features)\n",
    "        \n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "            }\n",
    "        \n",
    "        # Grid search with cross-validation\n",
    "        print(\"Performing grid search for SVM hyperparameters...\")\n",
    "        svm = SVC(probability=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            svm, param_grid, cv=3, \n",
    "            scoring='f1', n_jobs=-1, verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        self.svm_model = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "        \n",
    "        return grid_search\n",
    "    \n",
    "    def predict(self, text1: str, text2: str) -> Tuple[int, float]:\n",
    "        \"\"\"Predict similarity for a trademark pair\"\"\"\n",
    "        features = self.extract_all_features(text1, text2)\n",
    "        features_scaled = self.scaler.transform([features])\n",
    "        \n",
    "        prediction = self.svm_model.predict(features_scaled)[0]\n",
    "        probability = self.svm_model.predict_proba(features_scaled)[0].max()\n",
    "        \n",
    "        return prediction, probability\n",
    "    \n",
    "    def get_feature_importance(self, X_features, y_labels):\n",
    "        \"\"\"Analyze feature importance for linear SVM\"\"\"\n",
    "        if self.svm_model.kernel != 'linear':\n",
    "            print(\"Feature importance only available for linear kernel\")\n",
    "            return None\n",
    "        \n",
    "        # Get coefficients\n",
    "        coefs = self.svm_model.coef_[0]\n",
    "        \n",
    "        # Create feature importance dataframe\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature_idx': range(len(coefs)),\n",
    "            'importance': np.abs(coefs)\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return feature_importance\n",
    "\n",
    "# Initialize hybrid SVM classifier\n",
    "hybrid_svm = HybridSVMClassifier(\n",
    "    char_cnn,\n",
    "    ling_extractor,\n",
    "    phonetic_extractor,\n",
    "    ml_embeddings\n",
    ")\n",
    "\n",
    "# Create training pairs (for demonstration)\n",
    "print(\"Creating training pairs...\")\n",
    "train_pairs = []\n",
    "for i in range(min(50, len(X_train))):  # Limit for demo\n",
    "    # Create positive and negative pairs\n",
    "    mark1 = X_train.iloc[i]['wordmark']\n",
    "    \n",
    "    # Positive pair (similar mark)\n",
    "    mark2_similar = mark1[:3] + mark1[3:].replace('a', 'e').replace('i', 'y')\n",
    "    train_pairs.append((mark1, mark2_similar, 1))\n",
    "    \n",
    "    # Negative pair (different mark)\n",
    "    j = (i + 10) % len(X_train)\n",
    "    mark2_different = X_train.iloc[j]['wordmark']\n",
    "    train_pairs.append((mark1, mark2_different, 0))\n",
    "\n",
    "train_pairs_df = pd.DataFrame(train_pairs, columns=['mark1', 'mark2', 'label'])\n",
    "\n",
    "# Prepare features\n",
    "X_train_features, y_train_labels = hybrid_svm.prepare_training_data(\n",
    "    X_train, y_train, train_pairs_df\n",
    ")\n",
    "\n",
    "print(f\"Training features shape: {X_train_features.shape}\")\n",
    "print(f\"Training labels shape: {y_train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM classifier\n",
    "grid_search = hybrid_svm.train(X_train_features, y_train_labels)\n",
    "\n",
    "# Display training results\n",
    "pd.DataFrame(grid_search.cv_results_).sort_values('mean_test_score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d1709",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation and Metrics\n",
    "\n",
    "Comprehensive evaluation of the hybrid model with various metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88640b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive evaluation for the hybrid trademark similarity model\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def evaluate_on_test_set(self, X_test_features, y_test):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        # Scale features\n",
    "        X_test_scaled = self.model.scaler.transform(X_test_features)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = self.model.svm_model.predict(X_test_scaled)\n",
    "        y_prob = self.model.svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) == 2 else None\n",
    "        }\n",
    "        \n",
    "        return metrics, y_pred, y_prob\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred, labels=['Not Similar', 'Similar']):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
    "        \n",
    "        if tp + fn > 0:\n",
    "            recall = tp / (tp + fn)\n",
    "            print(f\"Recall (Sensitivity): {recall:.3f}\")\n",
    "        \n",
    "        if tp + fp > 0:\n",
    "            precision = tp / (tp + fp)\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "        \n",
    "        if tn + fp > 0:\n",
    "            specificity = tn / (tn + fp)\n",
    "            print(f\"Specificity: {specificity:.3f}\")\n",
    "    \n",
    "    def plot_roc_curve(self, y_true, y_prob):\n",
    "        \"\"\"Plot ROC curve\"\"\"\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        return thresholds\n",
    "    \n",
    "    def recommend_threshold(self, y_true, y_prob, target_metric='f1'):\n",
    "        \"\"\"Recommend optimal threshold for classification\"\"\"\n",
    "        thresholds = np.linspace(0, 1, 100)\n",
    "        scores = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "            \n",
    "            if target_metric == 'f1':\n",
    "                score = f1_score(y_true, y_pred_thresh)\n",
    "            elif target_metric == 'recall':\n",
    "                score = recall_score(y_true, y_pred_thresh)\n",
    "            elif target_metric == 'precision':\n",
    "                score = precision_score(y_true, y_pred_thresh)\n",
    "            else:\n",
    "                score = accuracy_score(y_true, y_pred_thresh)\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        optimal_idx = np.argmax(scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_score = scores[optimal_idx]\n",
    "        \n",
    "        # Plot threshold vs score\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(thresholds, scores, lw=2)\n",
    "        plt.axvline(x=optimal_threshold, color='r', linestyle='--',\n",
    "                   label=f'Optimal = {optimal_threshold:.3f}')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel(f'{target_metric.capitalize()} Score')\n",
    "        plt.title(f'Threshold Optimization for {target_metric.capitalize()}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "        print(f\"Optimal {target_metric}: {optimal_score:.3f}\")\n",
    "        \n",
    "        return optimal_threshold\n",
    "    \n",
    "    def analyze_errors(self, X_test_pairs, y_true, y_pred, y_prob, threshold=0.5):\n",
    "        \"\"\"Analyze false positives and false negatives\"\"\"\n",
    "        # Identify errors\n",
    "        false_positives = []\n",
    "        false_negatives = []\n",
    "        \n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == 0 and y_pred[i] == 1:\n",
    "                false_positives.append((i, y_prob[i]))\n",
    "            elif y_true[i] == 1 and y_pred[i] == 0:\n",
    "                false_negatives.append((i, y_prob[i]))\n",
    "        \n",
    "        print(f\"False Positives: {len(false_positives)}\")\n",
    "        print(f\"False Negatives: {len(false_negatives)}\")\n",
    "        \n",
    "        # Show examples of errors\n",
    "        if false_positives:\n",
    "            print(\"\\nExample False Positives (marked similar but actually different):\")\n",
    "            for idx, prob in false_positives[:3]:\n",
    "                if idx < len(X_test_pairs):\n",
    "                    print(f\"  Pair {idx}: Confidence = {prob:.3f}\")\n",
    "        \n",
    "        if false_negatives:\n",
    "            print(\"\\nExample False Negatives (marked different but actually similar):\")\n",
    "            for idx, prob in false_negatives[:3]:\n",
    "                if idx < len(X_test_pairs):\n",
    "                    print(f\"  Pair {idx}: Confidence = {prob:.3f}\")\n",
    "    \n",
    "    def create_risk_levels(self, y_prob):\n",
    "        \"\"\"Create risk level categories based on similarity scores\"\"\"\n",
    "        risk_levels = []\n",
    "        \n",
    "        for prob in y_prob:\n",
    "            if prob >= 0.8:\n",
    "                risk_levels.append('High Risk')\n",
    "            elif prob >= 0.5:\n",
    "                risk_levels.append('Medium Risk')\n",
    "            elif prob >= 0.3:\n",
    "                risk_levels.append('Low Risk')\n",
    "            else:\n",
    "                risk_levels.append('Minimal Risk')\n",
    "        \n",
    "        # Plot distribution\n",
    "        risk_counts = pd.Series(risk_levels).value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        risk_counts.plot(kind='bar', color=['red', 'orange', 'yellow', 'green'])\n",
    "        plt.title('Distribution of Trademark Similarity Risk Levels')\n",
    "        plt.xlabel('Risk Level')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "        return risk_levels\n",
    "\n",
    "# Create test pairs\n",
    "print(\"Creating test pairs...\")\n",
    "test_pairs = []\n",
    "for i in range(min(20, len(X_test))):  # Smaller test set for demo\n",
    "    mark1 = X_test.iloc[i]['wordmark']\n",
    "    \n",
    "    # Positive pair\n",
    "    mark2_similar = mark1[:4] + \"co\"\n",
    "    test_pairs.append((mark1, mark2_similar, 1))\n",
    "    \n",
    "    # Negative pair\n",
    "    j = (i + 5) % len(X_test)\n",
    "    mark2_different = X_test.iloc[j]['wordmark']\n",
    "    test_pairs.append((mark1, mark2_different, 0))\n",
    "\n",
    "test_pairs_df = pd.DataFrame(test_pairs, columns=['mark1', 'mark2', 'label'])\n",
    "\n",
    "# Prepare test features\n",
    "print(\"Extracting test features...\")\n",
    "X_test_features, y_test_labels = hybrid_svm.prepare_training_data(\n",
    "    X_test, y_test, test_pairs_df\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = ModelEvaluator(hybrid_svm)\n",
    "metrics, y_pred, y_prob = evaluator.evaluate_on_test_set(X_test_features, y_test_labels)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in metrics.items():\n",
    "    if value is not None:\n",
    "        print(f\"{metric}: {value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
