{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2a977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Use pytorch device_name: cpu\n",
      "INFO: Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "INFO: ================================================================================\n",
      "INFO: STEP 1: TRADEMARK SIMILARITY DATA PREPROCESSING\n",
      "INFO: ================================================================================\n",
      "INFO: üìÇ Loading dataset...\n",
      "INFO: ‚úì Loaded with encoding: MacRoman\n",
      "INFO:    Shape: (18737, 18)\n",
      "INFO: üìã Extracting relevant columns...\n",
      "INFO: üìù Sampled 5 rows (first)\n",
      "INFO: üóëÔ∏è  Removed 1 duplicates\n",
      "INFO: üåç Translating dataset...\n",
      "INFO:    Lexicon: 0 (0.0%)\n",
      "INFO:    Google: 30 (75.0%)\n",
      "INFO: üîÑ Generating negative pairs (ratio: 1.0)...\n",
      "INFO: ‚úì Generated 4 negative pairs\n",
      "INFO: üîç Extracting visual features...\n",
      "INFO: üëÇ Extracting phonetic features...\n",
      "INFO: üß† Extracting semantic features...\n",
      "INFO: ================================================================================\n",
      "INFO: ‚úÖ PREPROCESSING COMPLETE\n",
      "INFO:    Total pairs: 8\n",
      "INFO:    Positive: 4\n",
      "INFO:    Negative: 4\n",
      "INFO:    Features: 24\n",
      "INFO: ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Overview:\n",
      "   Total pairs: 8\n",
      "   Similar pairs: 4 (50.0%)\n",
      "   Dissimilar pairs: 4 (50.0%)\n",
      "   Total features: 24\n",
      "\n",
      "üìè Wordmark Statistics:\n",
      "   Average length: 13.0 characters\n",
      "   Min length: 7\n",
      "   Max length: 20\n",
      "\n",
      "üéØ Feature Quality:\n",
      "   Visual similarity (Jaro-Winkler): 0.371\n",
      "   Phonetic match rate (Soundex): 12.5%\n",
      "   Phonetic match rate (Metaphone): 0.0%\n",
      "   Semantic similarity (EN): 0.526\n",
      "   Semantic similarity (HA): 0.578\n",
      "   Semantic similarity (YO): 0.584\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: üìä All visualizations saved to: eda_visualizations_20260122_115637/\n",
      "INFO: \n",
      "üíæ Final dataset saved: trademark_similarity_dataset_final_2.csv\n",
      "INFO:    Shape: (8, 24)\n",
      "INFO:    File size: 0.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE DATA (First 5 rows)\n",
      "================================================================================\n",
      "      mark1_wordmark    mark2_wordmark  label  visual_levenshtein  soundex_match  metaphone_match  semantic_similarity_en\n",
      "   PADARIA MERCAPv‚Ä¶O         MERCOPv‚Ä¶O      1                   9              1                0                0.882134\n",
      "          OFFER % TV PADARIA MERCAPv‚Ä¶O      0                  15              0                0                0.152453\n",
      "             LeOffer        OFFER % TV      1                  10              0                0                0.248213\n",
      "           MERCOPv‚Ä¶O JAZZ IMOBILIv‚âàRIA      0                  15              0                0                0.260845\n",
      "CANTINETTA DEI FIORI          DI FIORI      1                  12              0                0                0.852821\n",
      "\n",
      "‚úÖ STEP 1 COMPLETE - Ready for Step 2 (CNN Architecture)\n",
      "   Use file: trademark_similarity_dataset_final_2.csv\n",
      "   Visualizations: eda_visualizations_20260122_115637 directory\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: COMPLETE DATA PREPROCESSING & FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "# This notebook prepares trademark similarity data for CNN+SVM training\n",
    "# Output: Single clean CSV with visual/phonetic/semantic features\n",
    "# =============================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from deep_translator import GoogleTranslator\n",
    "import chardet\n",
    "import os\n",
    "import warnings\n",
    "import jellyfish\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "TRADEMARK_LEXICON = {\n",
    "    # Business & Commerce\n",
    "    \"company\": {\"ha\": \"kamfani\", \"yo\": \"il√©-i·π£·∫πÃÅ\"},\n",
    "    \"business\": {\"ha\": \"kasuwanci\", \"yo\": \"i·π£·∫πÃÅ ow√≥\"},\n",
    "    \"market\": {\"ha\": \"kasuwa\", \"yo\": \"·ªçj√†\"},\n",
    "    \"store\": {\"ha\": \"shago\", \"yo\": \"·π£·ªçÃÅ·ªçÃÄb√π\"},\n",
    "    \"brand\": {\"ha\": \"alama\", \"yo\": \"√†m√¨\"},\n",
    "    \n",
    "    # Quality\n",
    "    \"premium\": {\"ha\": \"mai kyau\", \"yo\": \"iyeb√≠ye\"},\n",
    "    \"super\": {\"ha\": \"babba\", \"yo\": \"p√∫p·ªçÃÄ\"},\n",
    "    \"best\": {\"ha\": \"mafi kyau\", \"yo\": \"d√°ra j√πl·ªç\"},\n",
    "    \"gold\": {\"ha\": \"zinariya\", \"yo\": \"w√∫r√†\"},\n",
    "    \"silver\": {\"ha\": \"azurfa\", \"yo\": \"f√†d√°k√†\"},\n",
    "    \n",
    "    # Food & Beverage\n",
    "    \"coffee\": {\"ha\": \"kofi\", \"yo\": \"k·ªçÃÅf√≠\"},\n",
    "    \"tea\": {\"ha\": \"shayi\", \"yo\": \"tii\"},\n",
    "    \"food\": {\"ha\": \"abinci\", \"yo\": \"o√∫nj·∫π\"},\n",
    "    \"restaurant\": {\"ha\": \"gidan cin abinci\", \"yo\": \"il√© o√∫nj·∫π\"},\n",
    "    \n",
    "    # Technology\n",
    "    \"tech\": {\"ha\": \"fasaha\", \"yo\": \"√¨m·ªçÃÄ-·∫πr·ªç\"},\n",
    "    \"digital\": {\"ha\": \"na dijital\", \"yo\": \"on√≠j√¨t√π\"},\n",
    "    \"smart\": {\"ha\": \"mai hankali\", \"yo\": \"ol√≥gb·ªçÃÅn\"},\n",
    "    \"online\": {\"ha\": \"kan layi\", \"yo\": \"l√≥r√≠ ay√©luj√°ra\"},\n",
    "    \n",
    "    # Common terms\n",
    "    \"new\": {\"ha\": \"sabon\", \"yo\": \"t√≠tun\"},\n",
    "    \"fresh\": {\"ha\": \"sabo\", \"yo\": \"tuntun\"},\n",
    "    \"natural\": {\"ha\": \"na halitta\", \"yo\": \"√†d√°y√©b√°\"},\n",
    "}\n",
    "\n",
    "def get_incremented_filename(base_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a filename that doesn't exist yet by appending _1, _2, etc.\n",
    "    Example: 'data.csv' ‚Üí 'data_1.csv' if 'data.csv' exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_name):\n",
    "        return base_name\n",
    "\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    counter = 1\n",
    "    while True:\n",
    "        new_name = f\"{name}_{counter}{ext}\"\n",
    "        if not os.path.exists(new_name):\n",
    "            return new_name\n",
    "        counter += 1\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"Configuration for the entire pipeline\"\"\"\n",
    "    # Data sampling\n",
    "    max_rows: Optional[int] = None  # None = process all rows\n",
    "    sampling_strategy: str = 'first'  # 'first', 'random', or 'all'\n",
    "    \n",
    "    # Translation\n",
    "    translate_to_english: bool = True\n",
    "    translate_to_local: bool = True\n",
    "    \n",
    "    # Cleaning\n",
    "    fix_encoding: bool = True\n",
    "    remove_duplicates: bool = True\n",
    "    handle_missing: str = 'drop'\n",
    "    \n",
    "    # Negative pair generation\n",
    "    negative_ratio: float = 1.0\n",
    "    negative_strategy: str = 'mixed'\n",
    "    \n",
    "    # Output\n",
    "    output_filename: str = 'trademark_similarity_dataset_final.csv'\n",
    "    save_visualizations: bool = True\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def validate_wordmark(text: str) -> bool:\n",
    "    \"\"\"Check if a wordmark is valid (not empty/null)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text = str(text).strip()\n",
    "    return len(text) > 0 and text.lower() not in ['nan', 'none', 'null', '']\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove all rows with invalid wordmarks\"\"\"\n",
    "    initial_len = len(df)\n",
    "    \n",
    "    # Remove rows with missing/invalid wordmarks\n",
    "    if 'mark1_wordmark' in df.columns and 'mark2_wordmark' in df.columns:\n",
    "        df = df[\n",
    "            df['mark1_wordmark'].apply(validate_wordmark) &\n",
    "            df['mark2_wordmark'].apply(validate_wordmark)\n",
    "        ].copy()\n",
    "    \n",
    "    removed = initial_len - len(df)\n",
    "    if removed > 0:\n",
    "        logger.info(f\"üßπ Removed {removed} rows with invalid wordmarks\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED TRANSLATOR\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedTranslator:\n",
    "    \"\"\"Multi-strategy translator: Lexicon ‚Üí HuggingFace ‚Üí Google Translate\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lexicon = TRADEMARK_LEXICON\n",
    "        self.google_translators = {\n",
    "            'ha': GoogleTranslator(source='en', target='ha'),\n",
    "            'yo': GoogleTranslator(source='en', target='yo')\n",
    "        }\n",
    "        self.cache = {}\n",
    "        self.stats = {'lexicon': 0, 'google': 0, 'cache': 0}\n",
    "    \n",
    "    def translate_word(self, word: str, target_lang: str) -> str:\n",
    "        \"\"\"Translate a single word\"\"\"\n",
    "        if not word or not word.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        word_lower = word.lower().strip()\n",
    "        cache_key = f\"{target_lang}:{word_lower}\"\n",
    "        \n",
    "        # Check cache\n",
    "        if cache_key in self.cache:\n",
    "            self.stats['cache'] += 1\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Try lexicon\n",
    "        if word_lower in self.lexicon:\n",
    "            translation = self.lexicon[word_lower].get(target_lang, word)\n",
    "            self.stats['lexicon'] += 1\n",
    "            self.cache[cache_key] = translation\n",
    "            return translation\n",
    "        \n",
    "        # Fallback to Google Translate\n",
    "        try:\n",
    "            translation = self.google_translators[target_lang].translate(word)\n",
    "            self.stats['google'] += 1\n",
    "            self.cache[cache_key] = translation\n",
    "            return translation\n",
    "        except Exception:\n",
    "            return word\n",
    "    \n",
    "    def translate_text(self, text: str, target_lang: str) -> str:\n",
    "        \"\"\"Translate full text\"\"\"\n",
    "        if pd.isna(text) or not str(text).strip():\n",
    "            return \"\"\n",
    "        \n",
    "        words = re.findall(r\"\\b\\w+\\b\", str(text))\n",
    "        translated = [self.translate_word(w, target_lang) for w in words]\n",
    "        return \" \".join(translated)\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE EXTRACTOR (FIXED PHONETIC FEATURES)\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Extract visual, phonetic, and semantic features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    def extract_visual_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract character-level visual similarity features\"\"\"\n",
    "        logger.info(\"üîç Extracting visual features...\")\n",
    "        \n",
    "        df['visual_levenshtein'] = df.apply(\n",
    "            lambda row: jellyfish.levenshtein_distance(\n",
    "                str(row['mark1_wordmark']), \n",
    "                str(row['mark2_wordmark'])\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        df['visual_jaro_winkler'] = df.apply(\n",
    "            lambda row: jellyfish.jaro_winkler_similarity(\n",
    "                str(row['mark1_wordmark']), \n",
    "                str(row['mark2_wordmark'])\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_phonetic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract phonetic similarity features\n",
    "        FIXED: Now uses English translations for better phonetic matching\n",
    "        \"\"\"\n",
    "        logger.info(\"üëÇ Extracting phonetic features...\")\n",
    "        \n",
    "        # Use English translations for phonetic comparison\n",
    "        def safe_soundex(text):\n",
    "            try:\n",
    "                if pd.isna(text) or not str(text).strip():\n",
    "                    return \"\"\n",
    "                return jellyfish.soundex(str(text))\n",
    "            except:\n",
    "                return \"\"\n",
    "        \n",
    "        def safe_metaphone(text):\n",
    "            try:\n",
    "                if pd.isna(text) or not str(text).strip():\n",
    "                    return \"\"\n",
    "                return jellyfish.metaphone(str(text))\n",
    "            except:\n",
    "                return \"\"\n",
    "        \n",
    "        # Soundex comparison on English wordmarks\n",
    "        df['soundex_match'] = df.apply(\n",
    "            lambda row: int(\n",
    "                safe_soundex(row.get('mark1_wordmark_en', '')) == \n",
    "                safe_soundex(row.get('mark2_wordmark_en', ''))\n",
    "                and safe_soundex(row.get('mark1_wordmark_en', '')) != \"\"\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Metaphone comparison on English wordmarks\n",
    "        df['metaphone_match'] = df.apply(\n",
    "            lambda row: int(\n",
    "                safe_metaphone(row.get('mark1_wordmark_en', '')) == \n",
    "                safe_metaphone(row.get('mark2_wordmark_en', ''))\n",
    "                and safe_metaphone(row.get('mark1_wordmark_en', '')) != \"\"\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _safe_encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Safely encode texts handling NaN/empty values\"\"\"\n",
    "        clean_texts = [str(t) if pd.notna(t) and str(t).strip() else \"\" for t in texts]\n",
    "        return self.semantic_model.encode(clean_texts, show_progress_bar=False)\n",
    "    \n",
    "    def extract_semantic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract semantic similarity features for all languages\"\"\"\n",
    "        logger.info(\"üß† Extracting semantic features...\")\n",
    "        \n",
    "        for lang in ['en', 'ha', 'yo']:\n",
    "            col1 = f'mark1_wordmark_{lang}'\n",
    "            col2 = f'mark2_wordmark_{lang}'\n",
    "            \n",
    "            if col1 in df.columns and col2 in df.columns:\n",
    "                emb1 = self._safe_encode(df[col1].tolist())\n",
    "                emb2 = self._safe_encode(df[col2].tolist())\n",
    "                \n",
    "                df[f'semantic_similarity_{lang}'] = [\n",
    "                    float(cosine_similarity([e1], [e2])[0][0])\n",
    "                    for e1, e2 in zip(emb1, emb2)\n",
    "                ]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_all(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract all features\"\"\"\n",
    "        df = self.extract_visual_features(df)\n",
    "        df = self.extract_phonetic_features(df)\n",
    "        df = self.extract_semantic_features(df)\n",
    "        return df\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PREPROCESSOR\n",
    "# =============================================================================\n",
    "\n",
    "class TrademarkPreprocessor:\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PreprocessingConfig = None):\n",
    "        self.config = config or PreprocessingConfig()\n",
    "        self.translator = EnhancedTranslator()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.pt_to_en = GoogleTranslator(source='pt', target='en')\n",
    "        self.cache = {}\n",
    "    \n",
    "    def load_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV with encoding detection\"\"\"\n",
    "        logger.info(\"üìÇ Loading dataset...\")\n",
    "        \n",
    "        # Detect encoding\n",
    "        with open(file_path, 'rb') as f:\n",
    "            result = chardet.detect(f.read(100000))\n",
    "        encoding = result['encoding']\n",
    "        \n",
    "        # Try multiple encodings\n",
    "        for enc in [encoding, 'MacRoman', 'utf-8', 'latin-1']:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc)\n",
    "                logger.info(f\"‚úì Loaded with encoding: {enc}\")\n",
    "                logger.info(f\"   Shape: {df.shape}\")\n",
    "                return df\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        raise ValueError(\"Could not load CSV with any encoding\")\n",
    "    \n",
    "    def extract_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract and rename relevant columns (excluding class info)\"\"\"\n",
    "        logger.info(\"üìã Extracting relevant columns...\")\n",
    "        \n",
    "        column_mapping = {\n",
    "            'Process number RM': 'mark1_id',\n",
    "            'Process number TM': 'mark2_id',\n",
    "            'Name RM': 'mark1_wordmark',\n",
    "            'Name TM': 'mark2_wordmark',\n",
    "            'Status RM': 'mark1_status',\n",
    "            'Status TM': 'mark2_status',\n",
    "        }\n",
    "        \n",
    "        available = [c for c in column_mapping.keys() if c in df.columns]\n",
    "        df_proc = df[available].copy()\n",
    "        df_proc.rename(columns=column_mapping, inplace=True)\n",
    "        df_proc['label'] = 1  # All are similar pairs\n",
    "        \n",
    "        return df_proc\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text\"\"\"\n",
    "        if pd.isna(text) or not str(text).strip():\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Fix encoding issues\n",
    "        fixes = {\n",
    "            'vÔøΩ': '√ß', 'vÔøΩo': '√ß√£o', 'vÔøΩ': '√≠', 'v=': '√≥', '?': ''\n",
    "        }\n",
    "        for bad, good in fixes.items():\n",
    "            text = text.replace(bad, good)\n",
    "        \n",
    "        return ' '.join(text.split()).strip()\n",
    "    \n",
    "    def translate_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Translate wordmarks to English, Hausa, Yoruba\"\"\"\n",
    "        logger.info(\"üåç Translating dataset...\")\n",
    "        \n",
    "        for col in ['mark1_wordmark', 'mark2_wordmark']:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            # Clean first\n",
    "            df[col] = df[col].apply(self.clean_text)\n",
    "            \n",
    "            # Portuguese ‚Üí English\n",
    "            en_col = col.replace('_wordmark', '_wordmark_en')\n",
    "            df[en_col] = df[col].apply(\n",
    "                lambda x: self.pt_to_en.translate(x) if x else \"\"\n",
    "            )\n",
    "            \n",
    "            # English ‚Üí Hausa & Yoruba\n",
    "            for lang in ['ha', 'yo']:\n",
    "                lang_col = col.replace('_wordmark', f'_wordmark_{lang}')\n",
    "                df[lang_col] = df[en_col].apply(\n",
    "                    lambda x: self.translator.translate_text(x, lang)\n",
    "                )\n",
    "        \n",
    "        # Show translation stats\n",
    "        stats = self.translator.stats\n",
    "        total = sum(stats.values())\n",
    "        if total > 0:\n",
    "            logger.info(f\"   Lexicon: {stats['lexicon']} ({stats['lexicon']/total*100:.1f}%)\")\n",
    "            logger.info(f\"   Google: {stats['google']} ({stats['google']/total*100:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def generate_negative_pairs(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate dissimilar pairs (without class filtering)\"\"\"\n",
    "        logger.info(f\"üîÑ Generating negative pairs (ratio: {self.config.negative_ratio})...\")\n",
    "        \n",
    "        # Get valid wordmarks\n",
    "        all_marks = list(set(\n",
    "            df['mark1_wordmark'].tolist() + df['mark2_wordmark'].tolist()\n",
    "        ))\n",
    "        all_marks = [m for m in all_marks if validate_wordmark(m)]\n",
    "        \n",
    "        if len(all_marks) < 2:\n",
    "            logger.warning(\"‚ö† Not enough wordmarks for negative pairs\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Track existing pairs\n",
    "        existing = set()\n",
    "        for _, row in df.iterrows():\n",
    "            pair = tuple(sorted([\n",
    "                str(row['mark1_wordmark']),\n",
    "                str(row['mark2_wordmark'])\n",
    "            ]))\n",
    "            existing.add(pair)\n",
    "        \n",
    "        # Generate negatives\n",
    "        n_negative = int(len(df) * self.config.negative_ratio)\n",
    "        negatives = []\n",
    "        attempts = 0\n",
    "        max_attempts = n_negative * 100\n",
    "        \n",
    "        while len(negatives) < n_negative and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "            \n",
    "            mark1 = np.random.choice(all_marks)\n",
    "            mark2 = np.random.choice(all_marks)\n",
    "            \n",
    "            # Validate\n",
    "            if mark1 == mark2:\n",
    "                continue\n",
    "            \n",
    "            pair = tuple(sorted([mark1, mark2]))\n",
    "            if pair in existing:\n",
    "                continue\n",
    "            \n",
    "            # Length difference check for dissimilarity\n",
    "            if self.config.negative_strategy == 'mixed':\n",
    "                len_diff = abs(len(mark1) - len(mark2))\n",
    "                if len_diff < 3:  # Must have some length difference\n",
    "                    continue\n",
    "            \n",
    "            existing.add(pair)\n",
    "            negatives.append({\n",
    "                'mark1_wordmark': mark1,\n",
    "                'mark2_wordmark': mark2,\n",
    "                'label': 0,\n",
    "                'pair_type': 'negative'\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"‚úì Generated {len(negatives)} negative pairs\")\n",
    "        return pd.DataFrame(negatives)\n",
    "    \n",
    "    def add_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add basic statistical features\"\"\"\n",
    "        df['mark1_length'] = df['mark1_wordmark'].str.len()\n",
    "        df['mark2_length'] = df['mark2_wordmark'].str.len()\n",
    "        df['length_diff'] = abs(df['mark1_length'] - df['mark2_length'])\n",
    "        return df\n",
    "    \n",
    "    def process(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Complete pipeline\"\"\"\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"STEP 1: TRADEMARK SIMILARITY DATA PREPROCESSING\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # 1. Load\n",
    "        df = self.load_data(file_path)\n",
    "        \n",
    "        # 2. Extract columns (no class info)\n",
    "        df = self.extract_columns(df)\n",
    "        \n",
    "        # 3. Sample (if configured)\n",
    "        if self.config.max_rows and self.config.sampling_strategy != 'all':\n",
    "            n = min(self.config.max_rows, len(df))\n",
    "            if self.config.sampling_strategy == 'first':\n",
    "                df = df.head(n).copy()\n",
    "            elif self.config.sampling_strategy == 'random':\n",
    "                df = df.sample(n=n, random_state=42).copy()\n",
    "            logger.info(f\"üìù Sampled {n} rows ({self.config.sampling_strategy})\")\n",
    "        \n",
    "        # 4. Clean\n",
    "        df = clean_dataframe(df)\n",
    "        \n",
    "        # 5. Remove duplicates\n",
    "        initial = len(df)\n",
    "        df['_pair_key'] = df.apply(\n",
    "            lambda r: tuple(sorted([\n",
    "                str(r['mark1_wordmark']),\n",
    "                str(r['mark2_wordmark'])\n",
    "            ])), axis=1\n",
    "        )\n",
    "        df = df.drop_duplicates(subset='_pair_key').drop('_pair_key', axis=1)\n",
    "        logger.info(f\"üóëÔ∏è  Removed {initial - len(df)} duplicates\")\n",
    "        \n",
    "        # 6. Translate\n",
    "        df['pair_type'] = 'positive'\n",
    "        df = self.translate_dataset(df)\n",
    "        \n",
    "        # 7. Generate negatives\n",
    "        negative_df = self.generate_negative_pairs(df)\n",
    "        \n",
    "        if len(negative_df) > 0:\n",
    "            # Translate negatives\n",
    "            for col in ['mark1_wordmark', 'mark2_wordmark']:\n",
    "                en_col = col.replace('_wordmark', '_wordmark_en')\n",
    "                negative_df[en_col] = negative_df[col].apply(\n",
    "                    lambda x: self.pt_to_en.translate(x) if x else \"\"\n",
    "                )\n",
    "                \n",
    "                for lang in ['ha', 'yo']:\n",
    "                    lang_col = col.replace('_wordmark', f'_wordmark_{lang}')\n",
    "                    negative_df[lang_col] = negative_df[en_col].apply(\n",
    "                        lambda x: self.translator.translate_text(x, lang)\n",
    "                    )\n",
    "            \n",
    "            # Combine\n",
    "            df = pd.concat([df, negative_df], ignore_index=True)\n",
    "            df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # 8. Add features\n",
    "        df = self.add_basic_features(df)\n",
    "        df = self.feature_extractor.extract_all(df)\n",
    "        \n",
    "        # 9. Final cleaning\n",
    "        df = clean_dataframe(df)\n",
    "        \n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"‚úÖ PREPROCESSING COMPLETE\")\n",
    "        logger.info(f\"   Total pairs: {len(df)}\")\n",
    "        logger.info(f\"   Positive: {(df['label']==1).sum()}\")\n",
    "        logger.info(f\"   Negative: {(df['label']==0).sum()}\")\n",
    "        logger.info(f\"   Features: {len(df.columns)}\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# =============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS (SEPARATE VISUALIZATIONS)\n",
    "# =============================================================================\n",
    "\n",
    "class TrademarkEDA:\n",
    "    \"\"\"Comprehensive EDA with separate, readable visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "    \n",
    "    def create_visualizations(self, output_dir: str = '.'):\n",
    "        \"\"\"Create separate visualization files for better readability\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Label Distribution\n",
    "        self._plot_label_distribution(output_dir)\n",
    "        \n",
    "        # 2. Wordmark Length Distribution\n",
    "        self._plot_length_distribution(output_dir)\n",
    "        \n",
    "        # 3. Visual Similarity Distribution\n",
    "        self._plot_visual_similarity(output_dir)\n",
    "        \n",
    "        # 4. Phonetic Match Distribution\n",
    "        self._plot_phonetic_matches(output_dir)\n",
    "        \n",
    "        # 5. Semantic Similarity by Language\n",
    "        self._plot_semantic_similarity(output_dir)\n",
    "        \n",
    "        # 6. Feature Correlation Heatmap\n",
    "        self._plot_correlation_heatmap(output_dir)\n",
    "        \n",
    "        # 7. Length Difference by Label\n",
    "        self._plot_length_difference(output_dir)\n",
    "        \n",
    "        logger.info(f\"üìä All visualizations saved to: {output_dir}/\")\n",
    "    \n",
    "    def _plot_label_distribution(self, output_dir):\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        label_counts = self.df['label'].value_counts()\n",
    "        colors = ['#2ecc71', '#e74c3c']\n",
    "        ax.pie(label_counts.values, labels=['Similar', 'Dissimilar'], \n",
    "               autopct='%1.1f%%', startangle=90, colors=colors,\n",
    "               textprops={'fontsize': 14, 'weight': 'bold'})\n",
    "        ax.set_title('Label Distribution', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/01_label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_length_distribution(self, output_dir):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        all_lengths = pd.concat([self.df['mark1_length'], self.df['mark2_length']])\n",
    "        ax.hist(all_lengths, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(all_lengths.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {all_lengths.mean():.1f}')\n",
    "        ax.set_xlabel('Wordmark Length (characters)', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title('Wordmark Length Distribution', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.legend(fontsize=12)\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/02_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_visual_similarity(self, output_dir):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Levenshtein\n",
    "        for label in [0, 1]:\n",
    "            subset = self.df[self.df['label'] == label]['visual_levenshtein']\n",
    "            ax1.hist(subset, bins=20, alpha=0.6, \n",
    "                    label=f'{\"Similar\" if label==1 else \"Dissimilar\"}')\n",
    "        ax1.set_xlabel('Levenshtein Distance', fontsize=12)\n",
    "        ax1.set_ylabel('Frequency', fontsize=12)\n",
    "        ax1.set_title('Visual Similarity: Levenshtein Distance', fontsize=14, fontweight='bold')\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Jaro-Winkler\n",
    "        for label in [0, 1]:\n",
    "            subset = self.df[self.df['label'] == label]['visual_jaro_winkler']\n",
    "            ax2.hist(subset, bins=20, alpha=0.6, \n",
    "                    label=f'{\"Similar\" if label==1 else \"Dissimilar\"}')\n",
    "        ax2.set_xlabel('Jaro-Winkler Similarity', fontsize=12)\n",
    "        ax2.set_ylabel('Frequency', fontsize=12)\n",
    "        ax2.set_title('Visual Similarity: Jaro-Winkler', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/03_visual_similarity.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_phonetic_matches(self, output_dir):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Soundex\n",
    "        soundex_counts = self.df.groupby(['label', 'soundex_match']).size().unstack(fill_value=0)\n",
    "        soundex_counts.plot(kind='bar', ax=ax1, color=['#e74c3c', '#2ecc71'])\n",
    "        ax1.set_xlabel('Label', fontsize=12)\n",
    "        ax1.set_ylabel('Count', fontsize=12)\n",
    "        ax1.set_title('Phonetic Match: Soundex', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xticklabels(['Dissimilar', 'Similar'], rotation=0)\n",
    "        ax1.legend(['No Match', 'Match'], fontsize=11)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Metaphone\n",
    "        metaphone_counts = self.df.groupby(['label', 'metaphone_match']).size().unstack(fill_value=0)\n",
    "        metaphone_counts.plot(kind='bar', ax=ax2, color=['#e74c3c', '#2ecc71'])\n",
    "        ax2.set_xlabel('Label', fontsize=12)\n",
    "        ax2.set_ylabel('Count', fontsize=12)\n",
    "        ax2.set_title('Phonetic Match: Metaphone', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xticklabels(['Dissimilar', 'Similar'], rotation=0)\n",
    "        ax2.legend(['No Match', 'Match'], fontsize=11)\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/04_phonetic_matches.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_semantic_similarity(self, output_dir):\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        langs = ['en', 'ha', 'yo']\n",
    "        \n",
    "        # Distributions by label for each language\n",
    "        for idx, lang in enumerate(langs):\n",
    "            row = idx // 2\n",
    "            col = idx % 2\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            for label in [0, 1]:\n",
    "                subset = self.df[self.df['label'] == label][f'semantic_similarity_{lang}']\n",
    "                ax.hist(subset, bins=20, alpha=0.6, \n",
    "                       label=f'{\"Similar\" if label==1 else \"Dissimilar\"}')\n",
    "            \n",
    "            lang_names = {'en': 'English', 'ha': 'Hausa', 'yo': 'Yoruba'}\n",
    "            ax.set_xlabel('Cosine Similarity', fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            ax.set_title(f'Semantic Similarity: {lang_names[lang]}', fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=11)\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Comparison bar chart\n",
    "        ax = axes[1, 1]\n",
    "        means = [self.df[f'semantic_similarity_{lang}'].mean() for lang in langs]\n",
    "        colors = ['#3498db', '#e67e22', '#9b59b6']\n",
    "        bars = ax.bar(['English', 'Hausa', 'Yoruba'], means, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.set_ylabel('Average Similarity', fontsize=12)\n",
    "        ax.set_title('Semantic Similarity by Language', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/05_semantic_similarity.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_correlation_heatmap(self, output_dir):\n",
    "        fig, ax = plt.subplots(figsize=(14, 12))\n",
    "        \n",
    "        feature_cols = [\n",
    "            'visual_levenshtein', 'visual_jaro_winkler',\n",
    "            'soundex_match', 'metaphone_match',\n",
    "            'semantic_similarity_en', 'semantic_similarity_ha', 'semantic_similarity_yo',\n",
    "            'length_diff', 'label'\n",
    "        ]\n",
    "        \n",
    "        corr_data = self.df[feature_cols].corr()\n",
    "        sns.heatmap(corr_data, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
    "                   ax=ax, cbar_kws={'label': 'Correlation'}, \n",
    "                   square=True, linewidths=1)\n",
    "        ax.set_title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/06_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_length_difference(self, output_dir):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        data_to_plot = [\n",
    "            self.df[self.df['label'] == 0]['length_diff'],\n",
    "            self.df[self.df['label'] == 1]['length_diff']\n",
    "        ]\n",
    "        \n",
    "        bp = ax.boxplot(data_to_plot, labels=['Dissimilar', 'Similar'],\n",
    "                       patch_artist=True, showmeans=True)\n",
    "        \n",
    "        colors = ['#e74c3c', '#2ecc71']\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_xlabel('Label', fontsize=12)\n",
    "        ax.set_ylabel('Length Difference', fontsize=12)\n",
    "        ax.set_title('Length Difference by Label', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/07_length_difference.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        \"\"\"Print detailed statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\nüìä Dataset Overview:\")\n",
    "        print(f\"   Total pairs: {len(self.df):,}\")\n",
    "        print(f\"   Similar pairs: {(self.df['label']==1).sum():,} ({(self.df['label']==1).mean()*100:.1f}%)\")\n",
    "        print(f\"   Dissimilar pairs: {(self.df['label']==0).sum():,} ({(self.df['label']==0).mean()*100:.1f}%)\")\n",
    "        print(f\"   Total features: {len(self.df.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìè Wordmark Statistics:\")\n",
    "        print(f\"   Average length: {self.df['mark1_length'].mean():.1f} characters\")\n",
    "        print(f\"   Min length: {self.df['mark1_length'].min()}\")\n",
    "        print(f\"   Max length: {self.df['mark1_length'].max()}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Feature Quality:\")\n",
    "        print(f\"   Visual similarity (Jaro-Winkler): {self.df['visual_jaro_winkler'].mean():.3f}\")\n",
    "        print(f\"   Phonetic match rate (Soundex): {self.df['soundex_match'].mean()*100:.1f}%\")\n",
    "        print(f\"   Phonetic match rate (Metaphone): {self.df['metaphone_match'].mean()*100:.1f}%\")\n",
    "        print(f\"   Semantic similarity (EN): {self.df['semantic_similarity_en'].mean():.3f}\")\n",
    "        print(f\"   Semantic similarity (HA): {self.df['semantic_similarity_ha'].mean():.3f}\")\n",
    "        print(f\"   Semantic similarity (YO): {self.df['semantic_similarity_yo'].mean():.3f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run complete pipeline\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = PreprocessingConfig(\n",
    "        max_rows=5,  # Set to None to process all data\n",
    "        sampling_strategy='first',\n",
    "        negative_ratio=1.0,\n",
    "        negative_strategy='mixed',\n",
    "        output_filename='trademark_similarity_dataset_final.csv',\n",
    "        save_visualizations=True\n",
    "    )\n",
    "    \n",
    "    # Process\n",
    "    preprocessor = TrademarkPreprocessor(config)\n",
    "    df = preprocessor.process('trademark_file.csv')\n",
    "    \n",
    "    # EDA\n",
    "    eda = TrademarkEDA(df)\n",
    "    eda.print_statistics()\n",
    "    \n",
    "    if config.save_visualizations:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        viz_dir = f\"eda_visualizations_{timestamp}\"\n",
    "        eda.create_visualizations(viz_dir)\n",
    "    \n",
    "    # Save final dataset\n",
    "    # Generate a safe, non-overwriting filename\n",
    "    safe_output_file = get_incremented_filename(config.output_filename)\n",
    "    df.to_csv(safe_output_file, index=False, encoding='utf-8')\n",
    "    logger.info(f\"\\nüíæ Final dataset saved: {safe_output_file}\")\n",
    "    logger.info(f\"   Shape: {df.shape}\")\n",
    "    logger.info(f\"   File size: {os.path.getsize(safe_output_file) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE DATA (First 5 rows)\")\n",
    "    print(\"=\" * 80)\n",
    "    display_cols = [\n",
    "        'mark1_wordmark', 'mark2_wordmark', 'label',\n",
    "        'visual_levenshtein', 'soundex_match', 'metaphone_match', 'semantic_similarity_en'\n",
    "    ]\n",
    "    print(df[display_cols].head(5).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n‚úÖ STEP 1 COMPLETE - Ready for Step 2 (CNN Architecture)\")\n",
    "    print(f\"   Use file: {safe_output_file}\")\n",
    "    print(f\"   Visualizations: {viz_dir} directory\")\n",
    "\n",
    "    with open('pipeline_config.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'processed_data_file': safe_output_file,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }, f)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    final_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948aa5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2 & 3: CNN+SVM HYBRID MODEL WITH LINGUISTIC FEATURES\n",
      "================================================================================\n",
      "‚úÖ Using dataset from Step 1: trademark_similarity_dataset_final_2.csv\n",
      "\n",
      "================================================================================\n",
      "STEP 2: TRAINING CNN+SVM HYBRID MODEL\n",
      "================================================================================\n",
      "\n",
      "üìÇ Loading preprocessed data...\n",
      "‚úì Loaded 8 trademark pairs\n",
      "  Positive: 4\n",
      "  Negative: 4\n",
      "‚úì Extracted 10 linguistic features\n",
      "\n",
      "üìä Data Split:\n",
      "   Train: 4 (2 positive)\n",
      "   Val:   2 (1 positive)\n",
      "   Test:  2 (1 positive)\n",
      "\n",
      "üèóÔ∏è  Building Character-level CNN Encoder...\n",
      "‚úì Built character tokenizer (vocab size: 23)\n",
      "\n",
      "üî§ Encoding texts to sequences...\n",
      "WARNING:tensorflow:From c:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From c:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From c:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <tf_keras.src.optimizers.adam.Adam object at 0x00000200C1EF6120>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 671\u001b[39m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     hybrid_model, metrics = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 592\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    589\u001b[39m test_m2_seq = cnn.encode_texts(data_splits[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1\u001b[39m])\n\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# Build and train CNN\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m \u001b[43mcnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_siamese_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m history = cnn.train(\n\u001b[32m    594\u001b[39m     train_data=(train_m1_seq, train_m2_seq, data_splits[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m2\u001b[39m]),\n\u001b[32m    595\u001b[39m     val_data=(val_m1_seq, val_m2_seq, data_splits[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m2\u001b[39m]),\n\u001b[32m    596\u001b[39m     epochs=config.EPOCHS\n\u001b[32m    597\u001b[39m )\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 276\u001b[39m, in \u001b[36mCharacterCNNEncoder.build_siamese_cnn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28mself\u001b[39m.model = models.Model(\n\u001b[32m    270\u001b[39m     inputs=[input_mark1, input_mark2],\n\u001b[32m    271\u001b[39m     outputs=output,\n\u001b[32m    272\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33msiamese_cnn\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    273\u001b[39m )\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# Compile\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbinary_crossentropy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPrecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# Store encoder for later use\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28mself\u001b[39m.encoder = encoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bubashir\\trademark-similarity-engine\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:98\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(identifier)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, Optimizer):\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not interpret optimizer identifier: <tf_keras.src.optimizers.adam.Adam object at 0x00000200C1EF6120>"
     ]
    }
   ],
   "source": [
    "# STEP 2 & 3: CNN+SVM ARCHITECTURE WITH ADVANCED FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "# This notebook builds a hybrid CNN+SVM model for trademark similarity detection\n",
    "# Input: Processed CSV from Step 1\n",
    "# Output: Trained models + evaluation metrics\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Feature Engineering (from Step 1)\n",
    "import jellyfish\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2 & 3: CNN+SVM HYBRID MODEL WITH LINGUISTIC FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model training\"\"\"\n",
    "    # Data\n",
    "    try:\n",
    "        with open('pipeline_config.json', 'r') as f:\n",
    "            pipeline_cfg = json.load(f)\n",
    "        DATA_FILE = pipeline_cfg['processed_data_file']\n",
    "        print(f\"‚úÖ Using dataset from Step 1: {DATA_FILE}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Run Step 1 first to generate the processed dataset!\")\n",
    "    \n",
    "    TEST_SIZE = 0.15\n",
    "    VAL_SIZE = 0.15\n",
    "    \n",
    "    # CNN Architecture\n",
    "    MAX_SEQUENCE_LENGTH = 50\n",
    "    MAX_VOCAB_SIZE = 10000\n",
    "    EMBEDDING_DIM = 128\n",
    "    CNN_FILTERS = [128, 64, 32]\n",
    "    CNN_KERNEL_SIZES = [3, 4, 5]\n",
    "    DROPOUT_RATE = 0.5\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    \n",
    "    # SVM\n",
    "    SVM_KERNEL = 'rbf'\n",
    "    SVM_C = 1.0\n",
    "    SVM_GAMMA = 'scale'\n",
    "    \n",
    "    # Output\n",
    "    MODEL_DIR = Path('models')\n",
    "    RESULTS_DIR = Path('results')\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.MODEL_DIR.mkdir(exist_ok=True)\n",
    "        self.RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class TrademarkDataLoader:\n",
    "    \"\"\"Load and prepare trademark data\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: str):\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        print(f\"‚úì Loaded {len(self.df)} trademark pairs\")\n",
    "        print(f\"  Positive: {(self.df['label']==1).sum()}\")\n",
    "        print(f\"  Negative: {(self.df['label']==0).sum()}\")\n",
    "    \n",
    "    def get_text_pairs(self) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Get wordmark pairs as text lists\"\"\"\n",
    "        mark1 = self.df['mark1_wordmark'].fillna('').tolist()\n",
    "        mark2 = self.df['mark2_wordmark'].fillna('').tolist()\n",
    "        return mark1, mark2\n",
    "    \n",
    "    def get_labels(self) -> np.ndarray:\n",
    "        \"\"\"Get similarity labels\"\"\"\n",
    "        return self.df['label'].values\n",
    "    \n",
    "    def get_linguistic_features(self) -> np.ndarray:\n",
    "        \"\"\"Extract all pre-computed linguistic features from Step 1\"\"\"\n",
    "        feature_cols = [\n",
    "            'visual_levenshtein', 'visual_jaro_winkler',\n",
    "            'soundex_match', 'metaphone_match',\n",
    "            'semantic_similarity_en', 'semantic_similarity_ha', 'semantic_similarity_yo',\n",
    "            'length_diff', 'mark1_length', 'mark2_length'\n",
    "        ]\n",
    "        \n",
    "        features = self.df[feature_cols].fillna(0).values\n",
    "        print(f\"‚úì Extracted {features.shape[1]} linguistic features\")\n",
    "        return features\n",
    "    \n",
    "    def split_data(self, test_size=0.15, val_size=0.15, random_state=42):\n",
    "        \"\"\"Stratified train/val/test split\"\"\"\n",
    "        mark1, mark2 = self.get_text_pairs()\n",
    "        labels = self.get_labels()\n",
    "        features = self.get_linguistic_features()\n",
    "        \n",
    "        # First split: train+val vs test\n",
    "        train_val_size = 1.0 - test_size\n",
    "        X_train_val_m1, X_test_m1, X_train_val_m2, X_test_m2, \\\n",
    "        y_train_val, y_test, features_train_val, features_test = train_test_split(\n",
    "            mark1, mark2, labels, features,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size_adjusted = val_size / train_val_size\n",
    "        X_train_m1, X_val_m1, X_train_m2, X_val_m2, \\\n",
    "        y_train, y_val, features_train, features_val = train_test_split(\n",
    "            X_train_val_m1, X_train_val_m2, y_train_val, features_train_val,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=y_train_val,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä Data Split:\")\n",
    "        print(f\"   Train: {len(y_train)} ({(y_train==1).sum()} positive)\")\n",
    "        print(f\"   Val:   {len(y_val)} ({(y_val==1).sum()} positive)\")\n",
    "        print(f\"   Test:  {len(y_test)} ({(y_test==1).sum()} positive)\")\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train_m1, X_train_m2, y_train, features_train),\n",
    "            'val': (X_val_m1, X_val_m2, y_val, features_val),\n",
    "            'test': (X_test_m1, X_test_m2, y_test, features_test)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# CHARACTER-LEVEL CNN ENCODER\n",
    "# =============================================================================\n",
    "\n",
    "class CharacterCNNEncoder:\n",
    "    \"\"\"Character-level CNN for trademark text encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    \n",
    "    def build_tokenizer(self, texts: List[str]):\n",
    "        \"\"\"Build character-level tokenizer\"\"\"\n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=self.config.MAX_VOCAB_SIZE,\n",
    "            char_level=True,\n",
    "            oov_token='<UNK>'\n",
    "        )\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        print(f\"‚úì Built character tokenizer (vocab size: {len(self.tokenizer.word_index)})\")\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Convert texts to padded sequences\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(\n",
    "            sequences,\n",
    "            maxlen=self.config.MAX_SEQUENCE_LENGTH,\n",
    "            padding='post',\n",
    "            truncating='post'\n",
    "        )\n",
    "        return padded\n",
    "    \n",
    "    def build_siamese_cnn(self):\n",
    "        \"\"\"Build Siamese CNN architecture for pair comparison\"\"\"\n",
    "        # Shared character encoder\n",
    "        char_input = layers.Input(shape=(self.config.MAX_SEQUENCE_LENGTH,), name='char_input')\n",
    "        \n",
    "        # Embedding layer\n",
    "        x = layers.Embedding(\n",
    "            input_dim=len(self.tokenizer.word_index) + 1,\n",
    "            output_dim=self.config.EMBEDDING_DIM,\n",
    "            input_length=self.config.MAX_SEQUENCE_LENGTH,\n",
    "            name='embedding'\n",
    "        )(char_input)\n",
    "        \n",
    "        # Multiple parallel CNN branches (different kernel sizes)\n",
    "        conv_outputs = []\n",
    "        for i, (filters, kernel_size) in enumerate(\n",
    "            zip(self.config.CNN_FILTERS, self.config.CNN_KERNEL_SIZES)\n",
    "        ):\n",
    "            conv = layers.Conv1D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                activation='relu',\n",
    "                name=f'conv_{kernel_size}'\n",
    "            )(x)\n",
    "            pool = layers.GlobalMaxPooling1D(name=f'pool_{kernel_size}')(conv)\n",
    "            conv_outputs.append(pool)\n",
    "        \n",
    "        # Concatenate all CNN outputs\n",
    "        concatenated = layers.Concatenate(name='concat_cnn')(conv_outputs)\n",
    "        \n",
    "        # Dense layers\n",
    "        dense = layers.Dense(256, activation='relu', name='dense_1')(concatenated)\n",
    "        dense = layers.Dropout(self.config.DROPOUT_RATE, name='dropout_1')(dense)\n",
    "        dense = layers.Dense(128, activation='relu', name='dense_2')(dense)\n",
    "        dense = layers.Dropout(self.config.DROPOUT_RATE, name='dropout_2')(dense)\n",
    "        embedding_output = layers.Dense(64, activation='relu', name='embedding_output')(dense)\n",
    "        \n",
    "        # Build shared encoder\n",
    "        encoder = models.Model(inputs=char_input, outputs=embedding_output, name='cnn_encoder')\n",
    "        \n",
    "        # Siamese architecture\n",
    "        input_mark1 = layers.Input(shape=(self.config.MAX_SEQUENCE_LENGTH,), name='mark1')\n",
    "        input_mark2 = layers.Input(shape=(self.config.MAX_SEQUENCE_LENGTH,), name='mark2')\n",
    "        \n",
    "        # Get embeddings for both marks using shared encoder\n",
    "        embedding_mark1 = encoder(input_mark1)\n",
    "        embedding_mark2 = encoder(input_mark2)\n",
    "        \n",
    "        # Compute similarity features\n",
    "        # 1. Absolute difference\n",
    "        diff = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([embedding_mark1, embedding_mark2])\n",
    "        \n",
    "        # 2. Element-wise product\n",
    "        product = layers.Multiply()([embedding_mark1, embedding_mark2])\n",
    "        \n",
    "        # 3. Concatenate all\n",
    "        merged = layers.Concatenate()([embedding_mark1, embedding_mark2, diff, product])\n",
    "        \n",
    "        # Classification head\n",
    "        x = layers.Dense(128, activation='relu', name='classifier_dense_1')(merged)\n",
    "        x = layers.Dropout(self.config.DROPOUT_RATE, name='classifier_dropout')(x)\n",
    "        x = layers.Dense(64, activation='relu', name='classifier_dense_2')(x)\n",
    "        output = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "        \n",
    "        # Build full model\n",
    "        self.model = models.Model(\n",
    "            inputs=[input_mark1, input_mark2],\n",
    "            outputs=output,\n",
    "            name='siamese_cnn'\n",
    "        )\n",
    "        \n",
    "        # Compile\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "        )\n",
    "        \n",
    "        # Store encoder for later use\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        print(\"\\nüèóÔ∏è  CNN Architecture:\")\n",
    "        self.model.summary()\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self, train_data, val_data, epochs=50):\n",
    "        \"\"\"Train the Siamese CNN\"\"\"\n",
    "        X_train_m1, X_train_m2, y_train = train_data\n",
    "        X_val_m1, X_val_m2, y_val = val_data\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stop = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=self.config.EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        checkpoint = callbacks.ModelCheckpoint(\n",
    "            str(config.MODEL_DIR / 'best_cnn_model.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(\"\\nüöÄ Training CNN...\")\n",
    "        self.history = self.model.fit(\n",
    "            [X_train_m1, X_train_m2], y_train,\n",
    "            validation_data=([X_val_m1, X_val_m2], y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def extract_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Extract CNN embeddings for texts\"\"\"\n",
    "        sequences = self.encode_texts(texts)\n",
    "        embeddings = self.encoder.predict(sequences, verbose=0)\n",
    "        return embeddings\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model and tokenizer\"\"\"\n",
    "        self.model.save(path)\n",
    "        with open(f\"{path}_tokenizer.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        print(f\"‚úì Saved CNN model to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        self.model = keras.models.load_model(path)\n",
    "        with open(f\"{path}_tokenizer.pkl\", 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        # Rebuild encoder\n",
    "        self.encoder = models.Model(\n",
    "            inputs=self.model.get_layer('cnn_encoder').input,\n",
    "            outputs=self.model.get_layer('cnn_encoder').output\n",
    "        )\n",
    "        print(f\"‚úì Loaded CNN model from {path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYBRID CNN+SVM CLASSIFIER\n",
    "# =============================================================================\n",
    "\n",
    "class HybridCNNSVM:\n",
    "    \"\"\"Combines CNN embeddings with linguistic features for SVM classification\"\"\"\n",
    "    \n",
    "    def __init__(self, cnn_encoder: CharacterCNNEncoder, config: ModelConfig):\n",
    "        self.cnn_encoder = cnn_encoder\n",
    "        self.config = config\n",
    "        self.svm = None\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def extract_hybrid_features(\n",
    "        self, \n",
    "        mark1_texts: List[str], \n",
    "        mark2_texts: List[str],\n",
    "        linguistic_features: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Combine CNN embeddings + linguistic features\"\"\"\n",
    "        # Get CNN embeddings\n",
    "        emb1 = self.cnn_encoder.extract_embeddings(mark1_texts)\n",
    "        emb2 = self.cnn_encoder.extract_embeddings(mark2_texts)\n",
    "        \n",
    "        # Concatenate: [emb1, emb2, linguistic_features]\n",
    "        hybrid_features = np.hstack([emb1, emb2, linguistic_features])\n",
    "        \n",
    "        print(f\"‚úì Hybrid features shape: {hybrid_features.shape}\")\n",
    "        return hybrid_features\n",
    "    \n",
    "    def train(self, train_data, val_data=None):\n",
    "        \"\"\"Train SVM on hybrid features\"\"\"\n",
    "        X_train_m1, X_train_m2, y_train, features_train = train_data\n",
    "        \n",
    "        print(\"\\nüîß Extracting hybrid features for SVM training...\")\n",
    "        X_train_hybrid = self.extract_hybrid_features(\n",
    "            X_train_m1, X_train_m2, features_train\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train_hybrid)\n",
    "        \n",
    "        # Train SVM\n",
    "        print(f\"\\nüöÄ Training SVM (kernel={self.config.SVM_KERNEL})...\")\n",
    "        self.svm = SVC(\n",
    "            kernel=self.config.SVM_KERNEL,\n",
    "            C=self.config.SVM_C,\n",
    "            gamma=self.config.SVM_GAMMA,\n",
    "            probability=True,\n",
    "            random_state=42,\n",
    "            verbose=True\n",
    "        )\n",
    "        self.svm.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate on training data\n",
    "        train_pred = self.svm.predict(X_train_scaled)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        print(f\"‚úì SVM Training Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        # Evaluate on validation if provided\n",
    "        if val_data is not None:\n",
    "            X_val_m1, X_val_m2, y_val, features_val = val_data\n",
    "            X_val_hybrid = self.extract_hybrid_features(\n",
    "                X_val_m1, X_val_m2, features_val\n",
    "            )\n",
    "            X_val_scaled = self.scaler.transform(X_val_hybrid)\n",
    "            val_pred = self.svm.predict(X_val_scaled)\n",
    "            val_acc = accuracy_score(y_val, val_pred)\n",
    "            print(f\"‚úì SVM Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    def predict(self, mark1_texts: List[str], mark2_texts: List[str], \n",
    "                linguistic_features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict similarity labels\"\"\"\n",
    "        X_hybrid = self.extract_hybrid_features(mark1_texts, mark2_texts, linguistic_features)\n",
    "        X_scaled = self.scaler.transform(X_hybrid)\n",
    "        return self.svm.predict(X_scaled)\n",
    "    \n",
    "    def predict_proba(self, mark1_texts: List[str], mark2_texts: List[str],\n",
    "                      linguistic_features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict similarity probabilities\"\"\"\n",
    "        X_hybrid = self.extract_hybrid_features(mark1_texts, mark2_texts, linguistic_features)\n",
    "        X_scaled = self.scaler.transform(X_hybrid)\n",
    "        return self.svm.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save SVM and scaler\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({'svm': self.svm, 'scaler': self.scaler}, f)\n",
    "        print(f\"‚úì Saved SVM to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load SVM and scaler\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.svm = data['svm']\n",
    "            self.scaler = data['scaler']\n",
    "        print(f\"‚úì Loaded SVM from {path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATOR\n",
    "# =============================================================================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation and visualization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate(y_true, y_pred, y_proba, dataset_name=\"Test\"):\n",
    "        \"\"\"Calculate all metrics\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred),\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_true, y_proba)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{dataset_name} Set Evaluation\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "        print(f\"ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(f\"\\n{classification_report(y_true, y_pred, target_names=['Dissimilar', 'Similar'])}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, save_path=None):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Dissimilar', 'Similar'],\n",
    "                   yticklabels=['Dissimilar', 'Similar'],\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_roc_curve(y_true, y_proba, save_path=None):\n",
    "        \"\"\"Plot ROC curve\"\"\"\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_training_history(history, save_path=None):\n",
    "        \"\"\"Plot CNN training history\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        ax1.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training & Validation Loss', fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Accuracy\n",
    "        ax2.plot(history.history['accuracy'], label='Train Acc', linewidth=2)\n",
    "        ax2.plot(history.history['val_accuracy'], label='Val Acc', linewidth=2)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Training & Validation Accuracy', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run complete training pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: TRAINING CNN+SVM HYBRID MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    print(\"\\nüìÇ Loading preprocessed data...\")\n",
    "    loader = TrademarkDataLoader(config.DATA_FILE)\n",
    "    data_splits = loader.split_data(\n",
    "        test_size=config.TEST_SIZE,\n",
    "        val_size=config.VAL_SIZE\n",
    "    )\n",
    "    \n",
    "    # 2. Build Character-level CNN\n",
    "    print(\"\\nüèóÔ∏è  Building Character-level CNN Encoder...\")\n",
    "    cnn = CharacterCNNEncoder(config)\n",
    "    \n",
    "    # Build tokenizer on all training text\n",
    "    all_train_texts = data_splits['train'][0] + data_splits['train'][1]\n",
    "    cnn.build_tokenizer(all_train_texts)\n",
    "    \n",
    "    # Encode all datasets\n",
    "    print(\"\\nüî§ Encoding texts to sequences...\")\n",
    "    train_m1_seq = cnn.encode_texts(data_splits['train'][0])\n",
    "    train_m2_seq = cnn.encode_texts(data_splits['train'][1])\n",
    "    val_m1_seq = cnn.encode_texts(data_splits['val'][0])\n",
    "    val_m2_seq = cnn.encode_texts(data_splits['val'][1])\n",
    "    test_m1_seq = cnn.encode_texts(data_splits['test'][0])\n",
    "    test_m2_seq = cnn.encode_texts(data_splits['test'][1])\n",
    "    \n",
    "    # Build and train CNN\n",
    "    cnn.build_siamese_cnn()\n",
    "    history = cnn.train(\n",
    "        train_data=(train_m1_seq, train_m2_seq, data_splits['train'][2]),\n",
    "        val_data=(val_m1_seq, val_m2_seq, data_splits['val'][2]),\n",
    "        epochs=config.EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    ModelEvaluator.plot_training_history(\n",
    "        history,\n",
    "        save_path=config.RESULTS_DIR / 'cnn_training_history.png'\n",
    "    )\n",
    "    \n",
    "    # Save CNN\n",
    "    cnn.save(str(config.MODEL_DIR / 'cnn_encoder'))\n",
    "    \n",
    "    # 3. Train Hybrid CNN+SVM\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: TRAINING HYBRID CNN+SVM CLASSIFIER\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    hybrid_model = HybridCNNSVM(cnn, config)\n",
    "    hybrid_model.train(\n",
    "        train_data=data_splits['train'],\n",
    "        val_data=data_splits['val']\n",
    "    )\n",
    "    \n",
    "    # Save SVM\n",
    "    hybrid_model.save(str(config.MODEL_DIR / 'hybrid_svm.pkl'))\n",
    "    \n",
    "    # 4. Evaluate on Test Set\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_test_m1, X_test_m2, y_test, features_test = data_splits['test']\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = hybrid_model.predict(X_test_m1, X_test_m2, features_test)\n",
    "    y_proba = hybrid_model.predict_proba(X_test_m1, X_test_m2, features_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = ModelEvaluator.evaluate(y_test, y_pred, y_proba, \"Test\")\n",
    "    \n",
    "    # Visualizations\n",
    "    ModelEvaluator.plot_confusion_matrix(\n",
    "        y_test, y_pred,\n",
    "        save_path=config.RESULTS_DIR / 'confusion_matrix.png'\n",
    "    )\n",
    "    \n",
    "    ModelEvaluator.plot_roc_curve(\n",
    "        y_test, y_proba,\n",
    "        save_path=config.RESULTS_DIR / 'roc_curve.png'\n",
    "    )\n",
    "    \n",
    "    # 5. Save Results\n",
    "    results = {\n",
    "        'config': config.__dict__,\n",
    "        'metrics': metrics,\n",
    "        'data_splits': {\n",
    "            'train_size': len(data_splits['train'][2]),\n",
    "            'val_size': len(data_splits['val'][2]),\n",
    "            'test_size': len(data_splits['test'][2])\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(config.RESULTS_DIR / 'evaluation_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"\\n‚úÖ TRAINING COMPLETE!\")\n",
    "    print(f\"   Models saved to: {config.MODEL_DIR}/\")\n",
    "    print(f\"   Results saved to: {config.RESULTS_DIR}/\")\n",
    "    print(\"\\nüéØ Next Step: Implement inference API (Step 5)\")\n",
    "    \n",
    "    return hybrid_model, metrics\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    hybrid_model, metrics = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
